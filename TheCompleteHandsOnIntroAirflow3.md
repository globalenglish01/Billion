### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/01_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p01 1. Prerequisites.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎来到Apache Airflow的完整实操入门课程
我是玛玛莉
非常高兴今天能和大家见面参加这个课程
因为你们将要发现Airflow的强大之处
以及它为何在当今科技行业如此广泛使用
为了确保你们能以最佳方式学习本课程
有一些重要的先决条件需要满足
首先是必须在电脑上安装Docker
如果你不了解Docker是什么
熟悉这个工具非常重要
Docker帮助你在容器化环境中运行应用程序
例如
可以将其视为一个小型虚拟机
Airflow将在此运行
我们将使用Docker在本地机器上运行Airflow
这对我的重要性
因为我不了解你的环境，必须确保能帮助到你
我们将使用Docker
这样我们环境一致
好的 这非常重要
稍后课程中我会展示安装Docker的步骤
同时也会使用Docker安装和运行Airflow
下一个要求是必须掌握Python
在Airflow中
所有操作都是用Python完成的
显然 如果你完全不了解Python
将很难跟上课程内容
好的 你需要具备Python编程基础
因为课程中不会教授Python
请确保自己
至少不需要成为专家
只需掌握基础 好的
就是这样 这才是需要牢记的唯一要求
这是一门Airflow入门课程
非常适合初学者的Airflow介绍
你将只学习启动Airflow所需的知识 下一节视频见
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/02_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p02 2. Course Objectives.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们看看如何从这门课程中获得最大收益
我们现在首先要做的是设定一些预期
这是一门完整的Apache Airflow实战入门课程
这是Airflow的入门介绍
这意味着如果你是初学者
或者刚开始接触Airflow
这门课程适合你
如果你对Airflow一无所知
这门课程适合你
如果你想使用一些基础但强大的功能编写第一个数据管道
这门课程适合你
但如果你想深入高级概念且已经了解Airflow
并且已经使用Airflow一段时间了
那么这门课程不适合你
好的，这种情况下我有其他课程你可以查看
如果你想学习更多高级功能
但再次强调 这门课程面向初学者
仅针对Airflow新手
事实上 完成该课程后
我建议你查看我的其他课程
这样你可以学习更多高级功能并使用Airflow创建更复杂的流程
让我给你几个建议确保你充分利用这门课程
首先作为新用户请慢慢来
有很多概念需要学习
这可能会让人感到压力
所以不要犹豫提出问题
有时不要犹豫跟着我在电脑上演示
并且不要犹豫重复观看同一视频
好的 慢慢来
这不是竞赛
此外 不要犹豫使用书签和笔记
例如 如果你想快速返回某个视频
可以使用书签功能
如果你想记录一些内容比如
你知道的 特定概念等等
在某个视频中 你也可以在Udemy播放器中进行
此外 如果你觉得我说得太慢
例如 嗯
不要犹豫加快视频速度
你可以这样做
使用Udemy播放器的UI界面
由于版权原因，我无法提供幻灯片
事实上 宇美
不允许这样做，截图也一样
你不能对课程内容进行截图
所以这就是为什么 你要记住关键要点的最佳方法
获取视频的核心要点是使用节点
所以不要犹豫，尽可能多地使用节点
在课程的某个阶段
你会感到卡住
这是正常的
好的 现在这是预期的
如何找到解决你遇到的问题的方法
有很多解决方案
第一个真正的方法是
你应该去Stack Overflow
谷歌或AI就可以了
不要犹豫使用云或聊天
GPT 无论你想解决什么问题
然后如果你找不到解决方案
你仍然可以在Udemy的问答区提问
我很乐意帮助你
但请记住
我还是人类
所以可能需要一点时间回复你的问题
正如你所想象的
我每天收到很多问题
好的 但我会尽力确保尽快回复你的问题
就这样了，说实话
这才是充分利用这门课程的方法 准备好，我们下个视频见
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/03_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p03 3. Who I am.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


一段快速自我介绍视频
我叫mormarti
我是法国人，现居毛里求斯
一个靠近南非的小岛
我也出生在法国
我是udms平台的畅销讲师
你还会发现我制作的关于Airflow和其他工具的其他课程
我是一名数据工程师
这意味着我实践所教授的内容
这一点很重要
我在工作中将Airflow用于生产环境
如果你想知道我的日常
我是Astronomer的客户教育负责人
如果你不知道Astronomer是什么
这是云端大规模运行Airflow的最佳平台，我确实这么认为
我坚信这是云端运行Airflow的最佳选择
话说回来 我在公司用Airflow管理
与教育和认证相关的多个数据管道
所以你将在课程中学习到的内容几乎就是我真实的工作
此外
但同样重要的是我要与你建立联系
我想了解你 因为我了解自己
但我不了解你
我希望更深入了解你
为此你可以通过领英联系我
请现在暂停视频并进行连接
这对我很重要
如果你想了解更多关于Airflow的相关功能
其他人也可以查看我的YouTube频道
最后我的网站虽然
我并不经常更新 但我会定期发布教程
随时可以查看
当你有时间时
这就是关于我的全部 我们下次视频再见
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/04_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p04 4. Set up your Development Environment.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 你将设置开发环境
我们开始吧，好的
如果你的电脑上没有安装Docker Desktop
你的第一步是访问docker.com
然后当你进入该页面
你可以点击 立即开始
然后下载Docker Desktop
正如你看到的这里
根据你的操作系统
你有不同的选项
例如如果你使用macOS
你可以选择第一个选项
然后下载Docker Desktop
安装Docker Desktop非常简单
但如果你有任何问题
可以前往学习如何安装Docker
然后根据你的项目系统
你会看到如何安装
例如如果你在Windows上
你可以点击这里选择Windows
安装完成后你会看到需要遵循的步骤
你应该能够打开它
最终你会看到以下仪表盘
现在让我们去顶部右角的设置
然后进入资源设置，确保有足够的资源运行Airflow
这意味着你需要至少8GB内存
对于CPU
这不太重要
我建议至少4核
但如果你有更多 请充分利用
但更重要的是
内存至少需要8GB
同时确保有足够的磁盘空间
如你所见底部有磁盘信息
并查看已用磁盘空间
并确认空间限制以确保足够
否则无法在Docker中运行Airflow
一种释放空间的方法是增加使用限制
或者前往故障排除
然后点击清理和清除数据
通过清理数据可以释放磁盘空间
好的 完成这些后
下一步是安装uv
如果你不了解uv是什么
强烈建议你开始了解它
因为它非常出色
基本上如果你习惯使用peep
别再想了
现在你应该使用uv
你可以看到原因 有很多原因
现在为什么要用uv而不是p来安装uv
你前往安装步骤
然后只需按照指示操作
所以如果你在mac os或linux系统
则需要运行以下命令
而如果你在windows系统
则应运行以下命令
操作非常简单
安装uv应该没有问题
安装完成后
你可以验证是否正常运行
为此你只需打开终端并确保输入uv时
会得到以下输出
如果是这样的话 恭喜你
到此为止 你已成功安装所有内容
你需要本地运行airflow 正如接下来的视频所示
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/05_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p05 5. Optional Quick intro to Docker.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


这个视频完全是可选的
但如果你不了解Docker
这个视频就是为你准备的
让我快速给你
快速介绍Docker
首先 什么是Docker
嗯 Docker是一个开源平台，能够帮助像你这样的开发者自动化部署
扩展和管理应用程序于轻量级可移植容器中
容器在这里非常重要
那么什么是容器呢
容器是一个自包含单元
封装了应用程序
连同其依赖项
库和配置
确保在不同环境中的一致性和可移植性
需要记住的是，使用Docker
你可以将应用程序嵌入容器
而该容器包含应用程序运行所需的一切
这个容器可以在任何操作系统任何环境中运行
无论已有的依赖关系如何
因为容器完全与运行环境隔离
好的 只需记住Docker能让开发更简单
以一致且隔离的方式打包和运行应用
这就是我们课程中使用Docker的原因
正是因为Docker
几乎就像我们共享同一环境
我可以帮助你
如果你遇到问题
现在 你知道什么是Docker了
简而言之它是如何工作的
你总是会从Dockerfile开始
Dockerfile描述了要在Docker容器中运行的应用程序
例如 在Dockerfile中
你将定义应用程序的依赖项
你将定义
一些代码或命令来运行
你还将定义应用程序所需的系统依赖等
好的 记住Dockerfile描述了应用程序及其所需的一切
准备好Dockerfile后
使用docker build构建Dockerfile
这会创建Docker镜像
该Docker镜像是Dockerfile的编译版本
更具体地说 它是一个只读模板，包含应用程序代码和库
以及运行容器所需的依赖项
因此镜像构成了容器的基础构建单元
而Docker镜像也可以基于其他镜像构建
现在你已准备好对应应用程序的Docker镜像
下一步是运行命令docker wren
并指定该Docker镜像
一旦执行此操作，就会创建对应的Docker容器
该Docker容器是此镜像的运行实例
容器与主机系统隔离
因此与操作系统和其他容器无关
这种隔离确保应用程序能稳定运行
无论底层基础设施如何
这就是你需要了解的Docker核心知识
这就是其工作原理
从Dockerfile开始描述你的应用程序
通过构建Dockerfile获取Docker镜像
然后使用docker run运行该镜像 以获得对应应用程序运行的容器
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/06_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p06 1. Why Airflow.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中我们将讨论Apache Airflow以及它为何对数据专家有用
我将给出三个主要原因
用简单易懂的语言和例子
让我们想象你是一位交响乐团的指挥
每位数据任务的执行者都清楚如何演奏自己的乐器
但你的职责是确保它们按正确顺序演奏
并在恰当时机创造美妙的音乐
无缝的数据管道
嗯 Apache Airflow就像为你的数据任务配备自动指挥
它帮助你定义任务的执行顺序
确保每个任务仅在其前置条件完成后启动
并管理整个数据工作流的定时调度
例如 假设你需要从数据库收集数据
清理数据并进行计算
然后生成报告Airflow
帮助你定义这个流程并确保每一步按正确顺序执行
即使某些任务耗时更长
好的 第二个原因是可见性
将Airflow想象成机场的控制塔
就像空中交通管制员能看见所有飞机在天空和地面
Airflow为你提供所有数据任务的全景视图
这种可见性帮助你监控工作流进度
快速识别和解决问题
并理解任务间的依赖关系
例如 如果你运行多个不同项目的数 据管道
Airflow提供一个仪表盘
你可以一目了然查看每个管道的状态
如果某个任务失败
你可以轻松发现并采取行动
而不是数小时后发现报告未生成
最后一个特点是灵活性和可扩展性
Airflow是数据工作流的瑞士军刀
足够灵活处理各种任务并随需求增长
这种灵活性允许连接多种数据源和工具
从小规模开始并随着项目扩展定制工作流以满足特定需求
例如
你可以从使用Airflow调度简单数据库查询开始
随着需求增长
可以扩展到包含机器学习模型
数据质量检查
甚至触发外部API等操作
这只是众多原因中的三个
Apache Airflow可以成为数据工具箱中的宝贵工具
在后续课程中你将看到
Airflow确实非常强大 但让我们先定义什么是
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/07_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p07 2. What is Airflow.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 你将了解什么是Apache Airflow
让我们看看Airflow的官方定义如下
Airflow是一个用于编程化创建
调度和监控工作流的开源平台
换句话说
Airflow是一个帮助你创建
组织 并跟踪数据任务的工具
它就像一个能自动运行的智能待办事项清单
这远不止于此
现在你应该明白了吧
让我们看看我认为使用Airflow的四大核心优势
首先是动态性
Airflow可以根据实际情况进行适应和调整
为什么呢
因为Airflow基于Python
Python易于使用且功能强大
因此你可以用Python编写工作流
这样一来你的工作流拥有无限可能
同时支持动态任务
可以根据动态输入生成任务
例如
周一预计接收五个文件
需要为每个文件创建一个任务
Airflow可以自动完成这一操作
但周二又有六个文件
无需修改工作流即可生成每个文件的任务
Airflow能根据输入动态生成任务
即使这些输入是预先未知的
接下来是动态工作流
这次可根据静态输入生成工作流
例如 你有配置文件
需要为每个配置文件生成一个工作流
最后但同样重要的是分支功能
可根据条件或结果执行不同任务集
这一优势的示例如下
假设你在分析销售数据
如果昨日销售额高于正常水平
Airflow可自动添加处理每个订单的任务
无需手动调整任何内容
这非常便捷
好的 接下来是第二个优势
可扩展性意味着Airflow能处理从小到大的工作量
就像橡皮筋能适应大小不同的物体
Airflow既能管理简单任务也能处理数百个复杂任务
Airflow提供多种执行模式
稍后课程中你将看到
但显然这取决于你的基础设施和预算
这种优势的一个例子就是当你开始使用Airflow处理单个商店的数据
当你的业务扩展到一百家门店时
Airflow可以轻松扩展以处理所有一百家门店的数据而无需新系统
使用Airflow的第三个优势是其完整的用户界面
或UI 意味着Airflow拥有可视化仪表盘可查看和控制任务
就像为你的数据配备控制面板
任务中可以看到当前状态并从UI进行修改
你可以监控和排查工作流
可以突出显示后续将看到的工作流和任务之间的关系
可以在工作流之间创建依赖关系
可以通过内置性能指标识别瓶颈
还可以管理Airflow实例的用户和角色
当不同团队使用时这非常方便
还有很多 课程中还会展示UI的许多其他功能
使用用户界面的一个例子是这个
你有一个每日任务在UI中更新公司的报告
可以看到Toly的更新是否正在运行并检查上次成功运行时间
甚至需要时可以暂停或重启
好的 现在 使用Airflow的最后一个优势是其可扩展性
意味着可以轻松添加新功能或连接其他工具
就像智能手机可以下载新应用实现更多功能
可以为Airflow添加新能力或连接其他数据工具
通过使用提供者
提供者是包含与工具或服务交互功能的包
例如AWS
Snowflake 等等
课程后续会看到相关内容或可自定义用户界面
如果你想修改页面
可以做到
但同样重要的是你可以自定义现有功能
例如 可以在现有函数上添加抽象层以简化使用复杂度
Airflow可扩展性的一个例子是这个
你正在使用新云存储服务
即使Airflow本身不支持
你或团队成员可以编写代码连接Airflow
到该新服务
扩展其功能
无需等待任何人
你可以自行完成
添加到Airflow实例
就是这样
你永远不会被Airflow锁定
这就是关于Airflow的所有内容 什么是Airflow
它带来的好处
显然还有其他好处 但我认为这些是主要的
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/08_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p08 3. Core Components.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这段视频中 你将发现Airflow的核心组件
让我们开始探索 首先介绍元数据数据库
元数据数据库非常重要
因为这里存储了任务及其状态的信息
例如 如果任务处于成功状态
这将被存入Airflow数据库
它还记录工作流的所有关键细节
但不仅限于工作流
还包括用户、连接和变量信息
以及其他后续课程中会提到的概念
请记住这是关键组件
如果没有Airflow数据库
如果没有元数据数据库
就无法运行Airflow
这个元数据数据库可以是PostgreSQL或MySQL
Oracle数据库
等等可以将其视为Airflow的内存
就像你使用日历记住约会
Airflow通过元数据数据库记录任务运行时间及结果
第二个组件是调度器
调度器负责确定任务何时运行
确保任务按时按序执行
可以想象调度器是数据任务的闹钟
如果你设置任务每日上午九点运行
调度器会确保每天准时启动
显然调度器是Airflow的重要组件
没有它 无法调度和运行任务
除了调度器
还有DAG文件处理器和DAG文件
并将其序列化到元数据数据库
过去曾是调度器的一部分
但在Airflow 3.0
已成为需自行运行的独立组件以提升扩展性和安全性
可以将DAG文件处理器视为图书管理员
持续阅读并整理食谱书籍
解析每个食谱的步骤和依赖关系
在提供给厨房团队前
就像图书管理员将复杂烹饪指令转化为有序目录
厨师可轻松遵循DAG处理器
将Python工作流定义转化为结构化执行
接下来的组件是执行器
这也是个有趣的部分因为执行器决定任务如何运行
这一点很重要执行器不执行任务
而是定义任务的执行方式和系统
因此管理任务的执行过程
决定是按顺序执行还是并行执行，以及在哪个系统上运行
例如 你希望在kubernetes集群上运行任务
也许你并不了解kubernetes
但这没关系
可以将其视为一个大规模任务运行的专用框架
这种情况下你需要使用kubernetes执行器
根据目标系统不同，会有不同的执行器
将执行器视为交通控制器
就像交通控制器决定何时放行车辆以优化交通流
执行器决定如何运行任务以优化性能
下一个组件是API服务器
API服务器提供任务操作的端点
所以如果你想运行任务
如果你想停止任务等操作
同时它还提供用户界面
没有API服务器
就无法运行任务
也无法访问Airflow用户界面
负责处理执行器发送的任务操作并允许查看
管理 并通过浏览器监控工作流
稍后你将看到 可以将其视为汽车的仪表盘
就像通过仪表盘查看车速
油量和其他车辆状态
通过API服务器提供的Airflow UI查看任务状态
启动器 停止工作流并查看日志
接下来是步行器
步行器非常重要
因为这是任务实际运行的地方
步行器是真正执行任务的进程
如果Airflow是餐厅厨房
步行器就是厨师
接收订单
从队列中获取任务并执行
说到队列
这是下一个组件
队列是等待执行的任务列表
帮助管理任务执行顺序
尤其是在需要运行大量任务时
请记住 始终存在一个队列
可以是外部队列如RabbitMQ或内部队列
根据使用的执行器而定
想象繁忙咖啡店的排队场景
Airflow中的队列机制
同样确保任务有序处理
最后一个组件是触发器
所以触发器是一个特殊的组件
因为它负责管理可禁用任务
这些任务需要等待外部事件
明白了 因此它允许Airflow高效处理依赖外部因素的任务
不会阻塞其他进程或任务
想象一下你在等待重要邮件后再开始任务
触发器就像你的助手
它会持续监控收件箱并在邮件到达时通知你
让你可以同时专注于其他工作
这虽然更复杂但非常实用
如果你有需要等待一段时间的任务
并且希望在等待期间运行其他任务
这就是关于Airflow组件的内容
我知道可能有点多
但请记住你需要API服务器来运行任务
并访问Airflow用户界面
你需要调度器来安排任务
你的工作流 你需要元数据数据库存储工作流信息
用户数据 等等
然后你需要工作节点来运行任务
请记住执行器和队列是内部组件 当你运行Airflow时不会看到它们
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/09_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p09 4. Core Concepts.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 你将了解Airflow的核心概念
让我们深入第一个也是最重要的概念——标签
DAG代表有向无环图
而DAG是所有需要执行的任务的集合
以反映它们之间的关系和依赖性的方式组织
它帮助你定义整个工作流的结构
显示哪些任务需要先于其他任务执行
你可以将DAG想象成一份食谱
就像食谱按正确顺序列出制作菜肴的所有步骤
Airflow的DAG则按正确顺序列出完成数据工作流的所有任务
这是一个DAG的例子
你有四个任务
任务一 任务二
任务三和任务四
任务四依赖于任务一
任务二和任务三
因此你有这些定向依赖关系
并且你知道要执行任务四
需要先运行任务一 任务二和任务三
然后在右侧
这不是一个DAG
因为存在循环
可以看到任务四依赖于任务一
任务二、任务三，但任务一又依赖于任务四
因此形成了类似无限循环的结构
这不是无环图
好的 这就是无环的含义
你的图或DAG中没有任何循环
所以请记住
DAG是按关系和依赖性组织的任务集合
这就是你在这里看到的内容
第二个概念是操作符的概念
操作符定义了DAG中的单个
理想任务单元
重要的是你可以多次运行此任务
此操作符对于相同输入可重复执行任意次数
你将始终得到相同输出
无论何时运行
操作符允许你将工作流分解为离散的
可管理的工作单元
如果DAG像食谱
操作符就像食谱中的单个指令
就像食谱中'打五个鸡蛋'是一个步骤
数据操作符可能是从特定源提取数据的单个任务
Airflow拥有数千种操作符
例如 你可以使用Python操作符执行Python脚本
一个函数 你可以使用Bash操作符执行Bash脚本或注释SQL
执行查询操作符来运行SQL
向MySQL或PostgreSQL等数据库查询
然后你有文件传感器等待文件
这里需要注意
它不会执行任何操作
它在等待某个条件
所以略有不同
但现在你可能会想
我该如何在工作流中找到想要使用的操作符
好吧 猜猜看有一个专门的网站
就是这个网站
这里列出了所有提供者
记住提供者是一个包含函数的Python包
用于操作符与工具或服务交互
例如你想与
假设一个字节
然后选择字节
可以看到工作流中可用的不同操作符
这个网站非常实用
去看看吧 如果你想了解工作流可交互的内容
接下来介绍下一个概念
任务与任务实例的概念
任务是操作符的具体实例
当操作符被分配到DAG时
它就成为任务
就这么简单
因此通常我们称操作符为任务
任务是工作流执行的实际工作单元
继续我们的食谱类比
如果'打五颗蛋'是操作符
在七月打五颗蛋
上午两点的任务实例
是该步骤在特定时间的实际执行
最后一个概念是工作流
工作流是DAG定义的完整流程
包括所有任务及其依赖关系
代表整个数据管道
展示所有组件如何协同实现目标
你可以将工作流想象成准备餐食的全过程
涵盖从备料到烹饪到上菜的所有步骤
在Airflow中
你的工作流可能是每日销售报告
包含提取数据的任务
数据处理生成报告
并向相关方发送邮件
这就是一个工作流示例
这里包含四个操作符
所以四个任务
一个用于提取数据
然后处理数据生成
生成报告并发送邮件
你可以看到发送邮件依赖于生成报告
生成报告依赖于处理数据
而处理数据又依赖于提取数据
所以当你运行这个工作流
首先执行的任务就是这个
当这个任务开始运行时
它有一个日期
因此这个任务变为任务实例
现在所有这些对应一个DAG
DAG就是工作流
因此在课程后续部分
我们将使用DAG概念而非工作流
因为在Airflow中大多数情况下你会看到DAG
DAG是整体工作流结构
就像数据管道的食谱
然后操作符是单个任务
就像食谱中的一个步骤
然后你有任务和任务实例
操作符的具体执行，比如在特定时间执行步骤 而工作流是DAG定义的从开始到结束的完整流程
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/10_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p10 5. Airflow is not.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中我们将探讨Apache Airflow不是什么
以及哪些用例场景下Airflow可能不是最佳选择
首先我们来明确Airflow不是什么，它不是一个数据处理框架
Airflow本身并非设计用于直接处理海量数据
它不是一个编排工具
这是一个数据处理
然后Airflow也不是实时流处理解决方案
Airflow专为批处理和任务调度而构建
而非处理实时数据流
最后Airflow也不是数据存储系统
虽然Airflow使用数据库存储元数据
元数据库
它并不适合作为数据仓库或实际数据的数据库
我觉得这一点非常合理
好的 现在你已经了解了Airflow不是什么
接下来我们讨论Airflow可能不适用的场景
第一个场景是需要高频亚分钟级调度
例如 如果需要每几秒执行任务
Airflow并不适合此类高频调度
Airflow更适合分钟级间隔的任务
或小时、天级调度
另一个场景是需要直接处理大规模数据
如果需要在TB级数据上进行复杂计算
Airflow本身并非处理工具
Airflow旨在协调此类作业
而非执行处理
你可以用Airflow触发Spark任务
如果你不了解Spark
可将其视为可大规模运行的数据处理框架
而非在Airflow中直接处理数据
这一点需要牢记
然而
根据你的基础设施和计算资源
或许能在Airflow中实现较高规模的数据处理
但这另当别论
好的 第三个可能不适用Airflow的场景是
需要实时数据流处理
这一点其实很明确
因为Airflow无法实现
对于需要实时接收并处理数据的系统
比如实时交易平台
无法在Airflow中实现
Airflow基于批处理调度运行，不支持持续实时数据处理
然而 需注意你可以基于数据事件调度管道
根据数据事件调度DAG
但依然无法实现流处理
最后，如果你有一个简单的线性工作流且依赖关系很少
对于这种情况，Airflow可能有点过度设计
Airflow的强大之处在于管理复杂依赖和并行任务的简单工作流
最终简单的工具如cron任务可能已经足够
总结来说，应追求效率而非复杂性
Airflow并不适合大规模数据处理
即使你能处理数据
你也必须非常谨慎，因为它涉及许多因素
那么Airflow不适合实时流处理
你无法做到或数据存储
Airflow对高频子任务也不太适用
直接大规模数据处理 实时响应或过于简单的流程
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/11_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p11 6. The Different Architectures.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


有两种常见的Airflow运行架构
这就是你将在本视频中发现的内容
我们首先介绍单节点架构
首先我们需要明确单节点的含义
在此语境下，节点仅指单台计算机或服务器
单节点架构意味着所有Airflow组件都在同一台机器运行
这是安装和运行Airflow时的典型配置
好的 现在让我们探讨此配置中组件如何协作
第一个组件是API服务器，它有两个作用
首先是提供用户界面服务
没有API服务器就无法访问Airflow用户界面
同时它为其他组件提供任务操作端点
稍后你将看到这一点
接下来是调度器
调度器持续检查可触发的任务
调度器的一部分是执行器
请记住执行器属于调度器的一部分
定义任务如何及在哪个系统运行
由于我们处于单节点架构
将使用本地执行器
本地执行器允许在同一台计算机上并行运行多个任务
除了调度器
还有Airflow元数据数据库
该组件至关重要，存储环境相关所有元数据
例如 你的任务实例
你的DAG运行
Airflow实例的用户
等等
通常会使用PostgreSQL数据库
然后有一个或多个工作进程
请记住 这是任务实际运行的地方
在单节点架构中
工作进程是调度器的子进程以运行任务
请注意执行器不运行任务
工作进程负责运行
接下来是触发器组件
该组件较为特殊
因为它服务于不同操作符
特殊类型任务
只需记住它的存在
在单节点架构中若使用可变形操作符也会用到
最后一个也是重要组件是DAG文件处理器
DAG文件处理器解析和序列化工作流
你的DAG文件
通常这些DAG文件存放在dags文件夹
但也可能是Git仓库
现在所有组件都在同一台计算机运行
你可能会想知道它们是如何互相通信的
或者说哪个组件与哪个组件进行通信
好的，你有触发器和与API服务器通信的执行器
好的 然后还有与Airflow元数据库通信的API服务器
调度器同样与Airflow元数据库通信
当然还有暗文件处理器
因为DAG文件处理器会将工作流序列化到元数据库
所以你看触发器
和执行器从不直接与Airflow元数据库通信
出于安全和扩展性考虑，它们必须通过API服务器
现在你可能会问何时应该使用单节点架构
以及何时不应该使用
单节点架构非常适合入门Airflow或处理小型工作流
易于设置和管理，因为所有组件都在同一台机器上
然而 当需求增长且工作流更复杂或数量更多时
可能需要转向多节点架构以提升性能和扩展性
接下来 让我们了解Airflow的多节点架构
首先需要理解在此语境下多节点的含义
多节点指在多台计算机或服务器上运行Airflow
而非仅一台
这种设置用于处理更大负载或需要更高可靠性时
这是生产环境中确保性能的典型配置
扩展性与可靠性
好的 让我们了解多节点架构中组件如何协作
如你所见，这里有多个节点
多台计算机
A、B、C
D E和F
现在从API服务器开始，就像单节点架构一样
API服务器提供用户界面和执行器的端点
以及触发器与Airflow元数据库通信
可以看到这里可以有多个API服务器
可以部署多个API服务器
如果有大量用户访问Airflow界面
或需要运行大量任务的执行器
就能更好地处理所有请求
通过负载均衡器前置API服务器
当然由于有多个API服务器
如果其中一个宕机
仍可访问UI
执行器仍能访问API端点
如同单节点架构
我们还有元数据库
这里使用PostgreSQL数据库
当然你可以使用任意数据库
请记住，大多数情况下
出于可扩展性和安全性的考虑，你应该让数据库运行在专用机器上
除了元数据数据库外
你还有调度器
这里我们有多个调度器
这与单节点架构不同
你应该使用多个调度器
如果一个调度器完成任务
你仍有其他调度器继续安排任务
同时要注意执行器是调度器的一部分
这就是你可以看到的内容
调度器直接与元数据数据库通信
除了调度器之外
你还拥有DAG文件处理器
请注意，DAG文件处理器负责解析和序列化你的DAG
将Python文件存入元数据数据库
你可以有多个DAG文件处理器
如果你需要解析大量DAG
这是扩展Airflow实例的好方法
接下来你会有一个队列
这专用于序列化执行器
如果你想使用Celery集群运行任务
如果你不了解Celery
这是一个用于多机任务分发的Python框架
这里的队列是外部队列
该队列可以是RabbitMQ或Redis
稍后你会看到这个队列为何如此实用
但除此之外
你还拥有执行器
请记住，执行器负责运行任务
这里每个节点对应一个执行器
在节点D有一个执行器
然后是E和F，这些执行器会从队列拉取任务执行
队列会从执行器接收待运行的任务
最后 但同样重要的是，当任务完成后
执行器与API服务器通信
以更新Airflow元数据数据库中的任务状态
请记住
执行器从不直接与Airflow元数据数据库通信
这只是多节点架构的一个示例
Airflow组件的组织方式多种多样，最终取决于你的架构
你的预算
以及可用资源
这种配置有几个优势
首先 具备可扩展性
可以添加更多执行器处理更大负载
同时具备可靠性，若某台机器或组件故障
其他组件可接管其工作
在性能方面也是如此
效果更好因为你可以分布
跨多台机器操作
因此可以同时处理更多任务
然而 这种架构管理复杂度极高，可能对小型工作流过度设计
这种多节点架构非常适合大型组织的复杂工作流
需要高可靠性
或大量任务处理
它允许Airflow扩展以应对高负载需求
同时提供更好的容错性
这就是本视频的内容
在课程剩余部分 我们将继续使用单节点架构
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/12_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p12 7. How does it work.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中，你将了解Airflow如何运行任务
让我们开始吧，为了说明任务运行时发生的情况
我们将使用最简单的架构
如你所见，这里有不同的Airflow组件
例如API服务器
元数据数据库
执行器，它实际上是调度器的一部分
但为了清晰起见 我决定将其单独列出
然后你有调度器
DAG文件处理器和DAGs文件夹
首先你需要创建一个新的Python文件对应你的DAG
并将其添加到DAGs文件夹中
在这里你将新DPI添加到DX文件夹
在此过程中 DAG文件处理器持续解析DDAX文件夹
每五分钟检查新DAG文件
然后检测到文本文件夹中有新DAG
即DUG PI
此时DAG文件处理器将DAG文件序列化到元数据数据库
这种序列化的DAG表示将被两者使用
API服务器在Airflow UI中显示DAG代码
同时调度器检查是否需要运行任务
假设有一个任务准备运行
此时调度器创建一个DARK对象
你的DAG实例
默认情况下，当Airflow可以运行DAG时，对象状态变为待命
现在DARK运行对象处于运行状态
调度器创建对应待运行任务的任务实例对象
默认状态为已调度
然后调度器将任务实例提交给执行器
因为执行器定义了任务的运行方式和系统
现在任务实例对象处于队列状态
请记住，任务始终在队列中按顺序运行
队列可以是外部或内部
取决于你为系统设置的架构
例如 这里的队列是内部队列
这就是为什么你看不到它
但实际上它存在
让我添加队列以提高清晰度
在这里我们将添加队列
像这样，任务状态进入队列
好的 接下来发生的是Walker组件
执行任务的进程
因为执行器并不直接运行任务
将从队列中获取任务实例对象
我们在此创建新组件
这就是Airflow Walker
然后它从立方体中提取任务实例
一旦完成，任务实例即进入运行状态
假设任务成功完成
在这种情况下
walker将与API服务器通信
以指示任务实例当前状态为成功
API服务器将更新元数据库中对应任务对象的状态
这样调度器就知道任务实例已成功完成
因此深色运行也可以成功完成
好的 这就是其工作原理
请记住我们处于单节点架构中
所有这些组件
调度器
执行器二和walker都在同一进程运行
但在分布式架构中
队列会部署在不同计算机，walker也在另一台计算机
请记住dac文件处理器每五分钟解析dax文件夹
默认情况下针对新DAG文件
一旦有新DAG文件出现
调度器会检查是否有待运行任务
如有则创建一个DAG运行
然后任务提交给执行器
任务现在处于队列状态
walker将按顺序从队列中提取任务执行
然后 与API服务器通信以更新元数据库中的任务状态
当所有任务成功完成后
或者部分任务失败
无论怎样 调度器将更新DAG状态 状态可能是成功或失败
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/13_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p13 8. Installing Apache Airflow.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在本视频中，你将学习如何在电脑上安装Apache Airflow
让我们开始吧 好的
你的第一步是查看Udemy课程中的相关视频
你将看到一个需要下载的文件
所以请下载该文件
下载完成后你会得到一个名为docker-compose.yml的文件
将该文件复制到你的电脑中
下一步是创建新文件夹
我们将其命名为airflow_intro
然后进入该文件夹
然后在该文件夹中打开你的代码编辑器
我正在使用Cursor 这是Visual Studio Code的一个分支版本
但也可以选择使用Visual Studio Code
然后确保将文件docker
Compose tml放入该文件夹
再次按F intro
该文件是从Udemy视频下载的文件
我不会详细解释该文件的内容
但你可以快速打开它
你们都有所有的CVC
对应airflow组件的Docker容器，用于运行airflow
例如，这里你可以找到airflow元数据数据库
这是一个PostgreSQL数据库
同时你还有airflow调度器和DAG处理器
你可以找到你的DAG文件等
这就是我们用来本地运行airflow的文件
使用Docker来说
现在你在airflow介绍文件夹中找到了这个文件
下一步是设置一个安装了airflow的Python虚拟环境
我们这样做是因为当你编写第一个数据管道时会获得自动补全功能
当你开始编写你的第一个数据管道时
这将对你有很大帮助
因此要创建Python虚拟环境
我们将使用uv工具在终端中操作
为此你可以点击终端并新建终端
请确保你位于airflow intro文件夹中
然后运行以下命令
Uv valve dash
Dash python
然后在该Python虚拟环境中选择所需的Python版本
比如说三十一一版
如果你的电脑上没有安装这个Python版本
Uv会自动为你下载
那我们就按回车键
正如你所看到的 我有一个新文件夹dot
对应我的Python虚拟环境，其中包含Python三十一一
要激活这个Python虚拟环境
我需要运行以下命令
所以让我们复制粘贴到这里然后按回车
现在提示信息不同了
这表明你已成功激活Python虚拟环境
现在是时候在该Python虚拟环境中安装Airflow了
这非常简单
你输入uv pip install
然后apache airflow和你想要的版本，比如这里
三 哦像这样按回车
现在让我们等待一会儿直到Airflow安装完成
好的 Airflow已安装在Python虚拟环境中
最后一步是确认没有其他实例
在你的电脑上运行Docker
所以打开桌面并进入容器
确保当前没有运行任何与Airflow组件相关的容器
例如 如果你正在运行PostgreSQL数据库
那么端口5432会出现端口冲突
请确保没有其他Docker容器在运行
这些容器可能与Airflow实例冲突
也就是说
我没有其他容器在运行
所以我可以继续在终端运行命令docker compose app up
让我们等待一会儿
这需要比预期更长时间
因为Docker需要下载不同Airflow组件的镜像
但一旦准备就绪
你会看到类似包含Airflow的内容
只需稍等片刻
几分钟后你应该能看到类似Airflow的内容
一个已退出并返回代码0
这是预期结果
但查看Docker桌面
你应该能看到Airflow相关容器
对应不同Airflow组件的Docker容器正在运行
可能需要等待两分钟
但点击该端口后会跳转到以下页面
表明你已成功安装Airflow
现在它正在运行
快速提示 你可能会问
为什么我们用Docker运行Airflow
而我们已用uv安装了Airflow
比如 安装等操作
再次说明 记住在生产环境中实际运行Airflow应使用Docker
在现实世界中 你会这样做
你会使用Docker
你永远不会用peep安装airflow
然后在现实世界中手动完成所有操作
这就是我们使用Docker的原因
然而 因为我们使用uv在Python环境中安装了airflow
我们还能获得自动补全等功能
这样我们创建数据管道会更轻松
但最后一点，如果你想访问Airflow主页
你只需输入airflow作为用户名和密码 如果你按下回车 你将进入Airflow主页
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/14_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p14 1. The Home View.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中，你将发现Airflow 3.x的新主页
让我展示当你通过该链接访问Airflow用户界面时
你会停留在这个页面
这就是Airflow的新主页
首先让我们从页面顶部开始
这里显示了每个Airflow组件的健康状态
元数据数据库
调度器 触发器
以及DAG处理器
这非常有用，因为如果某些原因导致无法调度任务
嗯 可能是调度器出现了问题
此时调度器状态会显示为红色
在健康状态下方有相关链接
所以 例如 如果你想访问失败的DAG
请点击这个按钮
如果你想访问正在运行或活跃的DAG
请点击对应的按钮
如果你想知道运行中的DAG和活跃DAG的区别
活跃DAG是未运行但未暂停的任务
而运行中的DAG显然是正在执行的任务，链接显示历史记录
默认显示最近四小时的数据
但你可以选择自定义时间范围
将获取DAG和任务实例的历史记录
例如这里显示已有三个DAG执行完成
其中没有处于运行状态
但有一个处于成功状态
还有两个处于失败状态
关于任务实例
目前已执行九个任务
四个处于上游失败
三个成功，两个失败
这真的很实用
可以快速查看各DAG的状态分布
同样适用于任务实例
我真的很喜欢这个新主页
右侧显示资产事件
如果你还不清楚什么是资产
可以将其视为数据的逻辑分组
例如资产可以是文件或数据库中的表
当DAG生成数据时
会创建资产并触发资产事件
你可以在这里查看到
后续课程中会详细介绍资产
差点忘了
若要快速访问成功或失败的DAG
甚至查看上游任务实例
成败皆可渐隐
只需点击对应状态
所以 例如在这里
如果点击上游失败
则会跳转到该页面并自动
你将看到所有该状态的任务实例
所以再次 超级便捷
我爱Airflow的新主页 三点图标
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/15_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p15 2. The DAGs view.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在主页之后
我认为你会经常使用的一个视图是DDA视图
所以让我向你展示这个页面吧
正如你在页面顶部看到的
你有三个标签页
DAGs 任务运行和实例
现在我们将重点关注DAGs标签
在标签下方
你可以搜索DAGs
这非常实用
因为如果你有很多DAGs
猜猜看你可以快速找到你的DAG
你只需要输入DAG的名称
例如 用户
就这样你就能获取到DAG
用户位置 好的
这非常实用
显然在搜索栏下方有很多DAGs
你有不同的状态，这些状态很有用
因为它们允许你根据最新运行状态过滤DAGs
例如 如果你想显示最新运行成功的DAGs
点击成功后你只会看到我的DAG
然后回到全部
你会看到用户位置和用户
但由于它们未被正确触发
点击失败 例如
你将看不到任何DAGs
好的 再次说明
这些状态对应DAGs的最新运行状态
右侧显示所有启用或禁用状态
如果你想显示启用的DAGs
即准备就绪可调度
通过此处开启的切换按钮即可看到
或者如果你想显示禁用的DAGs
即切换按钮关闭且未就绪
你也可以这样做，最后
但同样重要的是，如果你想按标签过滤
当然可以做到
例如这里用户位置有两个标签
API用户同样适用于用户
让我们按API过滤
得到两个DAGs
用户位置和用户如预期
好的 千万不要忘记移除标签
因为很容易就会忘记这一点
然后你会疑惑为什么看不到其他袋子
你可以自行决定如何展示你的dags
默认情况下你的dags会以平铺方式显示
但你可以使用列表视图
所以在这里 只需按你的偏好设置
同时你还可以按最新运行开始日期排序
或按显示名称排序
或按下次运行时间等再次选择你喜欢的排序方式
下方是你的不同dags
让我们看看这个
这是你的dag的唯一标识符
或者可能是显示名称
如果你在dag中使用了显示名称参数
下方是调度信息
例如这里 你知道这个任务每天午夜运行
这里有最新的运行记录，日期和状态，以及下次运行
所以下次这个dag运行时
右侧还有一些条形图
每个条形代表一个dagan
当你悬停在条形上时
可以看到额外信息
比如该特定dagan运行后的状态
开始 日期和日期
这些是时间戳
以及持续时间 或者条形越高
说明该dagan完成所需时间越长
如果你想访问其中一个dagan运行记录
嗯 只需点击你想要的那个
当前有切换按钮
表示dag已准备好调度
但如果你想暂停它
只需点击该切换按钮
现在已暂停
然后你有触发按钮
如果你想手动触发dag
点击这里即可
还可以传递一些高级选项
例如 你希望触发dag的逻辑日期
运行ID 通常不需要手动指定运行ID
因为它会自动生成
但你可以自行指定，最后
但最重要的是为你的DAG设置一些参数
还有一些注意事项
你可以为该特定品牌添加这些说明
然后点击触发
好的 这就是该标签页的内容
显然你会经常使用它
因为这是监控DAG的主要方式
或者至少能快速找到目标DAG
在DAG旁边
你有租金信息
或者实际是DAG运行记录
你可以按状态筛选
例如查找处于待定、运行中、失败或特定状态的运行记录
你可以这样做 你也可以按运行类型筛选
如果你想了解哪些DAG任务是手动执行的
则点击手动
如果你想查看哪些是定时调度的
则点击定时
这里显示任务实例列表
这里显示DAG ID
这是对应当前DAG的ID，我的DAG
然后是成功状态的运行记录
失败和成功
可以看到这里有两份任务处于失败状态，一份成功
运行类型中两份是手动触发
而只有一份是定时执行
还有开始和结束时间
每个任务的持续时间
以及对应DAG版本的任务执行记录
V1版本 好的
如果你修改DAG并重新运行
会看到新的任务实例
这次版本号为V2
这里有两个操作按钮
一个用于清除任务
若需重新运行该任务
可点击此处执行
若需标记该任务运行失败
同样可以操作
或标记为成功
好的 这是一个简洁的视图
但非常实用，后续可查看所有DAG运行记录
接下来是任务实例
基本相同逻辑
但针对具体任务，获取任务实例列表
显示DAG ID 所以你知道这个任务属于我的有向无环图（DAG）
然后你有这个图表
所以你知道这个任务
例如已为此DAG运行执行过
然后你有任务ID
这是任务的名称或唯一标识符
在这里是c
然后是任务状态成功
在这种情况下到结束日期
映射索引
这将被填充
如果任务是动态生成的
这里并非如此
然后是尝试次数
该任务尝试了多少次重试
这里仅一次
然后是任务使用的操作符
这是Python装饰器操作符
这是任务装饰器
然后是任务时长及对应的任务执行版本
在这里仍是v1
如果你修改DAG
那将是v2
你有两个操作
如果你想重启该任务
你可以点击此处操作
如果你想标记为失败
你可以同样操作成功标记
但最重要的是你可以在此输入任务名称搜索特定任务
或按状态
再次选择你想要的状态
这个标签非常实用
因为你可以列出并执行Airflow实例中部分任务的操作
好的 这就是关于这些标签和DAG的内容 我确定你会大量使用它
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/16_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p16 3. The Asset view.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


有一种观点认为你会在f three x中越来越多地使用
这就是资产视图
让我详细介绍一下
也许你还不知道什么是资产
可以将其视为数据的逻辑分组
例如CSV文件或数据库中的表
顺便说一句 如果你之前使用的是airflow three o，资产此前被称为数据集
好的 如果你之前使用过数据集
嗯 猜猜什么是资产
相同概念 只是名称不同
正如你在这里看到的
我们有一系列资产到资产
用户和用户位置
然后你知道用户属于组资产
确实可以为资产创建组
如果你有多个可以相互关联的资产
则可以创建组来更好地组织它们
你也知道这个用户集被此DAG消费
即用户位置
好的 这是DAG的名称
这基本表明用户位置与用户之间存在依赖关系
因为用户位置消费了用户
然后你有任务
即生成该资产的任务
正如你在这里看到的
任务是用户最后
但最后还有一个按钮
如果你想创建资产事件
当你点击它时
有两种选择
要么物化资产或仅创建资产事件
两者之间的区别很重要
当你物化资产时
实际上会触发该资产背后的DAG
好的 因此物化资产以在后台创建资产
资产是一个包含单个任务的DAG
因此你需要运行包含单个任务的DAG来物化资产
具体来说 如果资产是通过从源提取数据生成的文件
你需要执行该逻辑以获取文件
因此你的资产
但现在假设你不这样做
你只想创建资产事件
并模拟资产已被物化
那么在这种情况下你选择手动
这可能很有用 如果你有一些下游资产或DAG依赖该资产
并且不想再次物化它
因此在这种情况下使用手动选项
正如你看到的，手动选项
可以传递一些额外参数
这些参数可以在下游资产或任务中获取
最后创建一个事件
这将触发下游依赖
好的 这就是资产列表的工作原理
如顶部所示
你可以搜索特定资产
如果你有很多资产
这非常方便 但更重要的是
点击用户资产
例如你会进入这个视图
这个视图类似于DAG的图形视图
但对于资产而言
这里有两个内容
首先是包含一个物化资产任务的DAG
例如这里显示的用户DAG物化用户资产
然后是用户位置DAG物化用户位置资产
这就是为什么这里有切换选项
因为资产可以按特定调度间隔物化
好的 如果你想每天午夜物化资产
可以这样做
这就是为什么这里有切换选项
因为资产由dx后台生成
现在需要记住的是用户位置资产依赖用户资产
一旦用户资产被物化
就会触发用户位置DAG
物化用户位置资产
课程后续会更详细讲解资产
如果还不完全清楚
不用担心 后续有充足时间实践资产
右侧显示根据所选资产
你将看到相关信息
所属组别 生产任务和消费DAG
下方是资产事件
每次创建资产事件时
或资产被物化时
这里会显示事件
这就是该视图需要记住的内容
课程后续会深入讲解资产
请记住这个视图是完美的
查看您的资产之间的依赖关系 并创建触发下游dags或下游资产的资产事件
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/17_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p17 4. Additional Important views.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我想向您展示一些我认为重要的额外视角
所以让我来展示给您看
我想向您展示的第一个视角是x comes you
如果您去浏览 然后x comes到这里
您将获得Airflow实例中的x comes列表
也许您还不知道什么是x come
现在暂且将其视为一个包含值的小盒子
这个盒子由您的任务创建
因此可以通过创建x comes在任务间共享值
您有一个任务创建了包含值的x come
另一个任务可以拉取该x come获取值
例如这里您有键
这是该xcom盒子的标识符
然后是生成该xcom的dag
接下来是运行ID
这是创建xcom的dag运行的唯一标识
然后是任务ID
这是创建该xcom的任务名称
接下来是映射索引
这与高级概念动态任务映射相关
但此处我们不使用该功能
因此显示为-1和最后
但最重要的是该xcom的值
这意味着您希望与其他任务共享值一
别担心 稍后课程中您会大量看到x comes
除了x comes，如果您进入管理页面还有变量
变量在需要跨多个dag共享值时非常有用
因为您可以从任务中提取变量
例如您有一个需要共享的api端点
然后创建一个变量
我们称之为api端点
然后在此处输入所需值
然后点击保存
现在您已拥有api端点变量
该变量将具有以下值，可在所有dag中使用
如果您要编辑变量
可以点击该按钮
如果您要删除它
点击确认按钮
所以变量
概念简单但非常实用
接下来是提供者
提供者非常重要
每次需要与外部工具交互时
外部服务
都需要安装对应提供者
提供者是一个python包
可扩展您的Airflow实例功能
因此你可以与某些特定服务和工具进行交互
例如apache airflow providers包
亚马逊允许你与亚马逊云服务进行交互
例如Redshift等
你可以看到该提供者的版本，再次说明
每当你要与外部服务或工具交互时
必须安装对应的提供者，也就是说要与工具或服务交互
你需要创建一个连接
如果你前往管理-连接
然后添加新连接
你可以看到这里
连接ID 这是连接的唯一标识符
例如a
但现在要与aws服务交互
你需要选择连接类型
如果你看不到连接类型
Aws
这意味着你未安装亚马逊提供者
好的 再次说明 如果你看不到对应服务或工具的连接类型
这意味着未安装对应提供者
这就是为什么提供者非常重要
所以选择a即可，就像这样
你会获得需要填写的特定字段
用于你想要交互的服务地址
目前我们不创建任何连接
但不用担心 课程后续会创建
最后点击用户
有一些实用选项
例如 如果你想切换到浅色模式
可以这样做
我更喜欢深色模式
但由你决定
你也可以默认切换到图视图
点击dag时
这很酷 如果你偏好图视图
还可以更改UI显示的时间
如果需要的话
这就是airflow
UI的相关内容，现在你知道最重要的视图
当然还有其他视图 课程后续会介绍
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/18_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p18 1. The Project.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


我非常兴奋在这里见到你
因为这就是你将在下一视频中构建的数据管道
所以让我告诉你更多细节
好的 你将开始在PostgreSQL数据库中创建一个表
因此你将学会如何在数据管道中与PostgreSQL数据库交互
但更一般地说，你将学会如何使用Airflow与任何数据库交互
你将看到如何检查
通过特殊任务判断某个API是否可用
然后从该API中提取用户数据
并将处理后的用户数据最终存储回PostgreSQL数据库
在这个项目中你将全面了解所有内容
你需要按照最佳实践在Airflow中创建基础但强大的数据管道
所以我希望你已准备好迎接这个项目 我们下一视频见
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/19_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p19 2. Advices.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


为了确保你能尽可能享受这个项目
我想给你一些建议
第一条就是慢慢来
好的 这不是比赛
所以慢慢来
不要犹豫先看完整个视频
然后按照我在电脑上演示的步骤操作
此外对于项目中每个你完成的任务
你都能找到对应的代码和另一个视频
所以 例如 如果你正在实现extract_user视频中的用户提取功能
你会找到代码
所以如果遇到任何困难
只需下载代码
然后你可以将自己的代码与我的进行对比，最后
但至少要记住，如果你是Airflow新手
你看到的内容可能会有点难
如果你有任何疑问
请随时在Udemy问答区提问
我在这里帮助你
我很乐意帮助你
说到这里 那我们就继续项目吧
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/20_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p20 3. Define a DAG.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在Airflow中创建数据管道的第一步是定义你的dag
让我来演示一下吧
首先需要在dags文件夹中创建一个新的python文件
请记住dax文件夹包含你的dag文件
因此你可以在此处的dax文件夹中创建新文件
并将其命名为user_processing.py
一个dag文件就是一个python文件
接下来你总是需要先进行导入操作
首先需要导入的模块是dag装饰器
因此输入from airflow.dot.sdk
然后导入dag装饰器
这就是你将用来定义dag的工具，继续之前请记住
确保在代码编辑器底部
你使用的是正确的python解释器
如果你点击这里
然后此处应显示使用.dot.venv/python
这是之前安装的python工具环境
如你所见
好的 确保使用该python解释器
否则 可能会出现警告提示
好的 下一步是使用装饰器
在此处使用装饰器并在下方
由于这是一个装饰器
需要定义一个python函数
例如user_processing
这个python函数就是你的dag
user_processing是dag的唯一标识符
必须在整个项目中保持唯一
例如 顺便说一句
如果你不了解python装饰器
强烈建议你上网查阅相关资料
因为这个概念非常重要
我们在课程中会大量使用装饰器
但如你所见
python装饰器是一个接受函数作为参数的函数
在不修改原函数的情况下扩展其行为并返回修改后的函数
本质上我们正在改变经典python函数的行为
使其成为Airflow中的dag
接下来在该函数内
我们将定义任务
但稍后会进行
暂时先写pass占位
最后必须调用user_processing
否则dag不会显示在Airflow UI中
好的 请务必记住
如果你不将其调用为用户处理函数，因为这里它是Python函数
你将无法在Airflow UI中看到该DAG 就是这样 这就是在Airflow中定义DAG所需的一切
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/21_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p21 4. Create a Table.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中，你将创建你的第一个任务
让我们开始吧 你将要添加到数据管道的第一个任务是创建表
这个任务将用于在PostgreSQL数据库中创建表
为了做到这一点，很长时间以来
你必须使用PostgreSQL操作符
请记住你在课程中之前看到的内容
操作符是预定义任务的模板
在我们的情况下 因为我们想要在PostgreSQL数据库中创建表
我们需要使用postgreoperator运行SQL查询
现在这已经过去三年了
因为postoperator已被弃用
你应该改用SQL执行操作符
实际上对于任何SQL数据库
你应该使用SQL操作符
好的 所以不要使用特定于数据库的操作符
回到数据管道使用SQL操作符
使用SQL执行查询操作符的第一步是导入它
因此你需要从airflow导入以下内容
提供程序模块然后是sequel操作符
Sql并导入sql执行查询操作符
好的 一旦导入完成
你就可以使用它了
所以复制sql执行查询操作符
然后我们移动
传递并创建一个新任务以创建新任务
你需要创建一个变量来保存该操作符
所以这里假设create_table等于sql执行查询操作符
然后你需要定义一些参数
第一个是任务ID
该任务在DAG中的唯一标识符
此外任务ID也是你在UI中看到的任务名称
无论使用哪个操作符
你始终需要为任务定义任务ID
除了任务ID之外
还有一些特定于SQL执行查询操作符的参数
第一个参数当然是SQL查询
你想要在PostgreSQL数据库中执行的查询
这个SQL查询如下
所以这里我们只需创建一个表
包含id字段
姓名 邮箱
以及created_at字段，该字段自动设置为当前时间
让我们稍微修改这个查询
因为我们还需要包含firstname和lastname
好的 现在我们已经准备好SQL查询了
SQL执行查询操作符的最后一步是定义连接ID
你还有一个名为cn_underscore_id的参数，需要输入字符串
这个字符串是连接到PostgreSQL数据库的唯一标识符
而这个连接是Airflow连接
因此你需要创建该Airflow连接
以便SQL执行查询操作符可以与PostgreSQL数据库交互
这就是接下来视频中要展示的内容，暂且到这里
别忘了在末尾添加括号
就这样 你已成功创建了第一个在PostgreSQL数据库中运行SQL查询的任务
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/22_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p22 5. Create a Postgres connection.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在让我展示如何在Airflow中创建连接
好的 请记住SQL执行查询操作符会与SQL数据库交互
在这种情况下我们需要与PostgreSQL数据库交互
因此显然需要连接该数据库来运行此SQL查询
这时就需要创建Airflow连接
每当需要与外部工具或服务交互时
通常需要创建Airflow连接
在这种情况下 我们需要创建的Airflow连接名称为postgres
如连接ID参数中定义的
所以让我们操作，如果你访问UI界面在本地主机
输入8080端口这里需要填写用户名airflow
然后密码airflow以访问Airflow主页
从那里可以进入管理页面然后选择连接
在这里你可以创建相应的连接
只需点击以下按钮
然后需要指定连接ID
请记住连接ID是连接的唯一标识符
不能有两个相同ID的连接
此处连接ID为postgres
如SQL查询操作符的ID参数定义
然后需要选择连接类型
我们选择PostgreSQL
接下来需要填写一些字段
首先是描述
我建议为连接定义有意义的描述
例如这里这是Airflow数据库
然后填写主机地址
主机为postgres
登录名是airflow
密码同样是airflow
端口为5432
完成这些后
点击保存
已成功创建新连接
每次需要与PostgreSQL数据库交互时
必须在操作符中使用连接ID 用于获取连接并连接数据库
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/23_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p23 6. What is a Provider.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在之前的视频中，你已经创建了你的第一个Airflow连接
但有一件事我想告诉你
这非常重要
所以如果你添加了一个新连接
然后指定连接ID
当你查看连接类型时
想象一下你无法在这里找到所需的连接类型
比如说你想与Elasticsearch交互
但在这里找不到Elasticsearch
那你应该怎么做呢
如果你找不到所需的连接类型
这意味着你尚未安装对应的Airflow提供程序
请记住Airflow是模块化构建的
这意味着Airflow的核心提供核心调度功能
允许你编写一些基础任务
但通过安装称为提供程序的额外包
可以扩展Airflow的功能
例如访问io注册网站
我建议你这样做
因为这能节省大量时间
假设你想与Byte或类似服务交互
比如可能用Flink
然后选择Flink Airflow提供程序
然后你会在右侧看到如何安装该提供程序
通过安装该提供程序
你将获得连接到Flink所需的连接类型
同时该提供程序还包含操作符
所以你可以看到这里有两个额外的操作符
请记住
Airflow拥有许多提供程序
如果你想与特定工具或服务交互
通常需要安装对应的Airflow提供程序以获取连接类型 操作符等
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/24_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p24 7. The secret weapon!.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


每当向数据管道添加新任务时
始终建议先进行测试
这就是我现在要展示的内容
好的，在之前的视频中你已经创建了第一个任务create table
使用了SQL执行查询操作符
现在是时候验证该任务是否正常运行
为此你需要在Airflow命令行界面使用特殊命令
但首先让我们访问Airflow命令行界面
前往Docker next up
然后在容器中展开你的compost堆栈
应该是airflow intro
然后选择调度器
接着进入exec并在此输入/ bin / bash
如提示所示
你现在位于运行Airflow调度器的Docker容器中
现在如果输入airflow
你应该看到类似这样的内容
这确认你已访问到Airflow命令行界面
用于测试任务的命令如下
是airflow
然后tasks tests
接着是包含待测任务的DAG名称
所以是user_processing
然后是待测任务的任务ID
即create_table
完成这些后
你可以执行并稍等片刻查看任务是否运行
如日志底部所示，任务标记为成功
表示任务已成功执行
此命令非常实用用于验证
所添加任务是否正常运行无需执行整个DAG
这样测试任务更便捷高效
同时不会存储待测任务执行的相关元数据
保持数据库整洁
本视频到此结束
现在你知道如何访问Airflow终端界面
当Airflow在Docker环境中运行时
同时也学会了使用airflow tasks测试任务 test
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/25_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p25 8. Is the API available.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这段视频中，我将向你展示如何验证
你尝试访问的API是否可用
所以让我们开始吧，为了项目的进展
我们将使用一个假API
相反，我们将使用以下链接
这个返回对应假用户的JSON值的链接
但请记住，接下来你看到的内容同样适用于真实API
好的 这里并不重要，只要我们能从URL获取数据即可
现在问题来了
如何验证该URL是否可用
是否返回预期结果
让我们来验证
该API是否可用
有很多不同的方法可以做到这一点
但我要在这里展示的方法是使用传感器
传感器是一种特殊类型的运算符
允许你在执行下一步任务前等待条件成立
例如
如果你在等待某个文件
或等待API可用
你可以使用传感器
传感器会定期检查时间间隔
判断条件是否成立，如果条件成立
传感器将成功
如果不成立 一段时间后传感器将失败
现在我将展示如何在Airflow中实现第一个传感器
第一步是导入另一个装饰器
除了暗装饰器之外
导入任务装饰器
然后在SQL执行查询运算符下
需要导入PokeReturnValue类
为此，输入from airflow sdk bases sensor import PokeReturnValue
不用担心 很快你就会明白为什么需要PokeReturnValue
现在你有了任务装饰器
在创建表之后
可以创建一个新任务来验证
该API是否可用，为此
使用任务装饰器
然后使用该传感器来表明你装饰的Python函数
现在将成为一个传感器
任务到底是什么
任务是一种对应Python运算符的装饰器
基本上，如果你想执行Python代码
将使用@task
当我们说@task.sensor时
表示我们装饰的Python函数是一个传感器
或一个等待条件成立才能成功的空间运算符
让我们创建一个与任务对应的Python函数以验证
检查API是否可用，所以API可用吗
然后我们向以下链接发起请求
这是对应的一个假API
我们可以打印响应的状态码
如果状态码是200这样的
这意味着条件为真
否则条件为假
但最重要的是，这个Python函数是一个传感器
你需要返回一个特殊对象，即poke返回值
所以这里返回poreturn值
这里需要设置一个参数，即is_done
然后传递条件变量
基本上如果条件为真
则传感器成功
如果条件为假
传感器将重试直到失败
此外 由于我们返回的是poreturn值对象
需要在这里标明poke返回值
由于我们在此处调用API进行验证
检查API是否可用
我们还需要获取后续项目中所需的假用户
这样在下一个任务中无需再发起请求
我们将实现 只需在此处获取用户并返回
下游任务即可使用这些数据，非常简单
只需在此处创建另一个变量
例如fake_user等于响应转JSON
然后fake_user等于节点
然后传递到xcom值参数
假用户和最后
但最重要的是 别忘了在任务中导入requests
否则会在此处报错
就这样
你已成功实现第一个传感器
现在有两个参数我强烈建议在需要传感器时使用
第一个是book_interval参数
定义传感器验证条件的时间间隔
例如此处设置30秒
则表示传感器每30秒检查一次
该条件
然后应定义超时时间
传感器在此后失败的时间
因为条件仍为假
因此可设置300秒（5分钟）
就这样，这就是我认为必须定义的两个参数
使用传感器时
总结你到目前为止完成的内容
你已创建了一个新任务
易普传感器可用
这是一个使用传感器装饰器的传感器
你已定义每三十秒检查一次条件
五分钟后传感器将失败
如果条件仍为假
该条件如下
你向假API发起请求
如果API返回100
因此如果API可访问
此时条件为真
然后在以下变量中启动假用户
否则条件为假
传感器将在三十秒后再次尝试
但最后你返回包含条件的poreturn值对象
如果条件再次为假
传感器将再次尝试
如果条件为真
传感器将成功
并按xcom值参数定义跳转假用户
由于使用了装饰器
需要调用api可用函数
否则无法在UI上看到该任务
和之前为DAG做的完全一样
所以这里 调用api检查
现在进行验证
如果任务正常工作，请返回Airflow命令行界面
然后运行命令airflow tasks tests
然后执行daid用户处理
任务ID为api可用
然后回车并等待片刻
可以看到它正常运行
恭喜 此时你已成功实现第一个任务创建表
以及第二个任务pi可用
并掌握了如何与PostgreSQL数据库交互
如何创建Airflow连接
以及如何创建传感器进行验证
在下一视频中将学习如何定期检查条件是否为真
下一视频将展示如何从获取的数据中提取用户
来自pi可用任务 下一视频见
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/26_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p26 9. Other useful Sensors to know.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在之前的视频中 你看到了在Airflow中使用传感器最强大且灵活的方式
为什么说灵活呢
因为有了这个传感器
你可以完全按照自己的需求实现传感器功能
因为最终运行的是Python代码
因此你可以定义所需的条件
现在Airflow中还有许多预定义的传感器可供使用
例如 如果你前往注册表
你会看到HTTP传感器
HTTP传感器允许验证URL是否可用
这几乎就是我们之前做的
但这次你使用的是预定义的传感器
因此无需自行编写代码
此外 你可能需要文件传感器
文件传感器可检查文件系统中特定位置的文件是否存在
或者你可能需要使用S3密钥传感器
在这种情况下 你想要验证文件是否存在于存储桶中
所以你看，有许多传感器可用
许多预构建的传感器可在Airflow中使用
在我看来 使用传感器装饰器可能更好，因为它提供了所需的所有灵活性
但如果你不太在意这些，想要节省时间
那么寻找所需的预建传感器是个好主意
关于传感器的内容就到这里
我们下次视频再见 在接下来的视频中你将从API提取用户
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/27_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p27 10. Extracting users (the classic way).ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


此时你已成功创建了两个任务
创建表和easy pi现已可用，现在是时候创建第三个任务了
也就是提取用户信息
我们开始吧，好的
请记住API返回以下虚拟用户
我们关注的是名字
姓氏
以及邮箱和ID
这就是我们想要从数据中提取并返回数据管道的内容
为了提取所需的虚拟用户信息
我们将运行一些Python代码
要在Airflow中执行Python代码
你需要使用Python操作符，只需在下方导入返回值
让我们导入Python操作符
这里输入from airflow
然后providers点stout
接着operators
然后导入Python操作符
拥有Python操作符后
可以在DAG文件末尾，在api可用之后
然后创建新变量
命名为extract_user，使用Python操作符
由于它是操作符
需要定义任务ID
我们使用extract_user作为任务ID
需要指定Python调用函数
要调用的Python函数
执行所需的Python代码
这里应为_ extract_user
定义该Python函数_extract_user
现在回到DAG文件顶部
在此处定义extract_user函数
形参为task_instance对象
稍后会解释原因
现在在该Python函数中
需要获取前一个任务is_api_available的数据
记住is_api_available
返回以下虚拟用户
这就是要获取的数据
要获取数据需使用task_instance对象
该task_instance对象是任务实例
可访问所需方法
获取前一个任务的数据
请注意ti是Airflow保留关键字
现在使用ti访问xcom_pull方法
需传递任务ID
此处为is_api_available
然后打印从easy_pi获取的虚拟用户
让我简要解释一下
xcom是Airflow中任务间共享数据的机制
你正在做的事情是拉取任务创建的xcom，即我可用
其中包含伪造的用户
请记住api是否可用
你返回伪造的用户
通过返回伪造用户
你创建了一个包含伪造用户的xcom
而该xcom存储在airflow元数据库中
因此你可以看到下一个任务作为包含所需值的方框
默认情况下它存储在airflow元数据数据库中
所以这里你返回伪造用户
因此你创建了一个包含伪造用户的xcom
此xcom存储在确认数据库中
然后在下一个提取用户任务
你正在拉取由api可用性创建的xcom
其中包含来自afl数据库的伪造用户
然后在这里你可以打印它
如果你对xcom的一切都不太理解
别担心 后续课程会有专门视频讲解
在课程中 好的 现在我们有了伪造用户
我们想要提取所需的信息
所以id 名字
姓氏、邮箱
就是这样
所以这里我们来做
我们将返回对应用户数据的字典
在此我们需获取此处显示的id
然后在个人信息中获取名字
所以这里输入名字
然后从伪造用户提取
个人信息
然后获取名字如这样
我们同样处理姓氏
所以这里姓氏
然后伪造用户个人信息
但这次我们需要此处定义的姓氏
最后还需要个人信息中的邮箱
我们可以复制这一行
然后在此处输入邮箱并添加邮箱
好的
就这样
我们成功从伪造用户提取所需信息
差不多就是这样
此时 你已成功创建提取目标用户信息的任务
并将数据存储到后续表中
现在是时候验证了
如果任务执行到这里
这里需要做些什么
因为用户需要提取可用的数据返回
不能像之前那样使用airflow任务测试
因为当你运行airflow任务测试时
不会创建任何x变量
所以要验证任务是否有效我们需要稍作调整
基本上我们要复制这一行
实际上这两行
然后在提取用户处粘贴类似代码
并在此处注释掉虚拟用户变量
而是使用虚拟用户
带响应点j
我们在这里的操作是
仅为了测试目的
直接从api获取数据
因此无需依赖
此处的x变量
现在验证任务是否运行
保存文件 然后在airflow调度器终端
运行与之前相同的命令airflow tasks test用户处理
但这次要运行提取用户
然后进入并查看
好的 看起来成功了，任务标记为成功
但更重要的是可以看到任务返回的对象
以下的python字典
包含正确的id
正确的名字
姓氏和邮箱如预期
恭喜
现在你知道如何使用python操作符运行代码
并使用xcom共享数据
通过xcom池方法
这是在airflow中运行python代码的一种方式
在下一个视频我会展示我的首选方法 我们下期再见
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/28_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p28 11. Extracting users (the better way).ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的，在之前的视频中
你已经使用Python操作符创建了提取用户任务
但如果你还记得课程中我之前提到的
任务装饰器就是Python操作符
现在让我展示如何用任务装饰器完成同样的操作
使用任务装饰器
你可以移除Python操作符导入
不再需要了
因为任务装饰器本质上就是后台的Python操作符
然后复制所有内容
接着移除使用Python操作符的提取用户部分
然后将函数粘贴到这里_提取用户
确保缩进正确
然后移除这里的下划线
我们也移除ti变量
然后在该函数上方使用@task装饰器
就是这样
你已成功创建了与之前完全相同的任务
但使用了任务装饰器
任务装饰器的优点在于
你可以像使用Python操作符一样操作
但代码更简洁如你所见
并行池部分因为不再需要
相反 你的函数将接收一个参数
对应从上一步任务获取的数据
在这种情况下
用户
因此也可以移除这部分
以及这里的ii_available返回所需伪造用户
只需像这样写fake_user
等于api_available
然后调用extract_user时
只需传递fake_user
因为现在使用任务装饰器
需要显式调用函数
否则无法在UI中看到任务
如你所见非常简洁
我清楚知道api_available返回用户
然后fake_user用于extract_user返回所需数据
这里可以像这样写user_info等于extract_user
说实话
我强烈建议始终使用任务装饰器
每当需要运行Python代码时
在我看来 编写、阅读和维护都更高效
总之 我看不到使用任务装饰器的缺点
相比Python操作符
现在你已经成功使用任务装饰器实现了提取选择器
下一视频见 接下来的任务
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/29_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p29 12. Processing users.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在你已经成功实现了提取用户功能
现在是时候学习如何实现处理用户功能了
这将会非常简单
回到我们的数据管道来实现处理用户评分任务
我们将使用任务装饰器，就像处理提取用户任务时那样
但我想让你自己实现这个任务作为练习
所以任务应该命名为处理用户评分
该任务需要一个包含以下字典的用户信息参数
然后你的目标是将该字典存储到CSV文件中
命名为用户信息.csv
该CSV文件必须存入tp文件夹
好的 试着完成这个任务
暂停视频 你已经具备完成该任务所需的一切
稍后我会看到你们
好的 让我展示如何在提取用户之后实现处理用户任务
使用任务装饰器创建任务
然后定义处理用户函数，接收用户信息参数
然后需要导入csv库
这可以操作CSV文件
接着只需使用with open
然后是用户信息.csv
确保路径是gmp文件夹
同时传递正确的模式和newline参数为空
并将文件赋值给f变量
传入文件和字段名
这里对应id
名字 姓氏和邮箱
我们在CSV文件中使用这些字段的原因
是因为它们需要与用户表的列对应
完成这一步后
下一步是写入正确的表头
然后像这样写入行
就这样
你已成功实现处理用户功能，将用户信息存入CSV
将从提取用户获取的数据存入CSV文件
用户信息将存入gmp文件夹
最后你需要在提取用户之后显式调用处理用户
并像往常一样传递来自提取用户的用户信息
我们需要验证任务是否正常运行
但请记住不要运行其他任务
然而处理用户需要来自提取用户的数据
因此我们将再次模拟用户信息对象
你可以直接复制这个字典
然后在这里创建变量
用户信息等于该字典
在这里我们可以随便放一些假数据
比如说一、二、三、四
然后第一次绘制数据
然后姓氏部分填写do
然后这个邮箱
好的 这样应该足够了
保存文件 现在去终端对应的airflow调度器目录
运行之前相同的命令
但这次我们要运行process_underscore_user
按回车并稍等片刻
好的 可以看到它成功运行了
我们可以通过文件标签验证
然后前往此处的/slash tp并打开该文件夹
应该能看到用户信息
用文件管理器打开该文件
可以看到这里的数据列
Id 名字 姓氏
以及邮箱 还有预期的数据
恭喜
到此为止 你已成功创建了一个从上一任务获取数据的任务 并将数据存储到本地文件系统的csv文件中
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/30_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p30 13. Storing users.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 现在你已经成功创建了进程用户
现在是时候创建最后一个任务，即存储用户
让我们深入实现存储用户功能
我们需要一个你还不熟悉的Airflow新概念
这就是钩子（Hook）的概念，所以钩子
H o o k 是与特定工具或服务交互的抽象接口
例如
如果你之前使用过Postgres操作符
Postgres操作符依赖于Postgres钩子
在Postgres操作符下
你有Postgres钩子来与Postgres数据库交互
同样的，如果你使用Redshift操作符与Amazon Redshift交互
在这种情况下，在操作符下
你有Redshift SQL钩子
这允许你与Amazon Redshift交互
请记住，每当通过操作符与工具或服务交互时
该操作符依赖于一个钩子
这是一个与特定服务或工具交互的抽象接口
使用钩子的好处是你可以访问
操作符未提供的方法
所以 例如 在Postgres钩子中，你可以访问
Copy专家和Copy专家
允许从文件复制数据并加载到对应表中
让我先在最后的导入返回值下展示
执行以下导入：airflow.dot.providers
Dot postgres hooks postgres
然后导入Postgres钩子
如你所见这里出现了一个警告
这是因为我们尚未在虚拟环境中安装对应提供者
现在在终端中执行
Uvp install
然后安装apache flow provider postgres并使用版本
六点一三
然后回车并等待片刻
好的 完美，现在已安装
你可以看到警告已消失
现在我们有了Postgres钩子的访问权限
在文件底部创建任务
在进程用户之后
创建名为store_underscore_user的任务
任务名称为store_underscore_user，不带参数
然后创建一个钩子变量，调用Postgres钩子
Postgres钩子需要以下参数
Postgres_underscore
Underscore id
该参数使用之前课程中创建的Postgres数据库连接
这是你需要为钩子定义的唯一参数
然后一旦你有了这个就可以使用它
所以钩子复制专家
现在你可以访问到这个方法
而且你没有任何操作符允许直接使用这个方法
在这里输入你想要执行的SQL请求
也就是接下来这个复制
来自estin的用户带CSV头
这里的用户来自我们在PostgreSQL数据库中创建的用户表
然后在SQL参数下方
最后一个需要使用的参数是fine a，指向你的文件路径
也就是斜杠tp
斜杠用户信息
Csv 一旦完成最后一步就是显式存储用户
否则任务不会在UI中显示
就这样 你已成功创建了复制用户数据的任务
从用户信息Csv中复制数据并加载到用户表中，在测试存储用户前
我们需要在处理用户部分做轻微修改
以确保存储用户能正确返回到处理用户
添加以下代码用户信息
然后创建时间并指定时间戳
即用户被提取的时间
然后从导入的时间模块
好的 一旦完成
让我们验证存储用户和处理用户是否正常
因为我们刚刚在Airflow终端修改了代码
运行airflow任务
测试用户处理
然后处理用户执行
好的 可以看到它正常运行
现在运行相同命令
这次针对存储用户执行
可以看到它正常运行
恭喜
因为你已成功实现项目最后一个任务
现在你可以使用PostgreSQL钩子将数据存储到PostgreSQL数据库
我们还未完成 如果你查看UI
会发现有些任务未关联
因为依赖关系未定义
所以我们需要定义它们 这就是下一视频将展示的内容
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/31_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p31 14. Define the dependencies.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


如果你前往UI并进入dags页面
然后点击你的dag用户处理任务
你将在图视图中看到以下任务
如你所见这里存在问题对吧
你有create table和store user任务像漂浮着
而api可用性箱用户和处理用户已设置依赖
你知道api可用性会先运行
然后提取用户再处理用户
问题是如何正确定义dag的依赖关系
因此create table会先运行
然后easy pi可用用户
处理用户和存储用户完全如该架构所示
嗯 其实很简单
返回数据管道并查看底部
可以看到已定义了一些依赖
但这些依赖是隐式的
因为你在easy pi可用性之间共享数据
提取用户和处理用户之间
Airflow能根据共享数据自动推断依赖
这里api可用性返回用户信息，提取用户使用后处理用户也使用
提取用户返回用户信息
这就是Airflow识别任务间依赖的方式
但create table和存储用户呢
你需要使用我们所说的b移位操作符
你有右移操作符
你可以这样定义任务a
然后使用右移操作符
任务b表示任务a先运行再任务b
也可以反过来
使用左b移位操作符
此时任务b先运行再任务a
通常使用右移操作符
另外 如果你想在任务a后运行多个任务
很简单 只需将任务放入列表
例如创建列表并添加任务c和d
这样任务a会最先运行
接着运行任务b
c和d同时运行
之后可能需要运行最后一个任务
即任务e
你还可以在多行定义依赖
例如任务a后接任务b
另起一行任务b后接任务c
好的 这相当于任务a、b、c依次运行
别忘了可以在多行定义依赖
有时当任务较多时
与其把所有内容都放在一行，不如这样做更好
让我们看看如何定义数据管道的依赖关系
因此我们首先需要运行创建表任务
所以复制任务名称然后粘贴到这里
然后我们需要按顺序执行eaapi可用提取用户和处理用户
我们可以调用处理用户任务
然后将提取用户作为参数传递给处理用户
最后调用ii可用在act chooser中，就是这样
这行代码等同于执行三行操作
因此我们可以删除这行
最后一个依赖是存储用户任务
就是这样完成了
你已成功定义了数据管道中的任务依赖
再次确认创建表任务最先运行
然后easy pi可用任务执行后返回模拟用户数据
接着执行提取用户任务并返回
供处理用户使用的用户信息
最后执行存储用户任务
这让我意识到需要在easy pi可用前添加创建表
否则会创建创建表与处理用户之间的依赖
但不要创建
然后按我们想要的执行easy可用
现在保存文件并前往UI界面
刷新页面后
可以看到以下依赖关系
创建表任务最先运行
接着执行easy i可用
然后运行act用户处理用户
最后执行存储用户
恭喜你
到此为止 你已成功定义了任务依赖 现在可以运行你的数据管道了
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/32_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p32 15. The DAG in action!.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们已经完成了所有任务，依赖关系也已设置
是时候测试我们的数据管道看看是否按预期运行
但首先我们需要做一些清理工作
所以如果你回到代码并向下滚动一点
你需要在这里删除用户信息
因为我们将在前一步提取用户任务中获取
然后保存文件
现在前往UI界面
进入DAGs然后选择用户处理
在这里你可以手动触发数据管道
因为你可以看到数据管道没有调度计划
所以必须手动触发
然后触发并点击启动
接着进入选项并从这里选择图表
你可以看到所有任务如图所示
让我们等待存储用户任务
可以看到所有任务已成功执行
恭喜 此时你已成功在Airflow中创建首个遵循最佳实践的数据管道
特别是你学会了如何定义DAG
使用DAG装饰器
如何使用SQL执行查询操作符运行SQL查询
如何创建Airflow连接以与工具或服务交互
然后你发现了如何实现传感器
这是一个等待条件为真后再执行后续任务的任务
在此使用传感器装饰器
你还学会了使用测试装饰器替代Python操作符
以节省时间和代码行数
如你所见这种方式比Python操作符更直接
你还使用了钩子来访问某些方法
比如copy expert这类在对应操作符中无法直接访问的功能
这就是使用钩子的优势
能更灵活地与目标系统或工具交互
但最后你学会了如何定义任务依赖关系
哦差点忘了
你知道如何在任务间共享数据
如图所示在处理用户和提取用户任务间
好的 所以我们涵盖了大量功能
但这还不是全部，后续课程中你将学到更多
顺便说一下这个管道正常运行
但如果你再次运行
store用户任务会出现问题
这是预期中的情况因为我们模拟了API
虚拟用户始终是相同的
因此会遇到ID相关的问题
因为ID必须唯一
但你会一直得到相同的ID
请记住这一点
这就是本视频的全部内容 然后我在下一个场景见到你
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/33_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p33 1. The new Asset paradigm.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 你将发现一种全新的思考Airflow工作流的方式
让我们开始吧 通常当你在Airflow中创建数据管道时
你需要定义包含一些任务的DAG
例如这里有一个包含三个任务的DAG
T1、T2和T3
你现在思考数据管道的方式是基于任务的
如果我们仔细查看每个任务
也许T1是提取一个文件并命名为txt
然后T2可能是清洗该文件并生成新文件
假设生成btxt，最后T3处理文件并创建新文件
c_txt
这是一个简单示例
但你可以看到
这里有三个任务
T1、T2和T3
每个任务都会产生新内容
在这种情况下是新文件
我的问题是：你真正关心的是什么
你关心这些任务吗
还是关心任务的输出结果
我认为我们都同意，你主要关心任务的输出
所以a_xt
btxt和c_txt
甚至可能是T1获取数据的来源以生成a_xt
所以这可能
比如一个API
好的 现在关键转变是：你应该将工作流视为
一系列相互关联的资产
经过不同转换生成其他资产
例如这里有一个API
这是一个资产
然后是a_xt，另一个依赖于API的资产
API资产生成btxt资产
btxt资产生成c_txt资产
这些资产之间存在依赖关系
就像之前思考任务时一样
但这里只是模型转变
你不再关注任务
而是关注工作流处理的资产
现在你理解了理论
接下来视频我们将看看 Airflow如何实现这一点
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/34_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p34 2. What is an Asset.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在本视频中您将了解Airflow中的资产是什么
让我们开始吧
我们先从资产的定义开始
因此，资产是相关数据的集合
例如，您有一个数据库或持久化存储中的表
机器学习模型或报告
或者一个目录
甚至API的输出
或者甚至一个文件
如果您觉得这个定义耳熟
那是因为这是Airflow数据集的定义
实际上在Airflow中
三个数据集已被重命名为资产
提供与数据集相同的特性
并增加本视频中将介绍的新功能
接下来您可能会想知道为什么应该使用资产
让我展示第一个使用资产的原因非常直接
假设您有两个数据管道
第一个包含任务A和B，第二个包含C和D
假设您不希望触发包含C和D的工作流
按时运行 而是希望在任务B生成输出后立即触发
好的 因此需要运行包含C和D的任务的工作流
当第一个工作流中的B任务生成输出时
为此您可以使用资产
具体来说，A任务运行并成功完成
然后B任务运行并成功完成
并在此处生成一个资产
这是一个文件 您将看到如何使用资产对象表示文件
该资产会触发第二个工作流
因此C任务运行并成功完成
最后是D任务，记住如果需要在不同工作流或DAG之间建立依赖关系
那么资产可能是个好选择
尤其是当需要非定时触发工作流时
而是基于任务输出触发
第二个使用资产的原因在于代码层面
让我演示一下，假设您在Airflow中
想要创建两个数据管道
一个是生成每日销售报告的数据管道
为此创建每日销售数据集
可以看到提取销售数据任务生成该数据集
这里显示输出为每日销售数据
然后是第二个数据管道
即月度报告
它依赖于每日销售数据集
因此在每日报告提取成功后立即运行
然后运行聚合月度数据的任务生成月度数据集
月度销售数据
好的 所以这就是你用airflow处理x需要做的事情
如果你想创建两个相互依赖的数据计划使用数据集
虽然它有效 这需要一些代码
让我展示如何用完全相同的两个数据管道
在airflow三中使用资产
现在你看右边的内容
与左边看到的内容完全相同
而不是写两个
九行代码
你只需写八行代码
正如右边顶部所示
这里有每日销售资产对应左侧的每日报告dag
然后有月度销售资产对应左侧的月度报告dag
因此第二个使用资产的原因是专注于你的资产
关注你工作流的输出
从而能用更少代码编写工作流
相比传统任务方法
第三个使用资产的原因涉及数据血缘
确实通过资产
可以轻松追踪哪些工作流产生什么结果
以及基于资产的工作流依赖关系
我现在可以展示这一点
因为显然需要创建小型资产和工作流
之后你会看到
但需记住资产能更清晰展示差异
基于资产的工作流依赖
相比不使用资产的情况
现在你更了解资产的用途
让我展示如何创建资产
这是定义资产所需代码
首先需导入资产装饰器
然后像这样使用资产装饰器
然后是名称参数
资产必须有一个在airflow部署中唯一的名称
该名称在ui中代表资产
由于使用默认资产装饰器会以函数名作为名称
此处无需显式定义名称
但如果你想也可以
只需记住 资产必须有名称
资产必须有调度时间
例如该资产每天午夜生成
然后可以定义ui
如果你之前使用数据集
你知道这是
可以视为指向该资产所代表数据的路径
例如这里是一个位于以下位置的文件
强烈建议使用有意义的ui
即使目前尚未使用
因为之后可能会被Airflow使用
例如实际检查底层数据
所以如果你的资产代表一个文件
使用文件前缀
如果你的资产是S3存储桶中的文件
使用S3前缀等
然后如果你想，可以为资产附加额外信息
例如 这里文件的大小
最后你也可以使用group参数
如果你有多个可以相互关联的资产
这就是这个功能的亮点 这就是本期视频的内容 我们下期再见，创建你的第一个资产
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/35_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p35 3. Create your first Asset.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 你将创建第一个代表虚拟用户的资产
让我们深入其中创建你的第一个资产
你将重复使用课程开头使用的同一API
这就是randomuser.me
因此该API返回一个虚拟用户
如你所见这里有结果
结果中始终包含一个虚拟用户
这里有一系列属性
例如包含名和姓的姓名
以及地理位置
登录信息 电子邮件等
现在让我们看看如何创建第一个代表虚拟用户的资产
在Cody中使用该API
打开dags文件夹并新建user.py文件
接下来要创建资产
需要导入资产装饰器
所以输入from airflow.sdk import asset
要创建资产
操作非常简单
需要使用资产装饰器
所以asset
然后必须首先定义一些参数，首先是名称
但因为我们使用资产装饰器
无需显式定义名称参数
我们将使用函数名称
所以这里命名为
你在那里
目前函数中不需要执行任何操作
因此资产名称仍为user
如果你想定义不同名称
只需指定名称参数即可
接下来需要定义调度时间
请记住调度是必需参数
我们希望每天午夜生成该资产
可以使用daily定时
然后定义UI
请记住uri对应资产所代表的基础数据
uri是可选的
如果你为资产定义了名称
否则必须定义uri
但此处已有资产名称
无需指定uri
但作为最佳实践 建议始终定义uri
在本例中该资产的基础数据是API
因此只需填写API链接即可
差不多就是这样
我们无需为该资产添加额外信息
或者将其添加到组中
现在你已经定义了资产参数
让我们看看这个函数
在这里你需要请求API
因此你需要导入requests
然后创建一个新变量或等于requests
点get
在这里你需要使用uri
因为uri对应你想要请求的API
你可以这样做你的uri
但这不起作用
如你所见 这里出现了一个警告
表明你的uri未定义需要解决
你只需要在函数参数中使用self
在这里输入self点uri
现在你可以访问API了
链接到最后
但最后你可以直接返回响应
或者然后调用json返回Python字典
对应API的响应
现在作为最佳实践
我总是建议为函数添加类型提示
你的资产等
所以这里只需添加dict和str
这表明该资产返回包含字符串值的字典
就这样 你已成功创建第一个名为user的资产，返回此API的虚拟用户
同时请注意不要显式调用你的资产
最后 就像处理dags一样
因为这样不会起作用
好的 你可以直接定义资产无需显式调用
你可以在Airflow UI上看到你的资产
让我们看看是否有效
确保保存文件
然后在终端
确保Airflow正在运行
输入docker compose up
然后回车等待所有Docker容器
例如处于运行状态
所以等几秒钟
完美现在启动调度器
然后进入exec并输入bash
现在你可以通过这里访问Airflow CLI
运行以下命令airflow assets list
你应该能看到你的资产user
如果不行说明代码有误
因此仔细检查代码
另外别忘了你还可以访问udme平台上的文件和视频
好的，明白了 这就是这个视频的全部内容
现在你已成功创建了第一个资产，接下来 你将学习如何将资产实体化
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/36_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p36 4. Materialize an Asset.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


当你拥有一个资产
接下来你要做的是将其物化
让我们看看如何操作
请记住资产是一种可替代资源
比如数据库
一个CSV文件等等
并且通常无法直接运行
这就是为什么我们不说运行资产
而是说物化资产
现在看看如何物化你的资产
为此你需要执行docker stop
然后选择调度器
接着进入exec
在这里输入斜杠bean
从那里你可以访问Airflow CLI
现在你可能会想如何物化这个资产
如何运行它其实很简单
输入airflow assets
然后输入materialize --name
名称是user
输入后稍等片刻
就这样你成功物化了资产
但仔细查看状态
你会发现它处于队列状态
这意味着资产尚未完全物化
现在在流程UI查看
在Airflow UI中 进入dags页面
可以看到你的资产user
点击进入后
显示有一个运行任务处于队列状态
这是因为需要开启DAG的开关
现在你可能会困惑
等等 我以为我们在创建资产
而不是DAG 实际上后台
此时
资产是一个单任务的DAG
明白了 这就是为什么能看到资产作为DAG
因为它本质上是一个对应资产的单任务DAG
这就是实际情况
点击任务user
这里显示操作符
资产主操作符所以再次
回到资产
用户这里开启开关
像这样
然后可以看到资产正在物化
现在你可以通过状态看到它已成功实体化
如果你刷新页面
你可以看到这个资产已创建了一个事件
然后点击任务并查看日志
你可以看到你的资产对应一个字典
这是API的响应结果
显然如果你想实体化一个资产
不必通过命令行界面
你可以点击该按钮来实体化你的资产
同时查看资产标签
就在这里 你将再次看到你的资产
如果你想实体化它
可以点击该按钮
如果你点击该按钮
你可以看到两个选项：实体化和手动
如果你使用实体化
这意味着将触发与你的资产关联的上游DAG
好的 记住这正是我们刚才做的
但如果你想仅创建资产事件
而不运行实体化资产的代码（即不发送该请求）
你也可以这样做
这在下游DAG依赖你的资产时非常有用
但你不想通过创建资产事件再次实体化
现在将触发所有依赖它的下游DAG
你已成功实体化第一个资产
让我们看看如何创建另一个 顺便创建资产之间的依赖关系
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/37_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p37 5. Define dependencies between Assets.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在是时候创建你的信号资产了
然后看看如何在第一个资产和新资产之间建立依赖关系
我们开始吧，好的
我们将创建一个资产来物化虚拟用户的地理位置
所以让我们这样做打开你的债权人
然后在同一文件user.py中
用资产装饰器创建新资产
在这里我们将定义scale参数
但对于scale参数我们不希望定义时间
我们不希望每天午夜运行该资产
相反，我们希望用户物化时立即物化该资产
为什么因为我们需要从此资产提取用户位置
我们需要先有一个用户
所以在这里做
你输入用户，就像这样，一旦用户资产被物化
该资产也会被物化
我们将这个资产命名为用户位置
然后返回包含字符串值的字典
现在问题是如何从用户位置资产访问用户资产
嗯 很简单
只需将用户资产作为参数传递给用户位置
所以这里放用户
必须使用相同名称
这是资产类型
然后导入资产类
现在你可以从用户位置资产访问用户
你还需要添加第二个参数
以便获取该资产返回的xcom对应API响应
所以你的虚拟用户
好的 记住这里我们返回一个值
因为我们返回了值
这将成为我们可以在用户位置资产中获取的xcom
要获取用户位置资产
需要访问任务实例对象
要访问它必须访问该资产的上下文
所以输入context
然后是上下文类型
可以从airflow中导入context类型
现在下一步是创建新变量
所以我们称之为 用户数据等于
Ti代表任务实例
然后调用xcom_pull方法
在这里定义dag id
等于用户.name，好的
所以资产名称
记住实际上资产是一个包含单个任务的dag
所以我们需要从dag id user.name拉取xcom
所以这个任务ID也等于使用该名称，然后最后
但最重要的是你需要使用参数
包含之前日期
我们总是想要获取用户资产返回的最新xcom
如果你不这样做 你会遇到错误
现在我们已设置所有参数
下一步是返回用户数据
然后我们访问结果，并记住结果是一个结果列表
在这里我们想要访问第一个结果
虚拟用户和最后
但最重要的是位置
就这样
你已成功创建了信号集用户位置以访问用户资产
为了获取用户返回的xcom
虚拟用户 然后这个用户位置资产提取虚拟用户的地理位置
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/38_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p38 6. Monitor your Assets.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在之前的视频中，你已经成功创建了之前的视频中的内容
你现在已成功创建了第二个资产用户位置
让我们看看Airflow中会显示什么
如果你点击资产
你应该能看到两个资产
用户和用户位置
点击用户，这里可以看到用户与用户位置之间的依赖关系
因此可以看到用户资产是用户位置的上游
一旦用户物化成功
用户位置也会随之物化
让我们实际演示一下
为此你需要开启两个DAG的开关
用户和用户位置
这里和这里，然后返回DAG界面
然后触发用户任务
然后点击 触发并稍等片刻
可以看到用户位置正在物化，因为用户已成功物化
这就是如何在两个资产间建立依赖
现在让我们看看你获得的结果
点击此处的图表然后选择任务
然后点击x
可以看到该资产返回的XCom数据
这里可以看到我们成功获取了虚拟用户的地理位置
作为快速练习，请创建新资产用户登录
以提取用户的登录数据
现在返回你的代码
在用户.dot文件中在用户位置之后添加
请花几分钟时间，不必犹豫暂停视频
我将在几秒后展示解决方案
好的 现在查看解决方案
正如你将看到
其实很简单 首先需要使用资产装饰器
像之前一样调度该资产到用户资产
因为你需要用户数据
创建新函数命名为用户登录
同样需要从该函数访问用户资产
以及当前资产的上下文
并返回包含字符串值的字典
然后这里 和之前一样创建新变量
用户数据等于上下文通过任务实例调用XCom拉取方法
然后定义id为用户名
任务id使用用户名
包含之前日期
但这里要返回虚拟用户的登录数据
就是这样
你已成功创建第三个资产
为了生成虚假用户的登录数据
如果你保存文件并进入Airflow UI界面
然后点击资产
你应该能看到你的三个资产
如果不行 可能需要等待最多五分钟才能在UI上看到新资产
现在让我们再次点击用户
这次我们需要将下游资产关联到用户资产
用户位置和用户登录再次
如果你开启用户登录的开关
然后触发用户资产的事件
将会触发下游资产
现在让我们看看这个过程
返回到DAGs页面
然后让我们创建一个用户
你会看到用户位置和用户登录会在用户完成时立即生成
就是这样，正如你所见
这是一个非常强大的概念，拥有这种资产视图非常实用
能够明确了解不同资产之间的依赖关系
但回到代码编辑器
我想我们都同意这一点
这两个资产实际上是相同的资产
除了这些值 现在让我们看看如何改进这里的代码
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/39_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p39 7. Create multiple Assets at once.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在本视频中你将重构资产
你还将发现创建多个资产的另一种方法
所以让我展示一下，如果你还记得我们已创建了两个额外资产
使用我们的位置获取用户的位置
并使用登录获取与虚拟用户相关的日志数据
但如果你仔细查看这两个资产的代码
它们几乎完全相同对吧
唯一的区别是用户登录的密钥
它是登录，而用户位置
它是位置
显然你需要避免代码重复
那么如何解决这个问题呢
我会说有两种选择
第一种是创建一个包含所有代码的经典Python函数
然后在用户位置中调用该Python函数
然后在用户登录中也会生效
但我想向你展示另一种方法
那就是使用资产多方法同时生成多个资产
让我先展示如何操作，在两个资产之前
创建一个新资产
这次你将使用资产点多方法
保持空白，仅在创建资产下方
并将其命名为用户
就像之前一样
仍需访问用户资产和上下文对象
你可以复制这些参数并粘贴到这里
然后该资产返回包含字符串值的字典列表
好的
现在同样
像之前一样使用该方法
X可以池来获取用户资产返回的虚拟用户
然后返回此处指定的字典列表
这些字典是材料化的两个字典
来自用户位置资产和用户登录资产
你可以复制并粘贴到这里
同样处理登录数据
像这样，好的
这就是为什么我们有包含字符串值的字典列表
因为我们返回了两个字典
一个对应虚拟用户的地理位置数据
另一个对应虚拟用户的登录数据
现在完成这些后你仍需做几件事
在资产多方法中
需指定此函数材料化的资产列表
这里使用出站参数
然后创建一个列表
在此处创建两个资产使用资产类
现在创建第一个
一个名为用户位置的资产
就像之前一样，你可以看到这里使用位置
然后第二个集合是用户登录
就像我们之前做的用户登录
通过在资产中使用outlets参数
我们向Airflow表明此函数用户信息物化两个资产use location
并使用我们的登录
第一个资产用户位置对应这个字典
第二个资产用户登录对应这个字典
就这样你可以移除另外两个资产
在scale参数中对用户像之前保存文件
你已成功重构了代码
更重要的是
现在你知道如何创建同时物化多个资产的函数
这是一种非常强大的方法，但需谨慎使用
因为理念不是让所有资产都由一个函数生成
因为请记住如果位置出现任何问题
将无法物化位置
但同样登录
即使登录没有问题
所以这就是为什么 我建议始终使用一个函数物化单个资产
而不是一个函数物化多个资产
如果你查看资产
视图并点击用户资产你会看到该页面
这次可以看到用户信息生成两个资产
用户位置和用户登录
关于资产就到这里
请记住它们在Airflow三中仍较新 哦并且期待未来Airflow会有更多改进
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/40_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p40 1. What's an Executor.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我快速介绍一下执行器
现在我们来定义执行器
如果你还记得Airflow有不同的组件
比如API服务器
调度器 元数据数据库等等
但你还有执行器和步行器
执行器并不执行你的任务
请记住这点以免混淆
执行器并不执行你的任务
是步行器执行你的任务
而不是执行器
那么执行器的具体作用是什么
执行器负责定义任务如何以及在哪个系统上运行
执行器是调度器的一部分
你不会将其视为独立的Airflow组件
现在有不同类型的执行器
第一个是本地执行器
如果你想在本地机器上执行任务
在你的电脑上
则会使用本地执行器
因为它允许在同一台电脑上同时执行多个任务
但假设你的电脑资源不足
无法处理所有待运行的任务
这时就需要使用Celery执行器
也就是薪资执行器
也许你不知道Celery是什么
但它是一个Python框架，用于分配任务
在多台计算机之间
比如你有一个计算机集群
在这种情况下每台计算机将成为Airflow步行器执行任务
使用Celery执行器
你可以通过Celery框架分配任务
到多个Airflow工作节点
这些Airflow工作节点现在是不同计算机
假设你不太熟悉Celery
另一个选项是 如果你有Kubernetes集群，可以使用Kubernetes执行器
你可以在Kubernetes集群中运行任务
可能该集群包含多台机器
然后任务会在不同Pod中运行
如果你不了解Pod是什么
没关系 只需记住Pod类似于微型虚拟机
运行在Kubernetes环境中
这些Pod将成为执行任务的Airflow步行器
只需记住执行器定义如何
以及在哪个系统上运行任务
执行器并不执行任务
气流执行器将执行您的任务
您有不同的执行器类型
例如本地执行器可在同一台计算机上执行任务
Celery执行器
如果您使用Celery框架在多台计算机间分配任务
然后还有Kubernetes执行器
如果您使用Kubernetes集群
现在想要使用该集群运行任务以更改执行器
Airflow当前使用的执行器
您有一个设置项是执行器设置
通过修改该设置
即可选择所需的执行器
再次选择
本地执行器
Celery执行器
或人类执行器
根据使用的执行器
可能需要更改其他设置
但目前 需记住执行器设置是需要修改的 要选择执行器需调整此设置
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/41_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p41 2. The default configuration.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


正如你所知，要更改执行器你需要使用
你需要修改Airflow的设置
但问题是如何访问这些设置
好吧让我演示一下打开codex
暂停然后进入容器
你可以看到对应Airflow组件的不同Docker容器
例如你有元数据数据库
API服务器
调度器等等
现在让我们选其中一个
比如说调度器
然后进入fi
所以我们稍等片刻直到文件显示出来完美
现在在这里寻找
展开这个
然后展开Airflow
现在你有了airflow.dot
Cfg 这是Airflow的配置文件
你可以点击它
然后打开文件门以访问该文件
让我展示这里的内容
Airflow配置文件分为两个不同部分，例如这里有core部分
但还有其他部分
比如向下滚动一点
这里有数据库
日志记录等等
请记住配置文件中有不同部分
每个部分下有不同的设置
例如这里的文本文件夹
然后主机名
Collibel等等
但更重要的是
如果你仔细查看这里
这里我们有执行器
这就是之前视频提到的设置
如果你想修改执行器
如果你想使用不同执行器
就必须更改这个设置
默认情况下Airflow使用本地执行器
这意味着你可以运行本地机器能处理的任务数量
假设你想使用不同执行器
在这种情况下你不能只在这里修改值然后保存文件
就这样 不会生效
为什么因为记得我们是在Docker中运行Airflow
所以这里所做的更改仅适用于此Docker容器
因此你需要依赖环境变量
来覆盖Airflow配置文件中的对应设置
你可以看到如何操作
所以基本上如果你想更改执行器
你需要创建一个非环境变量与airflow_underscore_core
我们设置的这一节如你所见在这里core
然后你要覆盖的城市名称
也就是执行器
然后传递该值
例如本地执行器或串行执行器
由你决定 这就是如何更新你的airflow实例设置
当它在docker上下文中运行时
但老实说作为最佳实践
我从不建议直接修改配置文件
最好使用环境变量来覆盖
配置文件中想要的设置
让我展示一下
如果你回到代码然后打开docker compose文件
你可以看到我们修改的设置
例如你有airflow核心执行器使用本地执行器
这是默认设置
但你还有airflow核心执行API服务器URL
这是运行任务的API服务器URL
然后是airflow数据库
Sql alcheicon
这是连接到airflow元数据数据库等
所以你看要更改airflow实例设置
我们不直接修改配置文件
而是创建这些环境变量这就是本视频内容
别担心 你将在课程中后续更改airflow实例设置 但至少现在你知道如何使用环境变量覆盖airflow设置
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/42_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p42 3. The sequential executor.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个简短视频中 你将发现Airflow中最受欢迎的执行器之一
我们马上开始吧
请记住，执行器定义了任务如何以及在哪个系统上执行
很长一段时间以来
实际上直到三
哦 默认执行器是顺序执行器
正如你可以猜测的
顺序控制执行器意味着任务会按顺序运行
这意味着什么
我现在就演示给你看
所以想象你有一个包含四个任务a、b、c、d的数据管道
使用顺序控制执行器时
当你触发这个数据管道
当你运行它时
最终会呈现这样的执行顺序，先执行任务再完成
现在b和c可以同时并行运行对吧
但由于使用顺序控制执行器
无法同时运行多个任务
任何时候只能有一个任务在运行
例如这里b先运行
也可能是c
但让我们以b为例
然后c会等待b成功后才开始
接着c可以运行
然后c成功完成
现在c完成后d也需要成功
好的 这就是顺序执行器的工作原理，非常简单
任何时候只能运行一个任务
顺序控制执行器的局限性
是任务或DAG只能在单台机器上运行
无法将任务分配到多台计算机
好的 说到这里
Airflow中已不再存在顺序执行器
好的 直到Airflow 3.0之前
顺序控制执行器是默认执行器
从Airflow 3.0开始
默认执行器变为本地执行器
顺序执行器已被弃用
不能再使用了
它已不再存在
好的 但我想介绍顺序控制执行器
因为再次强调 它曾长期作为默认执行器
但请记住它已不再存在 现在你将在接下来的视频中发现本地执行器
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/43_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p43 4. The local executor.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在第二个控制执行器已经不存在了
新的默认执行器是本地执行器
让我详细介绍一下吧
本地执行器简单但功能强大
因为它可以并行执行多个任务
具体来说，有一个任务先运行然后成功
但现在你有任务b和c，两个任务可以并行运行
因为你在使用本地执行器
所以不再是任务b先运行然后检查权重
然后b成功后再运行c，现在你可以让两个任务并行执行
所以b和c同时运行
然后它们都成功了
最后d任务运行并成功
这就是工作原理
本地执行器允许并行运行多个任务
而使用第二个控制执行器时你可以做到这一点对吧
现在只能一次一个任务
请注意所有任务都在单台机器上运行
在单台计算机上
这也是为什么这里提到本地部分
你的任务本地运行
这意味着 这也是一个限制
你实际受限于运行任务的计算机资源
如果你需要运行几百个任务
可能计算机足够应对
但如果有数千个任务需要运行
可能需要增加更多CPU或内存
以便在单台计算机上运行这些任务
当你需要运行大量任务时
可能需要使用稍后介绍的其他执行器
好的 但请记住本地执行器是新默认执行器
从Airflow 3.0开始，你可以并行运行多个任务
建议你始终从它开始
好的 不要一开始就使用复杂的无限扩展执行器
先从本地执行器开始
看看是否足够
然后转向高级执行器
如果需要的话 根据需要运行的任务数量
等等 好的
关于本地执行器就讲到这里
默认配置
这里无需额外操作
这就是默认设置 首次运行Airflow时你会得到这个
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/44_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p44 5. The celery executor.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这段视频中 你将发现人们广泛使用的一种流行执行器
当他们在生产环境中运行Airflow时
并且需要运行尽可能多的任务
这就是Celery执行器
让我们开始吧 好的
首先要知道的是Celery执行器使用Celery
这是一个分布式任务队列
事实上 Celery是一个简单
灵活且可靠的分布式系统，用于处理大量消息
同时提供维护此类系统的必要工具
它专注于实时处理任务
同时也支持任务调度
Celery执行器的核心思想是使用Celery
通过队列分发你的任务
观察队列的定义
在Celery的上下文中
可以看出它是一种分发机制
跨越线程或机器
并支持任务通信
通常使用消息代理
这一点很重要 稍后你将看到它在客户端和工作者之间协调任务
客户端将消息添加到队列中
然后代理将消息传递给工作者
好的 让我在Airflow的上下文中解释一下
好的 使用Celery执行器时
你将进行一些架构调整
首先通常是不需要的
但大多数情况下你会有多个计算机
有一台计算机运行所有Airflow组件
API服务器
调度器等
在这种情况下这台计算机
然后你有额外的计算机作为工作者节点运行任务
例如 工作者一
二和三
总共四台计算机
你有一组计算机集群来分发任务
请记住 Celery执行器的目标是将任务分发到多个工作者机器
因此你可以无限扩展
我的意思是 只要预算或资源允许
你有多台计算机和工作者来运行任务
你还有另外两个组件
第一个是代理（broker），代理基本上就是任务队列所在的位置
这里你可以找到它
Q1和Q2
代理的目标是将任务分配给这些工作节点
好的 然后你有一个结果后端用于存储
在工作节点执行的任务结果
其工作原理非常简单
可以说想象你有一个需要运行的任务A
在这种情况下，Airflow执行器会向代理发送指令
这对应执行任务A的命令
该命令将被发送到其中一个队列
稍后你会看到，你可以自行选择
但默认情况下可能是Q1
因此任务A进入Q1队列
然后自动地
Celery会将执行任务A的指令分配给其中一个工作节点
例如分配给工作节点2
然后在工作节点2
执行运行任务A的指令，这里
任务A在工作节点2中运行
任务A成功完成后
结果会被存储在结果后端
然后Celery执行器从后端获取该结果
因此你可以在Airflow UI上查看任务输出
这就是其工作原理
只需记住
Celery执行器的目的是将任务分配给多个工作节点
使用队列
这些队列非常有用因为
稍后你会看到
根据你选择的队列
因为你可以选择队列
你可以将任务分配到不同的特定工作节点
好的 现阶段只需记住Celery执行器
通过队列将任务分配给多个工作节点
你还有其他组件
如代理和结果后端
需要通过Airflow执行器进行管理
这就是Celery执行器的内容
现在你已经了解其工作原理 让我们实际演示
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/45_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p45 6. Running Airflow with Celery.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中你将学习如何配置你的f
例如 因此你可以使用单元执行器
但好的一点是正如你将看到的
你不需要做太多
让我展示给你看吧
如果你还记得课程开始时你需要使用
而你仍然在使用 顺便说一下docker compose文件
请记住docker compose文件描述了你需要运行的所有docker容器
以便在本地机器上运行airflow实例
但这个docker compose文件是官方版本
我只是做了一些修改
但它确实是官方版本
官方docker compose文件默认包含salary执行器
这就是我说的
我做了一些修改
因为我不希望你立即使用单元执行器
相反我希望你使用本地执行器
但默认情况下它仍然使用单元执行器
你已经拥有了运行airflow所需的一切
改用celery执行器
所以让我先展示给你看
你有airflow核心执行器设置
请记住如果你想使用不同的执行器
这是你需要修改的第一个设置
在这里我们使用本地执行器
你可以将本地执行器改为celery执行器
像这样 然后你可以配置想要使用的airflow元数据数据库
通过更改此设置airflow数据库
Sql alchemical
但我会继续使用postgres
所以你可以保持此设置默认
现在你有两个被注释掉的额外设置如你所见
第一个是airflow salary结果后端
请记住结果后端基本上是数据库
用于存储工作节点执行任务的结果
正如你看到的默认情况下
它指向与airflow元数据库相同的数据库
这正是你期望的对吧
我的意思是 否则你将无法查看结果
任务在ui上的输出
所以再次
airflow结果后端应与airflow元数据数据库相同
让我们取消该行的注释
然后在下方你有air flow salary broker url
请记住broker负责将任务分配给多个工作节点
broker管理任务队列
其目标是将任务分配给工作人员
这就是为什么你可以看到它使用reis
如果你不了解 Redis是一种内存键值数据库，用作分布式缓存和消息代理
因此核心思想是使用reis接收评论
运行由cellar执行器发送的任务
并将这些命令分发给不同的airflow工作者
正如你之前看到的
为了运行这些任务
大多数情况下你会使用rabbitmq或redis作为代理URL
所以取消这行注释
顺便说一下这意味着你需要管理额外组件
这里就是redis
现在向下滚动一点
在服务下可以看到ready
显然我们需要使用reis作为代理
我们必须在本地机器上运行它
我们使用docker来实现
现在取消该服务的注释
现在我们要运行redis的docker容器
再次向下滚动一点
我们有一个airflow工作者
请记住airflow工作者执行你的任务
这些任务由salary执行器分发
我们需要取消所有注释
像这样 所以再次
现在你会有一个对应airflow工作者的docker容器
并包含airflow环境
还有一个cli工具floor
现在取消这些行和floor的注释
稍后你会看到
允许你监控
管理队列
但暂时先忽略
我们将在专门视频中学习
好的 现在你已取消所有服务注释以运行airflow使用cell执行器
包括reis、floor和工作者
再取消一行
检查其健康状态
好的 现在一切准备就绪
可以停止对应f的docker容器
例如 在终端按ctrl c停止容器
或者直接进入下一步
在airflow界面选择所有容器
然后点击此处停止所有容器
所以你可以看到所有Docker容器现在都已停止
让我们回到终端界面
现在运行与之前相同的命令启动airflow，使用docker co
但这次使用salary执行器
所以进入后请稍等片刻，如果你打开Docker桌面
你可以看到不同的容器
几分钟后你应该能看到所有容器状态健康且正在运行
除了airflow容器外
好的 所以你可以看到已经准备好了skillinder
触发postgres
dac处理器
API服务器和airflow walker
如果你想知道lower在哪里
别担心
这里看不到它是预期现象
因为它尚未运行
就是这样 此时你已通过celery执行器运行airflow实例
并且能够将任务分配给多个walker 我们将在另一个视频中演示
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/46_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p46 7. Monitoring your tasks with Flower.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


为了更好地可视化任务在单元执行器中运行时的情况
我们将使用flu
让我展示flis并解释它为什么有用，好的
如果你查看日志文件
然后向下滚动直到文件末尾
现在你可以在这里看到floor服务
你可能会想知道什么是lower well
如果你查看官方文档
可以看到fluis
一个用于监控和管理celery集群的开源Web应用
它提供关于celery工作者和任务状态的实时信息
所以在Airflow的上下文中
你将能够使用floor监控任务和工作者的运行情况
这就是为什么flora非常重要
每次使用celery执行器时
让我们看看如何在floor服务上运行它
你可以看到以下注释，这表明如何用flow运行Airflow
现在按Ctrl+C停止所有对应f的Docker容器
例如 如果你查看顶部
你应该能看到所有本地容器已停止
现在你可以查看
通过添加-dash profile参数启用flu
在docker compose命令中添加fluoption
现在在终端中执行
你输入docker compose
然后-dash profile floor up
进入后再次
等待片刻直到所有容器启动
除了Airflow外
几分钟后 你应该能看到所有Docker容器处于健康运行状态
除了Airflow外
更重要的是
这里有一个新本地容器
对应flow以访问lower的用户界面
只需点击这里
然后跳转到该页面
如你所见 这里只有一个在线的Airflow工作者
表示它可以接受任务执行
还有不同的状态active
已处理 失败
成功和重试
这些是任务在celery和last上下文中的不同状态
但最重要的是
我不会深入细节
这基本上是过去一
五和十五分钟的队列负载平均值
现在你有一个工作者
你可以点击它
你将访问一些附加信息
所以让我们点击刷新
有时你会看到类似这样的情况
现在你可以访问一些酷炫的信息
例如你有最大并发数
默认值是十六
意味着该walker可以同时运行最多十六个任务
好的 所以该walker无法并行执行超过十六个任务
显然你可以修改这个设置
但这属于基础设置
正如你稍后会看到的
在池旁边你有经纪人
它正在准备
这里没有真正重要的内容
然后你有队列，队列很重要
请记住该walker连接到一个队列
而该队列默认是这个
你可以看到这里名称为默认
意味着每次任务发送到该队列时
walker 这个会尝试从队列中拉取任务以执行
好的 这就是当前的工作原理
正如你稍后会看到的 你可以有多个队列在队列旁边
你有任务 如果你有任何正在运行的任务
你可以看到活跃状态
已安排等等
然后你有限制
配置系统等等
但我们不会使用这些标签
好的 所以目前
我希望你主要关注池和任务
说到任务 如果你点击这里的任务
可以看到当前正在执行的任务
但随着我们创建数据管道会改变
为了使用薪资执行器运行任务 让我们这样做
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/47_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p47 8. Running tasks with the CeleryExecutor.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在Airflow已经使用Celery执行器启动了
并且你有监控任务运行状态的界面
在你的Airflow工作区
你已准备好使用Celery执行器运行首个任务
让我先展示如何创建新的数据管道
如果你不想手动编写这个数据管道
所有所需资源都在Houdini视频教程中
但本质上这个数据管道包含四个任务a、b、c、d
现在让我们在代码编辑器中操作
进入dx文件夹
创建名为celery的新Python文件
从Airflow SDK导入
DAG和任务模块，然后创建以下DAG
我们使用dag装饰器定义DAG
接下来创建各个任务
使用task装饰器定义任务
任务a只需休眠
比如五秒钟
我们可以使用sleep函数并设置五秒
然后创建任务b同样休眠五秒
复制粘贴代码并修改为c和d
所有任务均休眠五秒
好的 还需要在顶部导入sleep模块
从time模块导入sleep，最后
但最重要的是定义依赖关系，即任务a优先执行
然后依次运行b、c，最后执行d
好的
当你创建好这个数据管道后
你将得到与之前完全相同的管道结构
如果你不想等待五分钟后获取DAG
登录Airflow账户
你可以直接重启Airflow服务
在终端按Ctrl+C
然后通过Docker重新运行Airflow
执行docker compose up -d命令
需要等待本地容器启动完成
确认所有容器已正常运行
查看Airflow界面
进入DAGs页面
看到名为Celery DAG的任务 这就是我们要使用的DAG
点击进入并切换到图视图
可以看到任务a，b、c可并行执行
以及任务d
如图所示
现在触发数据管道观察运行效果
请确保你有访问权限
返回主页界面
点击此处触发DAG执行
然后触发
好的 你应该能看到一次运行
如果你切换到下方，可以看到任务基本正在执行
现在可以看到三个任务成功，一个处于活跃状态并即将成功
我们基本上完成了数据平面部分
这也是你在UI上能看到的内容
好的 这就是它的运作方式
非常简单 没错
你可以看到任务由Airflow walker执行
不再是之前由Airflow调度器执行
当你使用本地执行器时
如果你点击任务
可以看到所有已执行任务的状态列表
如果你点击其中一个
比如这里的第一个，会获得更多详细信息
例如再次
任务的状态
还包括传递给运行任务命令的参数
以及任务被walker接收的时间
任务开始执行的时间
然后任务成功完成的时间
还有重试次数以及执行该任务的walker
好的 这意味着单元执行器已成功将任务发送到队列
然后该walker从队列中取出任务进行执行
这就是当前的工作流程
我知道你可能会说
这其实并不算分布式
我的意思是这里只有一个walker
所以并不真正分布式
我们能不能有多个walker
那会更有趣 这正是你将在下一节发现的内容
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/48_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p48 9. Adding new workers.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这段视频中 你将学习如何为你的f实例添加一个工作者
这样你可以将任务分配给多个工作者
而不仅仅是之前的一个
让我展示一下，到目前为止我们只有一个airflow工作者
而这个airflow工作者执行由调度器发送的任务
为了添加新的f工作者
其实很简单
你需要做的就是返回代码编辑器
然后打开docker-compose文件
接着向下滚动直到找到air walker服务
所以让我们看看这里
你需要做的就是添加另一个服务
这将是一个新的airflow工作者
你可以复制这个airflow worker服务的所有内容
然后将其粘贴在第一个服务下方
但别忘了将这里的f worker重命名为airflow-worker-2
在airflow-worker-2上方
将airflow-worker重命名为airflow-worker-1
好的 就是这样，这就是如何在你的f实例中创建新工作者
使用docker 请注意在实际应用中你也可以这样做
你的airflow工作者可能对应docker容器
但也可能对应专用计算机作为airflow工作者
你需要记住的是，如果你想将某台机器
某台计算机变成airflow工作者
只需在该计算机上运行celery walker命令
它会自动成为可执行任务的airflow工作者
就是这样，好的
这就是你现在看到的内容
此服务运行调度器工作者
这将使docker容器
或计算机成为等待执行任务的airflow工作者
现在我们可以保存文件
下一步是停止我们的airflow环境
在终端中按Ctrl+C停止所有docker容器
好的，像这样
你可以检查桌面，所有docker容器应已停止
你也可以使用docker来确认
选择所有容器
然后点击这里 现在只需返回终端
诺尔并运行与之前相同的命令docker compose加上选项
减号 配置文件 氟
现在等待所有docker容器启动运行
除了airflow外的每个容器
几分钟后即可完成
你应该能看到所有Docker容器都在运行中
除了其中的airflow one
但还有airflow worker one
因为我们将其重命名为airflow worker one
然后我们还添加了airflow worker two
这就是你现在看到的内容
现在你有两个airflow worker需要验证它们是否成功加入airflow实例
这非常简单 我们可以使用floor来检查
所以点击这里
你应该能看到你的两个airflow worker显示在这里
服务在线等待任务执行
好的 这就是如何向你的f添加新工作者的方法 例如 因此你可以执行更多任务因为任务会被分配到更多工作者
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/49_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p49 10. What is a queue.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


Airflow中有一个简单但极其强大的概念
那就是队列
这就是你现在即将发现的内容
什么是队列呢
到目前为止我们讨论过单元执行器
以及Airflow工作进程如何从队列中获取任务
但请记住即使没有单元执行器
Airflow始终使用队列来按正确顺序执行任务
你可以将队列真正想象成等待处理的人群队列
在Airflow的上下文中
这里的人群被替换成任务
想象这个绿色部分是一条队列
队列实际上就是这样
接下来会发生的是Airflow会将任务推入队列
例如 首先有任务T1
然后可能是另一个任务T2，接着所有任务T3等等
好的 就是这样继续下去
本质上队列是先进先出
这就是其工作原理
因此T1会先运行，接着是T2和T3
这是一个简单概念
但在Celery的上下文中
这非常强大因为记得使用Celery时可以有多个Airflow工作进程
例如 如右侧所示
有三个工作进程
其中一个工作进程拥有5个CPU
另一个工作进程配备更多GPU
最后还有一个仅有一个CPU的工作进程
你可以创建三个不同的队列
正如中间所示
有一个q_icpu队列负责转发任务
到拥有大量CPU的工作进程
然后有一个q_ml模型队列处理需要大量GPU的任务
并将任务链接到配备大量GPU的工作进程
最后还有一个默认队列
所有其他任务都会发送到这里
比如常规任务执行
将被发送到仅有一个CPU的工作进程
通过设置不同队列和工作进程
能够智能分配任务
从而优化时间和资源
记住队列是先进先出
任务按顺序执行
然后可以配置不同工作进程关联不同队列
以更高效分配任务
根据资源和其他特定需求
现在让我们在下一视频中学习如何创建第一个队列 同时分配你的任务
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/50_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p50 11. Smart distribution of your tasks.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 你将创建另一个队列以向特定工作者发送任务
让我向你展示目前的操作步骤
你可以看到这里有两个正在运行的工作者
首先是第一个，然后是第二个
如果你点击第一个并进入队列设置
你会发现这个工作者正在等待来自默认队列的任务
一旦默认队列中有任务
该工作者将从该队列中获取任务
另一个工作者也是如此
返回主页后点击第二个工作者
然后进入队列管理
你可以再次看到
该工作者正在等待来自同一默认队列的任务
好的 太好了 这意味着任务现在会在这两个工作者之间分配
假设这个工作者实际上拥有大量CPU资源
好的
我们希望将需要大量CPU的任务
发送到该特定工作者
如何实现这一点呢
首先需要创建一个新队列
让我们创建一个名为high_cpus的新队列
打开你的编辑器
然后在docker-compose文件中搜索Airflow工作者服务
如你所见
这里有airflow-worker-one和airflow-worker-two
我们希望将
需要大量CPU的任务分配给airflow工作者
创建新队列只需做一件事
在Celery工作者配置中添加以下选项
dash q和你的队列名称
例如high_cpu即可
你将创建一个新队列
high_cpu并将它关联到airflow工作者
当任务被发送到high_cpu队列时
airflow-worker-two将从中获取任务
而另一个工作者airflow-worker-one
仍会从默认队列获取任务
好的 因为我们这里没有定义队列
就是这样
至此你已成功创建新队列
只需保存文件
然后按Ctrl+C停止所有Docker容器
同样操作
可以在Docker中验证
确保所有本地容器已停止
然后用之前相同的命令重启实例
别忘了 快跑
Dash资料 向下移动它进入
我们稍等片刻直到所有容器启动运行
除了airflow需要一个和airflow worker一个就可以了
所有docker容器均已启动运行
如果我们回到flu
我们仍然有两个walker
但如果你点击第二个
也就是walker二
然后刷新页面并点击队列
现在可以看到这个新队列
高CPU关联到该walker所以同样
一旦airflow将任务发送到icpu队列
该walker会从该队列获取任务
如果你回到主页并点击第一个walker
可以看到它仍在从默认队列获取任务
Q 好的
此时我们已有两个队列
一个队列将任务发送给该walker
另一个队列将任务发送给另一个walker
现在问题来了
如何在airflow中定义
要将任务发送到哪个队列呢
这就是我现在要展示的内容
打开代码编辑器然后打开celery dag
好的 你应该已经有了这个dag
如果没有请返回之前的视频
你可以在Udemy的某个视频中找到它
当你创建并运行一个任务时
该任务会自动发送到默认名称的队列，名称为default
实际上 查看airflow文档
可以看到这里 默认队列
这是任务默认分配并监听的队列
默认名称为default
好的 因此默认情况下
所有任务都会被发送到队列
默认 现在要更改此设置，可以在任务级别使用一个参数
该参数为q
例如我们想将任务a发送到默认队列
但其他任务需发送到high cpu队列
你可以这样做：在任务装饰器中
使用q参数等于high cpu
就这么简单 就像这样，气流会将任务b发送到高CPU队列
因此对应的工作者将执行该任务
现在我们为任务c做同样的操作
同样处理任务d
这样我们就完成了，好的
所以请记住 如果你想将任务发送到特定队列
只需设置cue参数为目标队列
你希望发送任务的队列现在
保存文件后返回UI界面，再次触发流程
然后进入流程
让我们等待所有任务完成
好的 所以还有一个剩余
现在我们完成了
你可以清楚看到第二个工作者已按数据管道定义执行了三个任务
没错 我们说过要执行任务b
在工作者二中执行c和d
而对于工作者一
它如预期只执行了一个任务
这就是任务
嘿 恭喜
因为此时你已完全掌握如何智能分配任务
根据工作者的资源情况
以及任务所需资源 同时你也学会了如何创建不同队列
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/51_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p51 12. Back to the previous configuration.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在本课程剩余部分
我们不再使用薪资执行器
我们将改用本地执行器以简化操作
请确保打开docker compose文件
然后需要注释几部分内容
首先将单元执行器改为本地执行器在此处
然后可以像这样注释这两行同时也可以注释这部分
像这样 由于不再使用redis
然后向下滚动到服务部分
可以注释redis服务
接着对airflow工作者做相同操作
将airflow worker one服务也注释掉以及airflow dash walker
同样处理第二个服务以及最后
但至少在文件底部这里你仍有flour
需要注释该服务
在终端按Ctrl+C停止所有容器
好的 现在再次运行airflow无需flour配置
几分钟后你应该看到docker容器
例如postgres
调度器 触发器 处理器 以及api服务器已启动运行
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/52_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p52 1. Grouping your tasks with TaskGroups.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 你将发现一种更好的组织任务的方法
同时让它们更易于阅读
那我来展示给你看吧
让我们想象你有一个包含所有这些任务的数据管道
任务一 然后b二、b三、b四
接着是任务五，然后d六、d七、d八
现在当你查看任务b二
b三在它们之前，都负责从不同来源提取数据
当你查看任务d六
d七和d八
它们都负责处理数据
这里你可能希望更好地组织任务
因为这是一个简单示例
但在实际中你可能会有数千甚至数百个任务
如何更好地组织这些任务b二、b三、b四以及d六、d七、d八
因为它们最终实现共同目标
你可以使用任务组
任务组允许你将任务分组
当你有实现相同目标的任务时
将它们分组是有意义的
因为这能更好地组织任务
使代码更易读和调试
最终你会得到以下数据管道
那我们先移除这些
这里不再有三个任务
而是对应一个任务组
在该任务组中包含b二
b三 和b四
同样这里不再有三个任务
而是对应一个任务组，包含d六
d七和d八
稍后在UI中你会看到
可以双击该任务组
它会自动展开
可以看到其中的不同任务
任务组有很多优点
首先是
它们能通过视觉分组帮助组织复杂DAG
在Airflow中将相关任务归类
UI还允许为任务集设置默认参数而非仅在DAG层级
例如
如果你想在DAG中重试任务三次
那么 在DAG层级定义即可
但对于任务组
希望该组任务仅重试两次
可以在组层级定义
并且该组中的所有任务都将重试两次
而不是三次
此外你可以将任务模式转换为可在不同dags或Airflow实例中复用的模块
想象一下你有一个任务组
在该任务组中包含两个任务
一个用于从源提取数据，另一个用于清洗数据
在这种情况下你可以将其导出为Python文件中的组
然后可以将该任务组导入到任意数量的dags中以复用这两个任务
这就是为什么任务组能为你的dags增加更多模块化
在你的Airflow实例中
理论部分到此结束，下一视频见 你将创建第一个任务组
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/53_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p53 2. TaskGroups in action!.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 你将学习如何实现和使用DAG中的任务组
让我们进入dax文件夹开始
创建新的DAG文件group_pi并从airflow_sdk导入decorators
然后你可以完全不带参数定义DAG
接着创建与DAG组对应的函数
在这里你可以创建一些任务
所以先创建一个任务a，仅在标准输出打印a
同样为b和另一个任务做相同操作
比如see_last
但最重要的是要按以下顺序执行这些任务
首先执行A
然后执行b和c
最后调用group结束
好的 这是一个非常基础的DAG
但足以说明任务组的使用
如果你访问Airflow UI并点击你的DAG组进入图形视图
可以看到DAG包含预期的三个任务
A、B和C
现在 假设任务B和C实际相关联
比如可以将它们分组
如何操作？通过使用任务组
让我演示如何将B和C任务分组
在DAG中使用任务组
第一步是导入任务组装饰器
在任务装饰器后导入task_group
然后需要再次创建任务组
非常简单 你将使用任务组装饰器
所以在B之前（要将B和C分组时）
在此处使用任务组装饰器
然后需要创建一个函数
我们将其命名为my_group
好的 在该函数内部
my_group 放入两个任务
B和C如这样
现在你有了这个
最后一步只需调用my_group，此处显示B和C
移除原任务并调用my_group
现在保存文件
返回UI界面
刷新页面
现在可以看到任务A
更重要的是出现了一个任务组
我的组 但如果你仔细看看这里
你可以看到该组中没有任何任务
所以为什么会这样呢
如果你回到你的DAG
记住b和c需要显式调用否则在Airflow中
这些任务将不存在
所以在我的组的任务组中
你需要调用b和c
并且你需要定义依赖关系
所以你希望以什么顺序执行组内的任务
所以你先执行b
然后执行c 保存文件并返回UI
现在再次刷新页面
这次你可以清楚看到预期的两个任务
如果你点击该任务组
展开它
可以看到任务b和c
正如你所见 在Airflow中创建任务组以分组任务非常简单
现在你可能会问
能否在任务组中嵌套任务组
是的 可以，让我展示如何操作，回到代码编辑器
假设你想将c放入不同的任务组
但在任务组my group中为此你需要再次使用task_group装饰器
然后调用该任务组
我的嵌套组
并在其中放置任务c
别忘了在任务组中调用任务c
否则UI中无法显示
就是这样
最后需要在此处移除c
改为调用任务组
保存文件并返回UI
然后刷新页面
现在可以看到你的任务组
my group包含任务b
但有一个新任务组
名为my group
同样展开它
可以看到预期的任务c
作为最佳实践
我不建议使用超过一个嵌套任务组
为什么呢 因为可以想象如果有很多任务组和嵌套任务组
团队成员理解DAG和任务会非常困难
假设有一个任务组
在任务组内部
可以想象会变得多么复杂
因此，作为最佳实践
保持一个列出的任务组
这应该就足够了
任务组的一个酷功能是可以使用参数自定义单个任务组
例如 默认情况下，Airflow中的任务在失败前不会重试
但假设你想在任务失败前重试任务b两次
但这样做其实很简单
你只需为任务组使用default参数
该参数接受一个字典
而该字典可以包含任务支持的任何参数
例如，你有retries参数
这里你想将其设置为两次
通过这样做，任务组中的所有任务
我的组将在失败前重试两次
这意味着任务b将在失败前重试两次
但同样任务c也会
因为任务c在嵌套组中
而该组本身属于主组
现在我的组
假设你想在任务c失败前重试三次
对于任务组 主组
你需要定义不同的重试参数
再次使用default_arguments
但这次重试次数不同
好的 这是一个非常强大的功能
如果你想为一组特定任务应用默认参数
最后，你可能想知道如何在任务间共享数据
好吧 让我展示这其实很简单
假设对于任务a
而不是在标准输出打印a
我们想返回一个值如42
并显示任务a返回的值
即42与任务组中的任务b
我的组，因此你需要在任务组中添加以下参数
我的组添加一个名为val的整数参数
然后在任务b中添加另一个参数
我们称之为my_val
同样为整数类型
然后我们可以打印my_val
例如加上42
在任务组我的组中
确保调用b时传递val参数
好的 最后确保从任务a获取值
因此创建新变量val
并在调用my_group时使用
你可以像这样将值传递给测试组
好的 只需这样做
你就能将a返回的42值与任务组中的任务b共享
我的组只是确保你为参数使用不同的名称
在任务组层级
参数是val
然后对于任务
它是我的val
如果在任务b中直接使用val则无法工作
你会收到错误提示
保存文件并返回到du的UI界面
如果刷新页面
确保没有传递错误
然后可以触发die操作等待图表完成
现在已成功完成
点击它
如果点击任务b查看日志
可以看到预期的84值
恭喜
因为你能够在任务间共享数据
最后我想强调使用任务组时的注意事项
是任务的ID
在任务组中会带有任务组名称前缀
例如 这里的任务b带有任务组名称前缀
我的组 任务b的唯一标识是my group
点b，这在从其他任务获取b的值时可能很重要
或者当你使用分支Python操作符执行不同任务时
需要引用该任务ID
只需记住任务组内任务的ID
会带有任务组名称前缀
如果你在嵌套任务组中有任务
那么任务c的ID是my group
点minus state group
点c，这就是你现在看到的
关于任务组就讲到这里
当有多个可分组任务时非常实用 这能帮助你更轻松地导航图表和任务
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/54_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p54 3. Sharing data between tasks with XCOMs.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 你将深入了解Airflow中一个超级重要的机制
XCom机制将教你如何在任务间共享数据
让我演示给你看
好的 让我们想象这样一个包含四个任务a、b、c、d的数据管道
你希望在这些不同任务间共享数据
例如 任务a可能返回一个值
假设是42，你想将这个值42共享给任务b
在Airflow中如何实现这一点呢
存在一个名为XCom的机制
允许你在任务间共享数据
通过将要共享的数据存储在Airflow元数据库中（默认设置）
具体来说 你将拥有Airflow元数据库
然后使用稍后会看到的方法
将值42推送到Airflow元数据库中
然后在任务b中获取该值
从任务b中拉取值42
就这样 你就能在任务b中获取到42
同样操作应用于其他任务
假设在任务b中
你想共享另一个值如15
将该值推送到数据库
然后从任务c中拉取该值
以便实现这样的数据共享
要将值推入Airflow元数据库
将使用xcom_push方法
要从元数据库拉取值
将使用xcom_pull方法
这些方法需在需要共享数据的任务中使用
你将在下一视频中看到
如果你想知道什么是XCom
现在你可能觉得42只是一个可以直接推送的数字
请记住这个值 42会被存入对应XCom的容器中
该容器包含多个字段以唯一标识XCom
例如 你将有一个XCom名称作为键
存储要共享的值
通过使用XCom
还会记录创建时间、任务ID
哪个任务创建了该XCom以及DAG ID
哪个DAG创建了该XCom，可能还有其他字段
但需记住这些字段用于唯一标识XCom
以便从数据库正确还原到任务
你可能想知道能共享多少数据
嗯 存在一些限制
最终取决于你使用的数据库
因此 例如 如果你使用PostgreSQL
你可以为每个XCom共享最多1GB的数据
如果你使用SQLite
则可以为每个XCom共享最多2GB的数据
而如果你使用MySQL
在这种情况下 你可以为每个XCom共享最多64MB的数据
我的建议是使用
XCom适用于共享少量数据如元数据
如果你需要在任务间共享大量数据
则需要依赖更高级的概念
XCom后端
因此你不会将要共享的数据存储到Airflow元数据库中
在另一个工具中
比如S3存储桶
这样你就不再有任何限制
如果你想知道如何操作
查看我的其他课程
《Airflow实战指南》
你将看到如何设置你的XCom
稍后回来 总之
我们将专注于基础知识
这已经足够了 下一节视频见，看看XCom的实际应用
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/55_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p55 4. XCOMs in action!.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中，你将了解x的实际应用，或者更具体地说
你是如何在任务之间实际共享它的
让我们开始吧 好的，首先
在dx文件夹中创建一个名为xcom的新DAG，由pi
然后从Airflow SDK导入DAG和任务装饰器
然后使用无参数的DAG装饰器
并定义DAG
我们将其命名为xcom标签
然后我们可以创建一些任务
所以我们将创建一个任务a
或者实际上创建一个暂不执行的t1任务
然后我们将创建第二个任务
同样暂不执行的t2任务
我们希望先执行t1再执行t2
别忘了最后调用DAG
好的
现在想象你需要在t1和t2之间共享数据
实际上假设有一个值如42需要与t2共享
如何做到这一点呢
第一步是将值42推送到Airflow元数据库
这样t2可以访问该值
如何将该值推送到Airflow数据库
有两种方法
第一种是使用xcom_push方法
需要访问任务的上下文
这里添加新参数context和类型context
你可以在Airflow SDK中找到
所以像这样使用context
现在你有了任务的上下文
通过输入context访问xcom_push方法
然后搜索task_instance键
然后调用xcom_push
接下来需要定义一些参数
第一个参数是xcom的键
作为该xcom的标识符
我们将其命名为my_key
然后值是你想要与其他任务共享的值 在这种情况下
值对应42
就这样，你将值42推送到Airflow元数据库，键为my_key
你可以在t2中通过引用该键获取该xcom
好的 现在我们有了这个，看看t2
要获取该xcom
需要使用相同的机制
再次访问t2的上下文
然后使用context
访问任务实例
这次使用xcom_pull方法
这里有两个你需要使用的参数
第一个是任务ID
等于推送XCom的任务ID
在这种情况下你要找的是t_one
所以这里填t_one
然后是你要查找的XCom键名，比如my_key
好的 现在在这里创建新变量val等于该值
我们可以打印这个值，没问题
这就是经典方法
我认为这是在任务间共享数据的方式
你需要访问任务的上下文
然后调用x并推送，使用键和要共享的值
在另一个任务中再次访问上下文
使用x_can_pull
引用任务ID和目标XCom的键
但我觉得这需要不少代码对吧
只是用来展示数据
代码量有点多，不够简洁
现在让我展示第二种方法
我推荐使用这种方法
查看任务t_one
无需显式调用
x_can_push
我们将返回值42
就像在推送值一样
但无需显式调用
x_can_push
让我展示可以移除上下文
因为不再需要了
然后删除那行代码
改为直接返回val
这等同于执行xcom推送
使用键return_underscore_value和值
好的，别忘了顶部添加类型注解
这里返回整数类型
就是这样
我认为这比显式使用x_can_push更直接
现在这种方法更简洁
关于任务t_two
如何不显式调用x_can_pull来获取XCom
甚至更简单
只需移除上下文参数
添加对应要拉取的值参数
这里可以设为val整数类型
与推送值的类型一致
然后删除这行，直接打印val
但最后需要从t_one获取值
创建新变量value等于t_one
然后调用t_two
你只需将val作为t two的参数传递
就这样你就能在t one和t two之间共享它
好的 我想向你展示一个更高级的功能，我觉得你会喜欢
想象一下你想要从t one推送多个值
实际上你希望将t one的多个值共享给t two
如何做到这一点
但其实很简单
不再返回单个值
你要返回一个值的字典
假设你有一个新变量
我的句子
在这里我们称之为
我的val和我的句子是字符串
比如现在你不再返回val了
你要返回一个字典，字典的键对应x来的键
推送时对于第一个xcom
键是my val，值是my val
好的 所以42，然后下一个xcom
键是my sentence，值是my sentence
所以
然后你需要修改返回类型
这里是一个字符串字典，键对应任意类型值
因为你有整数和字符串
别忘了顶部导入typing
所以从typing导入Dict和Any
现在你要创建两个xcom，一个42和一个world
接下来问题是如何从t two拉取这些xcom
很简单
不再使用整型的val
你可以用data
与xcom相同的类型
推送一个包含以下类型的字典
然后你可以这样做打印data
引用键 所以my val
然后打印data
这次是my sentence
就这样
你就能拉取两个xcom并打印到标准输出
并且无需修改依赖项
让我演示一下
保存文件并运行流程
我看到如你所见
我们有 如你所见
我们有无错误的xcom DAG
所以触发它
等待直到流程成功完成
完美 让我们点击它并查看图表视图
你可以看到这两个任务已成功执行
但更重要的是
如果你查看这两个任务实例
在日志中可以看到值42以及一个
然后返回图表并点击t1，查看x comes
可以看到t1已返回2
x comes1对应我的句子，值为a world
然后还有一个键为
我的值为42
正如预期的 关于x comes的简要说明
这就是你在任务间共享数据的主要机制
我建议你坚持这种方式 而不是显式调用x compull和x can push
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/56_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p56 5. Chossing tasks based on conditions.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 你将发现一个简单却强大的概念，称为分支
让我演示一下，假设你有一个以下的数据管道
你的目标是根据条件选择任务c或d
例如，任务a返回一个值
比如数字一
然后在任务b中有一个条件判断
如果a返回的值大于或等于一
则执行任务c，如果值小于一
则执行
所以问题在于
如何在这里实现检查a返回值的条件
这就是你需要使用我们所说的
分支，分支是Airflow中的特殊概念
它允许根据条件选择一个或多个任务
你有多个操作符来实现这一点
让我在代码中展示这是什么意思
在dax文件夹中创建新dag文件，命名为branch.py
所以dag和task
然后创建名为branch的dag
并创建一些任务
首先是任务a，不做任何操作
暂时任务b不做任何操作
然后任务c不做任何操作
最后任务
d同样不做任何操作
最后别忘了调用dag为branch
好的 目前这些任务都只是空操作
假设任务a返回值如一
这里需要执行任务c
如果a返回一
否则执行任务d，我们先改任务名称
例如这里设为等于一，这里设为不等于一
那么该如何实现呢
需要使用task p
需要使用分支操作符
使用分支操作符根据条件执行不同任务
根据条件选择执行
需在任务装饰器中添加.branch
就是这样
现在这个任务成为分支任务
具体来说，你需要在该任务中创建条件
如果等于一
则返回要执行的下一个任务的id
在这种情况下是等于一
否则返回不等于一的任务id
通过这样做
通过返回任务id，Airflow将执行对应任务
别忘了将val作为任务b的参数
这是一个整数
因为该值是整数
差不多就是这样
现在你已成功实现
你在Airflow中进行了分支操作
现在可以根据条件执行其中一个任务或另一个
但最重要的是需要定义依赖关系
所以在这里DAG底部
你将先运行一个返回值的任务
然后需要将该值传递到这里
并执行等于一的情况
或不等于一的情况
好的 确保使用分支操作符或分支任务时
总要在其后指定任务列表
否则就没有可选任务了
好的 现在有了这些，你可以在这里打印获取的值
例如等于一的情况
这里则是不等于一的情况
保存文件 现在通过Airflow UI查看
如果没有语法错误
你应该能看到你的分支DAG
现在触发执行
等待DAG成功运行完成完美
点击图表进入流程视图
可以看到已成功在DAG中实现分支
任务a返回值为一
然后根据该值执行任务b
选择等于一或不等于一
可以看到因为值等于一
嗯 等于一的任务被执行了
而不等于一的任务被跳过
不妨将任务a返回值改为零
观察不等于一的任务是否执行，等于一的任务是否跳过
现在我想展示其他内容
返回代码部分
除了等于一的情况
当值等于一时还需运行另一个任务
嗯 这也是可以的
需注意分支任务可返回多个任务
从分支任务获取ID
这很简单
只需在此处添加列表
并将需运行的其他任务ID加入列表
例如可以执行run_if_one
现在你需要实现相应的任务
如果为一 如果值等于一
那么将执行两个任务
等于一并运行if一
好的 让我们回到等于一的情况
现在你可能会想知道如何传递到下游
任务等于一和不等于一
因为任务b确实返回任务ID
但不返回val
那这里该怎么办呢
第一步是为任务添加对应参数，这里
然后对于不等于一同样使用整数类型
然后修改打印语句
这里我们将使用字符串打印val，同样处理此处
不等于一
同样使用f字符串
现在将值传递给两个任务
等于一和不等于一
猜猜你需要用什么返回值
所以这里和这里使用a的返回值并保存文件
现在看起来有点奇怪
但如果你这样做并返回UI
刷新DAG视图
确保没有解析错误
然后点击分支
可以看到你的DAG与之前有所不同
正确，现在不等于一依赖于任务a和b
等于一同样依赖于任务a和b
但运行DAG时
最终行为与之前完全相同
让我们这样做
稍等片刻
好了 点击运行
可以看到不等于一的任务被跳过，如同之前
依赖关系确实合理
因为a与不等于一和等于一共享值
b也与不等于一和等于一有依赖
因为它选择执行哪个任务，如你所见行为相同
但现在点击等于一并查看日志
可以看到预期的等于一
这来自任务a的值
实际上点击任务a查看XCom
可以看到值为一
这就是如何根据条件执行任务的分支机制
在你的DAG中 这是Airflow中非常强大的机制
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/57_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p57 1. What you will build.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


祝贺 您已进入课程的最后一部分
而这一部分是为我准备的
可能是最有趣的部分
但也最具挑战性
确实如此 您将为airflow创建新功能
可以说是一个新的airflow操作符
在课程中您使用过迷你装饰器
可以说主要是单个装饰器
任务装饰器
如果您还记得，装饰器类似于操作符
它能更轻松快速地创建任务
例如 在这里使用任务装饰器
您可以创建运行以下Python代码的任务
实际上后台使用的是Python操作符
或者如此
例如任务bash装饰器
它允许执行bash命令
例如 此任务返回要执行的bash命令
此外 您还有任务点火花装饰器
它更复杂，可运行pi火花作业
如果您不了解火花是什么
它是一个数据处理框架
只需记住airflow有许多可用装饰器
事实上 如果您想知道可用的装饰器
查看源代码
可以看到右侧
有Python装饰器和虚拟装饰器
若要在虚拟环境中运行Python代码
也可使用外部Python装饰器
有分支装饰器
若需选择不同任务
根据条件判断
同样可以实现 有短路装饰器
Docker操作符
若需在Docker容器中运行代码等
因此有这些迷你装饰器
但有一个走廊装饰器在我看来缺失了
这个装饰器是SQL装饰器
我确信 您记得需要运行SQL查询
以在项目中创建表
为此需使用SQL执行查询操作符
但现在想象一下，无需显式使用SQL执行查询操作符
你可以使用装饰器
相反这个走廊装饰器
后续的走廊装饰器
而这个后续的走廊装饰器并不存在
但你会创建它
现在只需返回SQL查询
你就可以运行它，这样你看到的代码更少，编写和阅读都更快
同时你的SQL查询现在位于Python函数中
这意味着在SQL查询和函数名称之间
因此在这里你可以执行所需的Python代码
这为你提供了极大的灵活性
你可以根据之前的Python代码创建该SQL查询
所以再次 一个非常令人兴奋的项目
一个相当有挑战性的项目
但我相信你已经准备好了 那么我们在下一个视频见
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/58_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p58 2. Some important concepts.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 我将向你展示我们想要创建的内容的高层次概览
我们开始吧，好的
请记住在课程开始时你学过Airflow有很多提供者
基本上在核心部分你有Airflow核心包
没错 这就是你需要安装以运行Airflow的包
好的 当你需要获取Airflow的基本功能时
每当你想与某个工具或服务交互时
例如服务或Databricks
在这种情况下你需要安装对应的提供者
例如你将拥有Databricks
好的 这会添加一些操作符
钩子和功能到你的铜猫
你的Airflow
然后你可能需要安装亚马逊的版本
好的 这样你可以与AWS服务交互，同时带来更多功能
操作符和其他内容到你的空气
例如 这就是工作原理对吧
你有Airflow和许多提供者
实际上 如果你想知道提供者的作用
你可以访问 我强烈建议你查看这个网站
如你所见在左侧
提供者列表
在这里你可以看到所有Airflow支持的提供者
例如 如果你想与字节互动
读取Flink等，只需安装对应提供者即可
提供者是一个Python包，包含操作符
以及装饰器
还有钩子
好的 记住钩子是连接外部服务或工具的函数
它还可以有连接类型
例如
如果不安装亚马逊提供者
你将无法找到连接类型
在UI中的AWS等
现在我们想构建的是一个新的Python包
一个新的提供者
这个新提供者将非常简单
我们将称它为我的SDK
并在其中包含一个操作符
还有一个装饰器
因此，一旦我们安装了我们的提供者SDK
我们将能够使用我们自己的装饰器
我们将从Airflow实例的DAGs中创建的装饰器
好的 这就是核心思路
你将创建自己的提供者，包含自定义操作符和装饰器
你可以在Airflow中使用它们
因此这不仅有趣
因为你将学习如何创建自定义操作符和装饰器
同时这对当前在公司工作的你也很实用
你想要封装
你想要打包自定义操作符或装饰器
并在不同项目中复用
这就是为什么这个项目非常有趣
现在你对我们将要创建的内容有了更清晰的认识 让我们在下一视频开始实践
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/59_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p59 3. Creating the Airflow Provider.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中你将开始创建你的提供者
我们马上开始吧
当你想要创建一个提供者时
第一步是创建一个文件夹
这个文件夹将作为你的提供者名称
请确保你位于dax文件夹之外
但仍在你的项目目录中
现在创建一个新文件夹
我们将命名为my_dash_sdk
好的，再重复一次 这将是你的提供者名称
在该文件夹中我们将创建另一个文件
即pi_project.dot_tul
顺便说一下，我们会在这个文件中添加很多内容
我不希望你在视频中从头编写所有内容
你将找到所需的所有资源
所以可以直接复制粘贴
如果你想知道这是什么文件
嗯 这是一个由打包工具使用的配置文件
比如一组工具
以及其他工具
比如代码检查器
类型检查器等等
但基本上当你创建Python包时
现在强烈建议使用这个文件
在这个文件中有几个配置设置
首先是构建系统
好的 你想要使用的打包工具
在这种情况下我们将使用ling工具
这并不重要
无论你是否熟悉该打包工具
只需记住这是我们将用来构建包的工具
然后在构建系统设置下
我们需要指示打包工具在哪里找到包版本
在这种情况下包版本将位于该文件夹中
my_sdk再次
我们稍后会创建这个文件夹
然后在设置下我们需要添加另一个配置
即我们要构建的内容
这里我们要构建文件夹中的内容
my_sdk我们尚未创建该文件夹
接下来还需要指定项目相关信息
例如我们要给包命名，即my_dsdk
然后版本、包的描述
即my_sdk 你可以随意填写这里的内容
README文件的位置
以及该包所需的Python版本
在我们的情况中，版本需大于或等于三十分之一
此外我们可以指定依赖项
正如你所见我们有两个依赖项
显然Apache Airflow
版本应大于二点七
七点零和类型扩展
最后我们要在该文件中添加的设置如下
你必须指定此设置
因为它告诉Airflow此包是Airflow提供者
获取提供者信息是一个返回关键信息的函数
例如描述
要导出的装饰器
等等 好的 所以一旦完成配置文件就准备好了
现在让我们创建一个readme因为必须创建readme
如你所见这里
在我的Dash SDK文件夹中
我们可以创建readme.md
在其中内容并不重要
暂时只需添加提供者标题
好的 我的Dash SDK
但如果你没有readme
将无法构建你的包
至此我们完成了创建提供者的第一个步骤在下一视频
我们将创建返回关键信息的函数
获取提供者信息 以便Airflow识别此包为Airflow提供者
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/60_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p60 4. Defining provider information.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 你将实现获取提供者信息函数
这可以将你的Python包转换为兼容的Airflow提供者
让我们开始吧，首先表明你的Python包
我的Dash SDK是Airflow提供者
你需要实现获取提供者信息函数
因此在my_dsdk文件夹中操作很简单
创建一个新文件夹
my_underscore_sdk
然后在该新文件夹中
创建一个新文件__init__.py
在__score_score.py中
像这样操作
下一步是定义该函数
所以这里输入def
然后定义get_provider_info函数，返回类型如下
我们需要导入该类型，所以执行from typing import
现在该函数返回一个字典
该字典包含描述你的Airflow提供者的字段
所以这里可以返回一个字典
第一个键是包名
然后将包名设为my_dash_sdk
接着给出提供者名称
这里填my_sdk
还可以提供描述
例如 类似我的SDK
提供构建Airflow DAGs的工具集
为什么不呢 但你可以自行填写
然后定义版本字段
稍后会详细说明
最后 但最重要的是任务装饰器
你希望从提供者中暴露的装饰器
该字段包含字典列表
我们只需一个字典
因为我们即将创建一个装饰器，该装饰器
装饰器名称为sql
类的位置路径如下
即my_underscore_sdk
对应此处的文件夹
然后点decorators
点sql和sql_task，将在下一视频解释
这个路径具体含义
差不多就是这样
仍有两件事要添加
在文件顶部
需指定提供者版本
例如或类似版本
然后还需要导出你想要的装饰器
这将是sql
现在你可以保存文件并完成操作
此时你已成功定义了提供airflow信息的get_provider_info函数
我的sdk再次
如果你不提供该函数
你的python包将无法成为airflow provider
同时请注意，get_provider_info实际上是你的airflow provider的入口点
如在pi项目文件中定义的
这正是你可以在这里找到的内容对吧
你拥有apache airflow provider
然后你看到这里的入口点
对应该文件夹的我的sdk
我的下划线sdk
然后是get_provider_info
而这正是你现在看到的对吧 所以在下一视频中你将实现自定义操作符，供你的装饰器使用
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/61_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p61 5. Creating the custom SQL Operator.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中，你将创建一个自定义操作符，你的装饰器将使用它
我想说这个视频可能是本节中最难的一个
至少要跟着我一起坚持下去
我会给你所有需要的解释
但请记住这一点
它会比之前看到的稍微难一些
我们开始吧，好的
第一步是创建一个新文件夹
装饰器文件夹
我的sdk 所以在这里创建一个新文件夹
装饰器做
然后在该新文件夹中
创建一个新文件，命名为sql.dot.by
你将在该文件中实现自定义操作符
请记住我们正在创建新装饰器
任务SQL装饰器，你可以在这里找到
接收一个SQL请求
然后只需返回该SQL请求
它将自动执行
为此我们依赖SQL执行查询操作符
好的 我们不会创建新操作符
我们将创建新装饰器
它将使用SQL执行查询操作符来实现，现在回到代码
第一步是创建新类
命名为_sql_decorated_operator
继承自两个类
DecoratedOperator和SQLExecuteQueryOperator
如你所料
我们声明_sql_decorated_operator是装饰器操作符
并将使用SQL执行查询操作符，一旦完成
下一步是添加模板字段
我稍后会解释这是什么
然后序列
等于以下行
好的接下来 在下方添加另一个属性
这次是template_fields
renderers
类型为class dict等于DecoratedOperator
template_fields
renderers
然后SQL
ExecuteQueryOperator
点template_fields
然后在文件顶部进行相应导入
从typing导入Sequence
然后从Airflow SDK导入装饰器基类
我们导入DecoratedOperator
然后从 airflow 提供商的通用 SQL 操作符开始，比如 SQL
我们导入 SQL 执行查询操作符，好的
现在让我在 Airflow 中为大家做一些解释
你有一些具有模板化字段的操作符
这意味着这些字段可以包含在运行时渲染的值
让我举个例子
所以回到用户处理 DAG
并查看 SQL 执行查询操作符
SQL 执行查询操作符有一个 sql 参数
这可以具体化
你可以传递一些在运行时渲染的值
当此任务执行时
你的 SQL 查询将被渲染
让我展示一个具体例子
假设你想在 SQL 查询中添加以下条件
如你所见
日期是硬编码的
正如你所知 不应该硬编码日期，因为
如果运行该查询
你将始终为该日期获取相同的数据块
好的 你不需要关心创建表
等等 这只是为了举例
我知道这个查询不工作
但这只是为了说明
所以问题是
如何避免在这里硬编码日期呢？
这就是需要使用模板化的地方
与其这样硬编码日期
你可以移除并使用带有两个大括号的特殊语法
然后可以使用 Airflow 的特殊值
即 ds
好的
然后保存文件并进入 Airflow 界面
然后触发 DAG
顺便说一句不需要这样做
这只是为了举例
好的 你看它失败了
但点击任务然后进入渲染模板
可以看到这里的日期是当前日期
如果明天再次运行 DAG
日期会变化
这就是模板化的强大之处
你可以注入在运行时渲染的数据，说到这里
回到我们的自定义操作符
我们在这里合并了模板化字段
来自装饰操作符和 SQL 执行查询操作符
我们希望将它们用于我们的下划线 sequel 装饰器操作符
例如 通过 sql 执行查询操作符可用的 sql 参数
同样可自动补全的 sql 参数也将出现在下划线 sql 装饰器操作符中
这就是我们现在正在做的事情，对吧
接下来需要定义的字段
自定义操作符名称，使用你的装饰器名称
装饰器将位于 task 点
对应通过 sql 装饰器定义任务的方式
如图所示
最后一个属性是执行后的覆盖或 tif
将该参数设为 true
这是与模板字段相关的内部属性
这里需要大量代码生成
需要定义我们的 sequel 装饰器操作符结构
顺便说一句，你不必全部自己编写代码
如果你想的话 可以在 udemy 视频下方下载源代码
要做到这一点 你需要定义以下函数下划线评分在 conscore
然后使用以下参数
这些参数与 python 操作符参数基本一致
这不再需要，因为后台
请记住，任务装饰器是 python 操作符
而我们现在
将 sql 执行查询操作符封装在 python 操作符中
然后在构造函数中需要这样做
如果存在多个输出
则需发送以下消息表明不支持多个输出
多个输出是某些操作符可接受的参数
这是一个高级参数，允许导出多个 x 正常
但再次强调 这是高级功能
如果你不了解该功能，可随时在网上搜索
如果你想了解更多
最后在该条件之后
我们将初始化 sql 装饰器操作符继承的类
从装饰器操作符和 sql 执行查询操作符继承，使用 super
然后在 score 中使用以下参数
还需添加 sql 等待执行时设置
然后多个输出等于 false
如你所见
我们获取 sql
来自 sql 执行查询操作符的 sequel 参数
python 可调用参数来自装饰器操作符
现在我们需要在此处进行一些导入
在文件顶部需导入 set during execution
然后需导入任意集合和映射
别忘了添加 col
对于序列，实际上需从 typing 导入序列
但首先从collections.abc导入
然后导入sequence模块最后
但最重要的是需要导入warnings
所以让我们将导入语句放在最后像这样
好的 此时查看你的代码
不应该有任何警告对吧
到目前为止我们已经定义了所有sql装饰器操作符所需的属性
根据类结构
无论是读取装饰器操作符还是sql查询操作符
同时在这里的初始化函数
我们也初始化了操作符及其父类装饰器操作符和sql
使用super.init初始化执行查询操作符
然后用python参数
Coopiogs
Sql等等
这样我们就能正确初始化自定义sql装饰器操作符
下一步是定义另一个函数
但这次这个函数是execute和execute
接受以下参数并返回任意类型
这个函数将在airflow任务运行时立即执行
因此在该函数中需要完成几件事
首先是调用上下文函数
与context和self.op_key合并
然后需要创建新变量
key_walks等于key_walks
然后python
Credible Opix和context
我不会深入细节这些函数的作用
但请记住当在airflow中运行任务时
该任务会附加一个上下文
并且基本记住你使用该上下文在任务间共享
正确与任务实例
任务的上下文
但我们在这里是将任务上下文与key_walks合并
因此当调用任务时
可以添加一些参数
使用关键字和op
这就是我们在这里再次操作的
即使不完全理解也没关系
但需记住我们正在合并context与opix和key_walks
这些是可添加到任务的参数
下次调用时
定义self.sql
即要执行的sql请求值
self.python_callable
然后self.op_with_k_walks
请注意sql装饰器操作符是一个装饰器
这就是我们在这里所做的
我们将执行python可装饰函数
这是一个将被装饰的python函数
而该python函数返回sequel查询
我们希望完全按照此处所示执行
好的 这将是self.sql中的内容
好的 下一个
验证sequel请求是否为字符串
为此我们可以使用以下条件
如果sql不是字符串类型
如果sql请求为空
则需要抛出以下错误
任务流可装饰函数的返回值必须为非空字符串
即如果sql请求为空
或它不是字符串类型
将抛出错误
然后最后 但最重要的是需要使用上下文
并在此处调用任务实例的上下文
测试实例对象以渲染模板
请记住这是之前看到的内容
如果你在UI中查看任务
可以看到 这就是渲染模板
所以我们在这里做的是
渲染我们想要执行的sequel查询
最后我们只需返回super.execute
因为我们想要执行sql查询
使用sql执行查询操作符
这就是为什么我们调用super.execute
并传递上下文以使用该操作符执行sql查询
这就是sequel装饰操作符的全部内容
我们还需要导入context_merge和determine_k
在文件顶部只需执行以下导入
从airflow.utils.context导入context_merge
然后从airflow.tiis.operator.on_score_helpers导入determine_key_walks和last
但最重要的是从airflow.sdk.definitions.context导入context
好的 我知道这段视频特别难
这正是我一开始告诉你的
请慢慢来 不妨再看一遍视频
但总结一下 你创建了一个继承自装饰操作符的sql装饰操作符类
以及sql执行查询操作符
你定义了如何在dags中使用该装饰器
使用@task.sql
然后在这些操作符上
需要在上下文中初始化sequel装饰操作符
更具体地说调用super.__init__以初始化父类
装饰操作符和执行SQL查询操作符，参数如下
你可以在这里看到，这个SQL参数来自SQL
执行查询操作符
而Python电缆来自装饰操作符
然后你定义了将在任务运行时调用的执行函数
在该函数中，你获取Python可调用函数返回的内容
因此，Python函数返回的实际上是SQL请求
这就是为什么你要将其存入self.dot.sequel
然后你检查SQL请求是否为字符串或非空
最后只需调用super执行以运行SQL请求
通过调用执行方法来自SQL执行查询操作符，好的
所以恭喜
如果你走到了这一步
你现在做到了
是时候进入下一阶段了 也就是 创建用于装饰器的sql_task函数
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/62_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p62 6. Creating the new SQL decorator.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 你将创建一个Python函数，对应你的装饰器
而这个装饰器将使用你在上一个视频中完成的内容
SQL装饰器操作符类
让我们开始吧 相信我
这个视频会比上一个简单很多
所以你只需在sql.dot.pi文件末尾
好的 在sql装饰器操作符类之后
创建一个对应你的装饰器的新Python函数
所以这个Python函数将被命名为sequel_task
并且该函数需要两个参数
第一个参数是python_colon
参数类型和键值如所示
这个函数返回一个任务装饰器
这就是为什么需要task_decorator类型
然后在这个函数内部
返回一个任务装饰器对象
通过调用task_decorator_factory函数
该函数期望三个参数
第一个参数是python_cable，来自此处的参数
然后是decorated
操作符类
即你在上一个视频中创建的类
所以是sql_decorated_operator然后最后
但最重要的是key_walks
就是这样现在
你只需导入task_decorator类和task_decorator_factory函数
在文件顶部从airflow_sdk.bases.decorator导入
导入task_decorator和函数
就是这样
你已成功创建了对应装饰器的函数
实际上 查看你之前创建的init文件
在get_provider_info函数中，在decorators部分
我们有名称sql，即我们的装饰器名称
但类名对应我的sdk
所以该文件夹下的decorators
该文件夹下的sql文件
然后是sql_task
所以这个函数
好的，这就是原因
当你使用@sql装饰器时
例如以下Python函数
比如返回SQL请求的my_task函数
实际上这个Python函数将在这里执行
这就是对应的Python函数
对应这个Python函数
然后使用sql装饰器操作符执行该SQL请求
请记住，您将在此处获得正确的SQL请求以执行它
使用此处的SQL执行查询操作符
好的 就是这样
现在您有了Python函数
一切准备就绪
您已成功创建了装饰器
现在是时候看看如何安装您的Mice SDK提供程序
因此您可以使用您的SQL装饰器 让我们在下一视频中探索这一点
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/63_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p63 7. Installing your Provider.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在是安装您的提供商的时候了
我的sdk在你的air中
例如 就像你知道的那样我们来做吧
我们正在docker中运行airflow
实际上如果你打开docker桌面
你会看到对应airflow实例的不同docker容器
比如postgres
调度器 API服务器
触发器和DAG处理器
现在我们的提供商
我的sdk在pi上不可用
因此我们无法在
我告诉你通常的方法
相反我们需要创建一个docker镜像复制这个文件夹
对应我们提供商的my sdk
然后通过执行pip命令安装该提供商
安装并指定本地路径
第一步是创建docker文件
好的 请确保你在airflow入门的根目录
然后创建一个这样的docker文件
然后在docker文件中指定要使用的airflow镜像
即apache airflow三零版本
然后将my dash sdk文件夹从你的机器复制
到docker容器的以下路径
斜杠airflow
斜杠my sdk
只需这样做提供商就在运行airflow的docker容器中可用
最后一步是运行pip命令
安装dash e并使用该路径如airflow
my sdk
就是这样你可以通过本地路径安装提供商
现在只需保存文件
然后打开docker compose
你不再想使用这个docker镜像
对的你要使用自己的docker镜像
可以注释掉这行
然后取消build的注释
这样当你运行airflow时
使用docker compose命令
将构建docker文件以获取对应你的
例如带有提供商的镜像
我需要my dash sdk 好的
保存文件
现在去终端
确保通过按control c停止airflow
然后查看桌面
确保不再有任何正在运行的Docker容器
好的，我们现在回到终端
在这里你需要运行命令docker
Compose up ash dash build来构建Docker文件
并用你的提供商打开Docker镜像
现在运行这个命令
进入并等待一会儿
正如你看到的
注释按预期执行了
现在我们正在安装自己的提供商
我的SDK在运行Airflow的Docker镜像中
好的 那我们等待下一个步骤
几分钟后
你应该能看到不同的Docker容器正在运行
那我们继续等待
看起来它们都已启动并运行
看来我们成功安装了自己的提供商
我的SDK
接下来在视频中
我们将创建使用新装饰器的DAG
SQL装饰器来执行SQL请求
让我们看看到目前为止所做的 一切是否顺利
```

### /content/drive/MyDrive/bilibili/TheCompleteHandsOnIntroAirflow3/64_Udemy - The Complete Hands-On Introduction to Apache Airflow 3 p64 8. The SQL Decorator is ready!.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


哇 我不确定你
但我非常兴奋去验证
如果到目前为止我们所做的都符合这个
所以让我们看看是否能使用我们的新装饰器
或者后续装饰器
顺便说一下，Airflow中并没有这个
至少在我录制这段视频时
这将允许你执行SQL请求
通过从Python函数返回SQL请求
现在我们在dags文件夹中创建一个新DAG
你可以创建一个名为sql.py的新文件
这个DAG将非常简单，基于Airflow SDK
你需要导入DAG和任务
然后用装饰器实例化你的DAG
让我们将这个DAG命名为
SqlDag之类的
现在是时候使用我们的装饰器了
所以在任务.sql中
你需要指定连接ID
记得在之前的视频中你创建了一个名为postgres的连接
但如果你没有的话
我们现在要重新创建这个连接
因为我们需要运行SQL请求
所以我们需要连接到数据库
使用以下参数connection_id，id等于
顺便说一下，这个参数来自SQL执行查询操作符
我们通过装饰器传递给SQL装饰器
好的 这就是为什么我们有这个参数可用
然后创建以下Python函数
获取nb_xcomes
因为我们只需要执行以下SQL请求
统计Airflow实例中xcomes的数量
好的 在Airflow数据库中有一个名为xcom的表
你可以统计Airflow数据库中的xcomes数量
但最后你调用
获取nb_xcomes并在DAG结尾
SqlDag 所以非常简单
简单的DAG 只需验证我们所做的是否正确，保存文件
然后进入
然后前往管理页面和连接
确保你有这个postgres连接
如果没有，操作很简单
只需点击添加连接
然后将连接命名为postgre
你需要选择postgres
然后主机地址是postgres
登录名是postgres
密码是postgres
端口号是54
32 然后点击保存
好的 所以再次
现在很简单了
让我们回到代码编辑器
在终端输入ctrl c
好的 所以停止你
例如
当你停止所有docker容器后
如图所示
可以再次运行airflow
用docker compose up
减号 build 并等待docker容器启动
好的 看起来airflow已启动运行
我们可以回到airflow
Ui确保没有与sql dagger相关的错误
如你所见没有错误，效果很好
说实话 点击dags
这里就是我们的sql dig
点击按钮运行它
我们是da
等待运行完成
可以看到已成功完成
点击dag查看任务
获取nba x显示在这里
点击它
仔细查看
可以看到有一行被影响
点击xcom
可以看到任务使用了我们创建的sql decorridor
获取nbx返回以下值
包含数据库中的xcom数量
在我的案例中是34
事实上 仔细查看操作符
好的 在你的装饰器后面
就是你创建的内容
下划线 sequel 装饰操作符
所以恭喜你
因为你已经成功创建了一个全新的装饰器
这就是sql装饰器
现在你可以执行sql请求了
通过从python函数返回它们
这非常强大
不仅因为你创建了一个全新的装饰器
你还学会了如何创建自己的提供者
这对在新公司工作的人来说非常棒
并且拥有许多自定义操作符
所有走廊
你可以将它们全部打包成一个提供者
供公司内的所有dags使用
希望你喜欢这个项目
不妨用这个装饰器为airflow项目贡献代码
如果你想的话 或者任何你能想到的其他装饰器
我们下次视频再见 保重
```
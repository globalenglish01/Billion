{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfAQWpoCEqvcQxHFS6g12D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/globalenglish01/Billion/blob/main/parse_chatgpt_html.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4 pandas openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxusTQiq1mmv",
        "outputId": "50e47784-cb3d-4c6e-80cc-7bbf645c6cfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAmMlYbk1_gy",
        "outputId": "00d4bc16-706b-44bf-8268-2792635af10d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqq_OJeX1kIp",
        "outputId": "de617e88-3687-4d56-a491-3415a07591fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "解析完成，已保存至 /content/drive/MyDrive/parsed_pages.xlsx\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def clean_pre_text(text):\n",
        "    \"\"\"\n",
        "    清理 <pre> 标签内的多余文字，例如开头的“plaintext”、“复制代码”等。\n",
        "    \"\"\"\n",
        "    remove_phrases = ['plaintext', '复制代码', 'コピーコード']\n",
        "    for phrase in remove_phrases:\n",
        "        text = text.replace(phrase, '')\n",
        "    # 去除开头结尾多余空白符\n",
        "    return text.lstrip('\\n\\r ').rstrip()\n",
        "\n",
        "# 1. 读取文件，忽略非法字符\n",
        "def read_html_file(filepath):\n",
        "    with open(filepath, 'rb') as f:\n",
        "        data = f.read()\n",
        "    return data.decode('utf-8', errors='ignore')\n",
        "\n",
        "# 2. 解析内容，提取每页三部分\n",
        "def parse_pages(html_text):\n",
        "    soup = BeautifulSoup(html_text, 'html.parser')\n",
        "\n",
        "    # 获取所有文本块（包含标题和内容）\n",
        "    # 观察你提供的html结构，大致是 <p>标题</p> + <pre>内容</pre>，循环顺序依次出现\n",
        "    # 通过循环所有<p>标签，判断标题，配合后面的<pre>拿内容\n",
        "    ps = soup.find_all('p')\n",
        "    pres = soup.find_all('pre')\n",
        "\n",
        "    pages = []\n",
        "    # 这里假设标题和内容是交替出现的，即p - pre - p - pre - p - pre这种结构\n",
        "    # 或者三个一组 p-pre-p-pre-p-pre依次对应一页3部分\n",
        "    # 通过遍历p和pre，收集\"第X页 原文\"、\"第X页 中文翻译\"、\"第X页 逐行详细解析\"\n",
        "    # 并将内容保存\n",
        "\n",
        "    # 建立标题和内容的对应关系，假设p和pre对应序列是严格的\n",
        "    p_texts = [p.get_text(strip=True) for p in ps]\n",
        "    pre_texts_raw = [pre.get_text() for pre in pres]\n",
        "    pre_texts = [clean_pre_text(t) for t in pre_texts_raw]\n",
        "\n",
        "    # 存储临时信息\n",
        "    page_num = None\n",
        "    part_dict = {}  # keys: 原文, 中文翻译, 逐行详细解析\n",
        "    results = []\n",
        "\n",
        "    # 标题关键词匹配\n",
        "    def get_part_type(text):\n",
        "        if '原文' in text:\n",
        "            return '原文'\n",
        "        elif '中文翻译' in text:\n",
        "            return '中文翻译'\n",
        "        elif '逐行詳細解析' in text or '逐行详细解析' in text:\n",
        "            return '逐行详细解析'\n",
        "        return None\n",
        "\n",
        "    i_p = 0  # p索引\n",
        "    i_pre = 0  # pre索引\n",
        "\n",
        "    while i_p < len(p_texts) and i_pre < len(pre_texts):\n",
        "        title = p_texts[i_p]\n",
        "        content = pre_texts[i_pre]\n",
        "\n",
        "        # 匹配页码\n",
        "        page_match = re.search(r'第(\\d+)页', title)\n",
        "        part = get_part_type(title)\n",
        "\n",
        "        if page_match and part:\n",
        "            current_page = int(page_match.group(1))\n",
        "            # 如果是新的一页，先保存上一个\n",
        "            if page_num is not None and current_page != page_num:\n",
        "                # 确保三个部分都存在，不存在则设空字符串\n",
        "                results.append({\n",
        "                    '页码': \"第\" + str(page_num) + \"页\",\n",
        "                    '原文': part_dict.get('原文', ''),\n",
        "                    '中文翻译': part_dict.get('中文翻译', ''),\n",
        "                    '逐行详细解析': part_dict.get('逐行详细解析', ''),\n",
        "                })\n",
        "                part_dict = {}\n",
        "\n",
        "            page_num = current_page\n",
        "            part_dict[part] = content\n",
        "\n",
        "            i_p += 1\n",
        "            i_pre += 1\n",
        "        else:\n",
        "            # 遇到不匹配标题，跳过，防止死循环\n",
        "            i_p += 1\n",
        "            i_pre += 1\n",
        "\n",
        "    # 保存最后一页\n",
        "    if page_num is not None:\n",
        "        results.append({\n",
        "            '页码': \"第\" + str(page_num) + \"页\",\n",
        "            '原文': part_dict.get('原文', ''),\n",
        "            '中文翻译': part_dict.get('中文翻译', ''),\n",
        "            '逐行详细解析': part_dict.get('逐行详细解析', ''),\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# 3. 保存为Excel\n",
        "def save_to_excel(data_list, excel_path):\n",
        "    df = pd.DataFrame(data_list)\n",
        "    df = df.sort_values(by='页码')\n",
        "    df.to_excel(excel_path, index=False)\n",
        "\n",
        "# 4. 主函数示例\n",
        "def main():\n",
        "    #filepath = '/content/drive/MyDrive/chatgpt_export0.html'  # 替换成你的路径\n",
        "    filepath = '/content/drive/MyDrive/chatgpt_export.html'  # 替换成你的路径\n",
        "    excel_path = '/content/drive/MyDrive/parsed_pages.xlsx'  # 输出路径\n",
        "\n",
        "    html_text = read_html_file(filepath)\n",
        "    parsed_pages = parse_pages(html_text)\n",
        "    save_to_excel(parsed_pages, excel_path)\n",
        "    print(f'解析完成，已保存至 {excel_path}')\n",
        "\n",
        "# 运行主函数\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def clean_pre_text(text):\n",
        "    remove_phrases = ['plaintext', '复制代码', 'コピーコード']\n",
        "    for phrase in remove_phrases:\n",
        "        text = text.replace(phrase, '')\n",
        "    return text.strip()\n",
        "\n",
        "def get_part_type(text):\n",
        "    if '原文' in text:\n",
        "        return '原文'\n",
        "    elif '中文翻译' in text:\n",
        "        return '中文翻译'\n",
        "    elif '逐行詳細解析' in text or '逐行详细解析' in text:\n",
        "        return '逐行详细解析'\n",
        "    return None\n",
        "\n",
        "def parse_html_by_positions(html_text):\n",
        "    soup = BeautifulSoup(html_text, 'html.parser')\n",
        "\n",
        "    ps = soup.find_all('p')\n",
        "    pres = soup.find_all('pre')\n",
        "\n",
        "    # 收集标题：页码、部分类型、p标签位置\n",
        "    titles = []\n",
        "    for i, p_tag in enumerate(ps):\n",
        "        text = p_tag.get_text(strip=True)\n",
        "        m = re.search(r'第(\\d+)页', text)\n",
        "        part = get_part_type(text)\n",
        "        if m and part:\n",
        "            page_num = int(m.group(1))\n",
        "            titles.append({'page': page_num, 'part': part, 'p_index': i})\n",
        "\n",
        "    # 所有pre标签的位置\n",
        "    pre_positions = list(range(len(pres)))\n",
        "\n",
        "    # 记录结果：页码映射到部分内容字典\n",
        "    pages = {}\n",
        "\n",
        "    # 用于找到每个标题对应的pre标签\n",
        "    p_to_pre_map = {}\n",
        "\n",
        "    # 遍历每个标题，找第一个pre标签的位置比p标签位置大的那个\n",
        "    # 这里用ps和pres不在同一序列，这里用p标签序号对应文档中的位置不好用，改用标签在文档中的索引即可\n",
        "    # 方案：先找p标签在文档中的sourceline，然后找pre标签的sourceline，匹配\n",
        "\n",
        "    # 取p和pre标签在文档中的sourceline(行号)（有时解析器可用）\n",
        "    # 这里用bs4默认不能保证sourceline存在，改用在DOM中索引顺序来判断：\n",
        "\n",
        "    # ps[i] 和 pres[j] 在文档中的顺序：\n",
        "    # 我们认为p和pre交替出现，p[i].index < pre[j].index < p[i+1].index\n",
        "\n",
        "    # 所以先找所有p和pre的标签在文档的position序号\n",
        "    # bs4没有直接提供DOM中的绝对位置索引，换思路：html文档中p标签和pre标签的顺序决定他们的出现顺序\n",
        "\n",
        "    # 这里先把所有p和pre合并排序\n",
        "    all_tags = []\n",
        "    for tag in ps:\n",
        "        all_tags.append(('p', tag))\n",
        "    for tag in pres:\n",
        "        all_tags.append(('pre', tag))\n",
        "\n",
        "    # 在html中出现的顺序\n",
        "    all_tags_sorted = sorted(all_tags, key=lambda x: x[1].sourceline if x[1].sourceline else 0)\n",
        "\n",
        "    # 由于sourceline不一定存在，改用顺序索引来模拟\n",
        "    # 先构造一个列表，p和pre标签按照在html文本中的顺序排序，sourceline无效时使用下面代码\n",
        "\n",
        "    # 但如果sourceline不可用，不能用此方法。改用原始HTML文本用正则提取页码和内容匹配\n",
        "    # 因为bs4对sourceline支持不完全\n",
        "\n",
        "    # 换用正则和HTML文本解析更准确方案：\n",
        "    return None\n",
        "\n",
        "def parse_html_with_regex(html_text):\n",
        "    \"\"\"\n",
        "    用正则匹配页码标题和后面对应内容的pre，适合复杂页面。\n",
        "    \"\"\"\n",
        "\n",
        "    # 匹配页码和部分标题，如 “第123页 原文” ，后面跟着 <pre>...</pre>\n",
        "    pattern = re.compile(\n",
        "        r'<p[^>]*?>\\s*(第(\\d+)页[^<]*)\\s*</p>\\s*'    # <p>标题\n",
        "        r'<pre[^>]*?>\\s*(.*?)\\s*</pre>',              # <pre>内容\n",
        "        re.DOTALL\n",
        "    )\n",
        "\n",
        "    matches = pattern.findall(html_text)\n",
        "\n",
        "    pages = {}\n",
        "    for full_title, page_str, content in matches:\n",
        "        part = None\n",
        "        if '原文' in full_title:\n",
        "            part = '原文'\n",
        "        elif '中文翻译' in full_title:\n",
        "            part = '中文翻译'\n",
        "        elif '逐行詳細解析' in full_title or '逐行详细解析' in full_title:\n",
        "            part = '逐行详细解析'\n",
        "        else:\n",
        "            continue  # 跳过无法识别的\n",
        "\n",
        "        page_num = int(page_str)\n",
        "        if page_num not in pages:\n",
        "            pages[page_num] = {}\n",
        "        # 清理内容\n",
        "        content_clean = clean_pre_text(content)\n",
        "        pages[page_num][part] = content_clean\n",
        "\n",
        "    # 转换为列表并补全三列\n",
        "    results = []\n",
        "    for pnum in sorted(pages.keys()):\n",
        "        page_data = pages[pnum]\n",
        "        results.append({\n",
        "            '页码': pnum,\n",
        "            '原文': page_data.get('原文', ''),\n",
        "            '中文翻译': page_data.get('中文翻译', ''),\n",
        "            '逐行详细解析': page_data.get('逐行详细解析', ''),\n",
        "        })\n",
        "    return results\n",
        "\n",
        "# 使用示例\n",
        "#html_file_path = '/content/your_file.html'\n",
        "html_file_path = '/content/drive/MyDrive/chatgpt_export0.html'  # 替换成你的路径\n",
        "excel_path = '/content/drive/MyDrive/parsed_pages.xlsx'  # 输出路径\n",
        "with open(html_file_path, 'rb') as f:\n",
        "    html_content = f.read().decode('utf-8', errors='ignore')\n",
        "\n",
        "pages_data = parse_html_with_regex(html_content)\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(pages_data, columns=['页码', '原文', '中文翻译', '逐行详细解析'])\n",
        "df.to_excel(excel_path, index=False)\n",
        "print('解析完成，保存为 parsed_pages.xlsx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpwDBXEO35ug",
        "outputId": "3b2ef98f-781e-4483-ba37-2afba132a242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "解析完成，保存为 parsed_pages.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 挂载Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. 导入所需库\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# 3. 解析纯文本的辅助函数\n",
        "def clean_html_text(html_fragment):\n",
        "    \"\"\"用BeautifulSoup去除HTML标签，得到纯文本，并去除“plaintext”、“复制代码”等无意义文字\"\"\"\n",
        "    text = BeautifulSoup(html_fragment, 'html.parser').get_text()\n",
        "    # 去掉“plaintext”、“复制代码”等关键词，及多余空行\n",
        "    for noise in ['plaintext', '复制代码', 'コピーコード', 'copy code']:\n",
        "        text = text.replace(noise, '')\n",
        "    text = '\\n'.join([line.strip() for line in text.splitlines() if line.strip() != ''])\n",
        "    return text\n",
        "\n",
        "# 4. 解析函数：提取每页的三部分内容\n",
        "def parse_html_pages(html_text):\n",
        "    # 正则匹配所有 <p>标题 + <pre>内容，标题格式为第XXX页 XX，内容是<pre>里的内容\n",
        "    pattern = re.compile(\n",
        "        r'<p[^>]*?>\\s*(第(\\d+)页[^<]*)\\s*</p>\\s*'    # 匹配标题：例如“第378页 原文”\n",
        "        r'<pre[^>]*?>\\s*(.*?)\\s*</pre>',              # 匹配紧跟的pre内容\n",
        "        re.DOTALL\n",
        "    )\n",
        "    matches = pattern.findall(html_text)\n",
        "\n",
        "    pages = {}\n",
        "    for full_title, page_str, content in matches:\n",
        "        # 判断是哪部分内容\n",
        "        if '原文' in full_title:\n",
        "            part = '原文'\n",
        "        elif '中文翻译' in full_title:\n",
        "            part = '中文翻译'\n",
        "        elif '逐行詳細解析' in full_title or '逐行详细解析' in full_title:\n",
        "            part = '逐行详细解析'\n",
        "        else:\n",
        "            # 避免无关段落干扰\n",
        "            continue\n",
        "\n",
        "        page_num = int(page_str)\n",
        "        if page_num not in pages:\n",
        "            pages[page_num] = {}\n",
        "\n",
        "        clean_text = clean_html_text(content)\n",
        "        pages[page_num][part] = clean_text\n",
        "\n",
        "    # 组装成列表，方便转DataFrame\n",
        "    results = []\n",
        "    for page_num in sorted(pages.keys()):\n",
        "        page_data = pages[page_num]\n",
        "        results.append({\n",
        "            '页码': page_num,\n",
        "            '原文': page_data.get('原文', ''),\n",
        "            '中文翻译': page_data.get('中文翻译', ''),\n",
        "            '逐行详细解析': page_data.get('逐行详细解析', ''),\n",
        "        })\n",
        "    return results\n",
        "import pandas as pd\n",
        "\n",
        "def export_for_anki(page_contents, output_path):\n",
        "    \"\"\"\n",
        "    page_contents: 解析出来的结构，如 [{'page': 378, 'original': '...', 'translation': '...', 'analysis': '...'}, ...]\n",
        "    output_path: 导出路径，比如 '/content/anki_cards.tsv'\n",
        "    \"\"\"\n",
        "    anki_rows = []\n",
        "\n",
        "    for page in page_contents:\n",
        "        front = page.get('original', '').strip()\n",
        "        back_parts = []\n",
        "\n",
        "        if page.get('translation'):\n",
        "            back_parts.append(\"<b>中文翻译：</b><br>\" + page['translation'].replace('\\n', '<br>'))\n",
        "        if page.get('analysis'):\n",
        "            back_parts.append(\"<b>逐行解析：</b><br>\" + page['analysis'].replace('\\n', '<br>'))\n",
        "\n",
        "        back = \"<br><br>\".join(back_parts)\n",
        "        anki_rows.append([front, back])\n",
        "\n",
        "    # 转为 DataFrame\n",
        "    df = pd.DataFrame(anki_rows, columns=['Front', 'Back'])\n",
        "\n",
        "    # 保存为制表符分隔的 UTF-8 文件\n",
        "    df.to_csv(output_path, sep='\\t', index=False, header=False, encoding='utf-8')\n",
        "    print(f\"✅ Anki 卡片文件已导出到 {output_path}\")\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def export_with_section(page_contents, output_path):\n",
        "    \"\"\"\n",
        "    page_contents: [{'page': 378, 'original': '...', 'translation': '...', 'analysis': '...'}, ...]\n",
        "    output_path: 导出 CSV 路径\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    for page in page_contents:\n",
        "        rows.append([\n",
        "            page.get('page', ''),   # Page\n",
        "            '',                     # Section (空)\n",
        "            page.get('original', '').strip(),     # Original\n",
        "            page.get('translation', '').strip(),  # Translation\n",
        "            page.get('analysis', '').strip()      # Analysis\n",
        "        ])\n",
        "\n",
        "    df = pd.DataFrame(rows, columns=['Page', 'Section', 'Original', 'Translation', 'Analysis'])\n",
        "    df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"✅ 已导出 CSV 文件: {output_path}\")\n",
        "\n",
        "\n",
        "\n",
        "# 5. 读取你的HTML文件路径（请替换成你自己的文件路径）\n",
        "#html_file_path = '/content/drive/MyDrive/chatgpt_export.html'\n",
        "html_file_path = '/content/drive/MyDrive/chatgpt_export0.html'\n",
        "\n",
        "\n",
        "# 用二进制模式读取，忽略无法解码的字节\n",
        "with open(html_file_path, 'rb') as f:\n",
        "    html_content = f.read().decode('utf-8', errors='ignore')\n",
        "\n",
        "# 6. 调用解析函数\n",
        "page_contents = parse_html_pages(html_content)\n",
        "\n",
        "# 7. 转成DataFrame并导出Excel\n",
        "df = pd.DataFrame(page_contents, columns=['页码', '原文', '中文翻译', '逐行详细解析'])\n",
        "output_path = '/content/drive/MyDrive/parsed_pages.xlsx'\n",
        "df.to_excel(output_path, index=False)\n",
        "\n",
        "print(f\"完成！Excel文件已保存到：{output_path}\")\n",
        "\n",
        "# 使用示例（假设 page_contents 已经是你解析后的结构）\n",
        "#Anki_output_path = '/content/drive/MyDrive/aws_anki_cards.tsv'\n",
        "#export_for_anki(page_contents, Anki_output_path)\n",
        "\n",
        "# 调用\n",
        "#export_with_section(page_contents, '/content/drive/MyDrive/aws_anki_csv.csv')\n",
        "# 在 'Page' 和 'Original' 之间插入一个空列 Section\n",
        "df.insert(1, 'Section', '')\n",
        "\n",
        "# 导出 CSV\n",
        "csv_output_path = '/content/drive/MyDrive/aws_anki_csv.csv'\n",
        "df.to_csv(csv_output_path, index=False, encoding='utf-8-sig')\n",
        "print(f\"完成！CSV文件已保存到：{csv_output_path}\")\n",
        "\n",
        "print(df)\n",
        "\n",
        "# 8. （可选）展示部分内容确认\n",
        "\n",
        "for page in page_contents[:3]:\n",
        "    print(f\"第{page['页码']}页 原文片段:\\n{page['原文'][:300]}\\n\")\n",
        "    print(f\"第{page['页码']}页 中文翻译片段:\\n{page['中文翻译'][:300]}\\n\")\n",
        "    print(f\"第{page['页码']}页 逐行详细解析片段:\\n{page['逐行详细解析'][:300]}\\n\")\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6GvVAdcIF4S",
        "outputId": "219063a5-e613-4ccb-a24f-b53450e3979c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "完成！Excel文件已保存到：/content/drive/MyDrive/parsed_pages.xlsx\n",
            "完成！CSV文件已保存到：/content/drive/MyDrive/aws_anki_csv.csv\n",
            "    页码 Section 原文 中文翻译                                             逐行详细解析\n",
            "0  310                  ⓫【原文】（1つのSigned URL で1つのファイルにアクセス可能）\\n【発音】（ひとつ...\n",
            "第310页 原文片段:\n",
            "\n",
            "\n",
            "第310页 中文翻译片段:\n",
            "\n",
            "\n",
            "第310页 逐行详细解析片段:\n",
            "⓫【原文】（1つのSigned URL で1つのファイルにアクセス可能）\n",
            "【発音】（ひとつ の サインド ユーアールエル で ひとつ の ファイル に アクセス かのう）\n",
            "【翻訳】（一个签名 URL 可访问一个文件）\n",
            "【単語】\n",
            "① 1つ（ひとつ）：一个\n",
            "② Signed URL（サインド ユーアールエル）：签名 URL\n",
            "③ で：表示动作发生的场所或手段的助词\n",
            "④ ファイル（ファイル）：文件\n",
            "⑤ に：表示动作的目标的助词\n",
            "⑥ アクセス（アクセス）：访问\n",
            "⑦ 可能（かのう）：可能\n",
            "【語法】\n",
            "「～で」这里表示通过某手段（签名 URL）进行访问。\n",
            "「～にアクセス可能」表示可以访问某对象。\n",
            "❿【原文】S\n",
            "\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 运行前安装依赖（Colab可运行）\n",
        "#!pip install beautifulsoup4 lxml pandas openpyxl\n",
        "\n",
        "# 1. 挂载Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# -----------------------\n",
        "# 配置：文件路径（修改为你的路径）\n",
        "# -----------------------\n",
        "HTML_FILE_PATH = '/content/drive/MyDrive/chatgpt_export0.html'  # <- 修改\n",
        "OUTPUT_MID_EXCEL = '/content/drive/MyDrive/codeblocks_with_meta.xlsx'   # 中间表：代码块、页码、类别、其他\n",
        "OUTPUT_FINAL_EXCEL = '/content/drive/MyDrive/final_original_translation_analysis.xlsx'  # 最终三列表\n",
        "\n",
        "# -----------------------\n",
        "# 读取 HTML（以二进制读，utf-8 解码并忽略非法字节）\n",
        "# -----------------------\n",
        "with open(HTML_FILE_PATH, 'rb') as f:\n",
        "    html_bytes = f.read()\n",
        "html_text = html_bytes.decode('utf-8', errors='ignore')\n",
        "\n",
        "soup = BeautifulSoup(html_text, 'lxml')  # 使用 lxml 更稳健\n",
        "\n",
        "# -----------------------\n",
        "# 提取页面中按顺序出现的“代码块”\n",
        "# 优先采集 <pre>，若 <pre> 不够，再取 <code> 或含language-plaintext类的\n",
        "# 保持文档顺序：BS4 find_all 返回的顺序即文档顺序\n",
        "# -----------------------\n",
        "code_tags = []\n",
        "# 首先收集所有 <pre> 标签\n",
        "for tag in soup.find_all(['pre', 'code']):\n",
        "    # 过滤空的code/pre\n",
        "    txt = tag.get_text()\n",
        "    if txt and txt.strip():\n",
        "        code_tags.append((tag.name, txt))\n",
        "\n",
        "# 如果没有任何 pre/code，再尝试找带特定 class 的容器（fallback）\n",
        "if not code_tags:\n",
        "    for div in soup.find_all('div'):\n",
        "        txt = div.get_text()\n",
        "        if txt and txt.strip():\n",
        "            code_tags.append(('div', txt))\n",
        "\n",
        "# 初始 DataFrame 列：代码块, 页码, 类别, 其他\n",
        "df_mid = pd.DataFrame(columns=['代码块', '页码', '类别', '其他'])\n",
        "\n",
        "# 将提取到的代码块按顺序放入中间表（页码/类别/其他初始为空）\n",
        "for txt in [t[1] for t in code_tags]:\n",
        "    df_mid = df_mid.append({'代码块': txt, '页码': '', '类别': '', '其他': ''}, ignore_index=True)\n",
        "\n",
        "# -----------------------\n",
        "# 寻找并处理代码块内的“第XX页”页码行，按要求拆分\n",
        "# 规则：\n",
        "# - 对每个代码块，按行检测去掉空格后开头为第\\d+页 或 第\\d+頁 的行\n",
        "# - 当找到这样的行（可能多个），将该代码块拆分：\n",
        "#     - 如果 header 在块内，则把 header 以及 header 后的内容作为一个新块（header行不包含在最终代码块内容--但将 header 文本放入 \"其他\" 字段）\n",
        "#     - 对于 header 捕获的页码 xx：将该 header 行后紧接着的下一个代码块（在中间表顺序上的下一个） 的 '页码' 设置为 xx，并把 header 行内容写入该行的 '其他' 字段\n",
        "# - 若 header 出现在块中间/末尾：把 header 及其后内容分割成新的记录，原记录用前半部分替代\n",
        "#\n",
        "# 说明：为简化实现并保证稳定性，按如下实现：\n",
        "# - 找到 header 行在该代码块行内的位置\n",
        "# - 以 header 行作为“划分点”：把原块拆为多段（段不包含 header 行）\n",
        "# - header 所指页码页号将被赋给“紧随 header 的下一个代码块”（若存在）——这里我们把 header 所指页码赋给拆分结果中对应的“段”的页码字段，同样把 header 行文本放入该段的“其他”字段\n",
        "# -----------------------\n",
        "\n",
        "# 正则匹配 \"第123页\" 或 \"第123頁\"，允许中间有空白或非数字的情况也尽量捕获数字\n",
        "page_header_re = re.compile(r'^\\s*第\\s*([0-9０-９]+)\\s*[页頁]\\s*', re.UNICODE)\n",
        "\n",
        "def normalize_number_str(num_str):\n",
        "    # 将全角数字转换为半角\n",
        "    trans = str.maketrans('０１２３４５６７８９', '0123456789')\n",
        "    return num_str.translate(trans)\n",
        "\n",
        "new_rows = []\n",
        "for idx, row in df_mid.iterrows():\n",
        "    block = row['代码块']\n",
        "    lines = block.splitlines()\n",
        "    header_positions = []  # list of (line_index, matched_number_str, original_line_text)\n",
        "    for i, ln in enumerate(lines):\n",
        "        # 移除所有空白判断是否以 第xx页 开头\n",
        "        compact = re.sub(r'\\s+', '', ln)\n",
        "        m = page_header_re.match(compact)\n",
        "        if m:\n",
        "            num_str = normalize_number_str(m.group(1))\n",
        "            header_positions.append((i, num_str, ln.strip()))\n",
        "    if not header_positions:\n",
        "        # 无拆分，直接保留\n",
        "        new_rows.append({'代码块': block, '页码': row['页码'], '类别': row['类别'], '其他': row['其他']})\n",
        "    else:\n",
        "        # 有拆分点。我们把原块拆成若干片段：\n",
        "        splits = []\n",
        "        prev = 0\n",
        "        for pos, num_str, header_line in header_positions:\n",
        "            # part before header (保留)\n",
        "            if pos > prev:\n",
        "                part_before = '\\n'.join(lines[prev:pos]).strip()\n",
        "                if part_before:\n",
        "                    splits.append({'text': part_before, 'is_header_bound': False, 'page_from_header': None, 'header_line': ''})\n",
        "            # header-bound part: collect following lines until next header or EOF\n",
        "            # find start index after header line\n",
        "            next_start = pos + 1\n",
        "            # find next header position index\n",
        "            # (we will assign content after header to page num)\n",
        "            # to be decided after we determine next header index\n",
        "            # we'll just mark header here; content will be gathered in next loop\n",
        "            prev = next_start\n",
        "        # after processing headers, add tail\n",
        "        if prev < len(lines):\n",
        "            tail = '\\n'.join(lines[prev:]).strip()\n",
        "            if tail:\n",
        "                splits.append({'text': tail, 'is_header_bound': False, 'page_from_header': None, 'header_line': ''})\n",
        "\n",
        "        # The above split logic is conservative; instead implement direct linear processing:\n",
        "        # We'll iterate lines and whenever we encounter header, start a new segment for content after header, and attach header info to that segment.\n",
        "\n",
        "        segments = []\n",
        "        current_seg_lines = []\n",
        "        pending_header = None  # tuple (num_str, header_text)\n",
        "        i = 0\n",
        "        while i < len(lines):\n",
        "            compact = re.sub(r'\\s+', '', lines[i])\n",
        "            m = page_header_re.match(compact)\n",
        "            if m:\n",
        "                # when see header, if current_seg_lines not empty, finalize it as a standalone segment\n",
        "                if current_seg_lines:\n",
        "                    segments.append({'text': '\\n'.join(current_seg_lines).strip(), 'header_num': None, 'header_line': ''})\n",
        "                    current_seg_lines = []\n",
        "                # set pending header: the content after this header will be a segment marked with this header\n",
        "                num = normalize_number_str(m.group(1))\n",
        "                header_txt = lines[i].strip()\n",
        "                # start new current_seg_lines for content after header\n",
        "                current_seg_lines = []\n",
        "                # set pending header to be applied to next created segment\n",
        "                pending_header = (num, header_txt)\n",
        "                i += 1\n",
        "                # collect following lines until next header occurrence\n",
        "                while i < len(lines):\n",
        "                    compact2 = re.sub(r'\\s+', '', lines[i])\n",
        "                    m2 = page_header_re.match(compact2)\n",
        "                    if m2:\n",
        "                        break\n",
        "                    current_seg_lines.append(lines[i])\n",
        "                    i += 1\n",
        "                # finalize this header-bound segment\n",
        "                seg_text = '\\n'.join(current_seg_lines).strip()\n",
        "                segments.append({'text': seg_text, 'header_num': pending_header[0], 'header_line': pending_header[1]})\n",
        "                pending_header = None\n",
        "                current_seg_lines = []\n",
        "            else:\n",
        "                current_seg_lines.append(lines[i])\n",
        "                i += 1\n",
        "        # if leftover\n",
        "        if current_seg_lines:\n",
        "            seg_text = '\\n'.join(current_seg_lines).strip()\n",
        "            if seg_text:\n",
        "                segments.append({'text': seg_text, 'header_num': None, 'header_line': ''})\n",
        "\n",
        "        # Now segments is a list; replace original row with these segments in place\n",
        "        for seg in segments:\n",
        "            # Note: if seg has header_num, set that as 页码 and put header_line in 其他\n",
        "            if seg['header_num']:\n",
        "                new_rows.append({'代码块': seg['text'], '页码': seg['header_num'], '类别': '', '其他': seg['header_line']})\n",
        "            else:\n",
        "                new_rows.append({'代码块': seg['text'], '页码': '', '类别': '', '其他': ''})\n",
        "\n",
        "# 重建中间表\n",
        "df_mid = pd.DataFrame(new_rows, columns=['代码块', '页码', '类别', '其他'])\n",
        "\n",
        "# -------"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "2i3R65PXxlWd",
        "outputId": "20fa6c04-de2d-4b36-925e-14170ada0e0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DataFrame' object has no attribute 'append'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1574640468.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# 将提取到的代码块按顺序放入中间表（页码/类别/其他初始为空）\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtxt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcode_tags\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mdf_mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'代码块'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'页码'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'类别'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'其他'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# -----------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6301\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# 1. 读取 HTML\n",
        "#html_file_path = \"/content/chatgpt_session.html\"  # 你的HTML文件路径\n",
        "html_file_path = '/content/drive/MyDrive/chatgpt_export0.html'  # <- 修改\n",
        "OUTPUT_MID_EXCEL = '/content/drive/MyDrive/codeblocks_with_meta.xlsx'   # 中间表：代码块、页码、类别、其他\n",
        "OUTPUT_FINAL_EXCEL = '/content/drive/MyDrive/final_original_translation_analysis.xlsx'  # 最终三列表\n",
        "with open(html_file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "    html_content = f.read()\n",
        "\n",
        "# 2. 用 BeautifulSoup 解析 HTML\n",
        "soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "# 3. 提取所有代码块（<code>标签）\n",
        "code_tags = []\n",
        "for code in soup.find_all(\"code\"):\n",
        "    code_text = code.get_text(\"\\n\")  # 取出代码块内容\n",
        "    if code_text.strip():\n",
        "        code_tags.append(code_text)\n",
        "\n",
        "# 4. 初始化 DataFrame\n",
        "df_mid = pd.DataFrame(columns=[\"代码块\", \"页码\", \"类别\", \"其他\"])\n",
        "\n",
        "# 5. 用 concat 一次性构造\n",
        "df_mid = pd.concat([\n",
        "    pd.DataFrame({\"代码块\": [txt], \"页码\": [\"\"], \"类别\": [\"\"], \"其他\": [\"\"]})\n",
        "    for txt in code_tags\n",
        "], ignore_index=True)\n",
        "\n",
        "# 6. 拆分含“第xx页”标记的行\n",
        "pattern_page = re.compile(r\"第(\\d+)页\")\n",
        "new_rows = []\n",
        "\n",
        "for _, row in df_mid.iterrows():\n",
        "    text = row[\"代码块\"]\n",
        "    lines = text.splitlines()\n",
        "    split_indexes = []\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        if pattern_page.search(line.strip().replace(\" \", \"\")):\n",
        "            split_indexes.append(i)\n",
        "\n",
        "    if split_indexes:\n",
        "        start_idx = 0\n",
        "        for idx in split_indexes:\n",
        "            if idx > start_idx:\n",
        "                # 保存之前的部分\n",
        "                new_rows.append([ \"\\n\".join(lines[start_idx:idx]), row[\"页码\"], row[\"类别\"], row[\"其他\"] ])\n",
        "            # 保存页码信息\n",
        "            page_match = pattern_page.search(lines[idx].replace(\" \", \"\"))\n",
        "            if page_match:\n",
        "                page_num = page_match.group(1)\n",
        "                new_rows.append([\"\", page_num, \"\", lines[idx]])  # 页码在“页码”列，原始行放“其他”列\n",
        "            start_idx = idx + 1\n",
        "        # 保存最后一部分\n",
        "        if start_idx < len(lines):\n",
        "            new_rows.append([\"\\n\".join(lines[start_idx:]), row[\"页码\"], row[\"类别\"], row[\"其他\"]])\n",
        "    else:\n",
        "        new_rows.append([text, row[\"页码\"], row[\"类别\"], row[\"其他\"]])\n",
        "\n",
        "df_mid = pd.DataFrame(new_rows, columns=[\"代码块\", \"页码\", \"类别\", \"其他\"])\n",
        "\n",
        "# 7. 分类\n",
        "def contains_japanese(text):\n",
        "    return bool(re.search(r\"[\\u3040-\\u30ff\\u4e00-\\u9fff]\", text))\n",
        "\n",
        "def contains_chinese(text):\n",
        "    return bool(re.search(r\"[\\u4e00-\\u9fff]\", text))\n",
        "\n",
        "for i, row in df_mid.iterrows():\n",
        "    if \"原文\" in str(row[\"其他\"]):\n",
        "        df_mid.at[i, \"类别\"] = \"原文\"\n",
        "    elif \"翻译\" in str(row[\"其他\"]):\n",
        "        df_mid.at[i, \"类别\"] = \"中文翻译\"\n",
        "    elif \"逐行详细解析\" in str(row[\"其他\"]):\n",
        "        df_mid.at[i, \"类别\"] = \"逐行解析\"\n",
        "    else:\n",
        "        code_text = str(row[\"代码块\"])\n",
        "        if contains_chinese(code_text) and contains_japanese(code_text):\n",
        "            df_mid.at[i, \"类别\"] = \"逐行解析\"\n",
        "        elif not contains_japanese(code_text) and contains_chinese(code_text):\n",
        "            df_mid.at[i, \"类别\"] = \"中文翻译\"\n",
        "        else:\n",
        "            df_mid.at[i, \"类别\"] = \"原文\"\n",
        "\n",
        "# 8. 导出中间Excel\n",
        "df_mid.to_excel(OUTPUT_MID_EXCEL, index=False)\n",
        "print(\"已保存中间表 mid_result.xlsx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPP6GEx7y2EH",
        "outputId": "a22b0ad0-7308-4aec-a8e5-641f9e979118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "已保存中间表 mid_result.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 运行前请确保安装依赖（在 Colab 可运行以下命令）\n",
        "# !pip install beautifulsoup4 lxml pandas openpyxl\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# ---------------------------\n",
        "# 配置：将下面路径替换为你的文件路径\n",
        "# ---------------------------\n",
        "HTML_FILE_PATH = '/content/drive/MyDrive/chatgpt_export0.html'   # <- 修改\n",
        "OUTPUT_MID_EXCEL = '/content/drive/MyDrive/codeblocks_with_meta.xlsx'   # 中间表输出\n",
        "OUTPUT_FINAL_EXCEL = '/content/drive/MyDrive/final_original_translation_analysis.xlsx'  # 最终三列输出\n",
        "\n",
        "# ---------------------------\n",
        "# 读取 HTML（容错：二进制读 -> utf-8 decode 忽略错误）\n",
        "# ---------------------------\n",
        "with open(HTML_FILE_PATH, 'rb') as f:\n",
        "    raw = f.read()\n",
        "html_text = raw.decode('utf-8', errors='ignore')\n",
        "\n",
        "# ---------------------------\n",
        "# 用 BeautifulSoup 解析\n",
        "# ---------------------------\n",
        "soup = BeautifulSoup(html_text, 'lxml')\n",
        "\n",
        "# ---------------------------\n",
        "# 识别页面中“代码块”的策略（按文档出现顺序）\n",
        "# 优先选择：<pre>、<code>、class含language-plaintext/whitespace-pre 的 <div> 等\n",
        "# ---------------------------\n",
        "def collect_code_blocks(soup):\n",
        "    blocks = []\n",
        "    # collect tags in document order by searching for these tag types\n",
        "    # We'll iterate the DOM tree and pick nodes that match our criteria\n",
        "    for tag in soup.find_all(True):\n",
        "        name = tag.name.lower()\n",
        "        # 1) pre or code\n",
        "        if name in ('pre', 'code'):\n",
        "            text = tag.get_text()\n",
        "            if text and text.strip():\n",
        "                blocks.append(text)\n",
        "            continue\n",
        "        # 2) div/span with class indicating code/plaintext\n",
        "        classes = tag.get('class') or []\n",
        "        classes_join = ' '.join(classes).lower() if classes else ''\n",
        "        if 'language-plaintext' in classes_join or 'whitespace-pre' in classes_join or 'language-plain' in classes_join or 'language-plaintext' in classes_join:\n",
        "            text = tag.get_text()\n",
        "            if text and text.strip():\n",
        "                blocks.append(text)\n",
        "            continue\n",
        "    return blocks\n",
        "\n",
        "code_blocks = collect_code_blocks(soup)\n",
        "\n",
        "# 如果没有识别到任何代码块，作为回退，提取所有 <div> 文本但这是非常宽松的\n",
        "if not code_blocks:\n",
        "    for div in soup.find_all('div'):\n",
        "        t = div.get_text()\n",
        "        if t and t.strip():\n",
        "            code_blocks.append(t)\n",
        "\n",
        "# 构造中间 DataFrame：代码块、页码、类别、其他（初始化后三列为空）\n",
        "df_mid = pd.DataFrame({\n",
        "    '代码块': code_blocks,\n",
        "    '页码': [''] * len(code_blocks),\n",
        "    '类别': [''] * len(code_blocks),\n",
        "    '其他': [''] * len(code_blocks)\n",
        "})\n",
        "\n",
        "# ---------------------------\n",
        "# 检测并拆分含“第xx页” header 的代码块\n",
        "# 规则实现：\n",
        "#  - 在单个代码块行内，去除空白后若某行以 \"第<digits>页\" 或 \"第<digits>頁\" 开头，则视为 header\n",
        "#  - 将 header 行之后的内容作为一个新块，并为该新块设置 页码=xx，其他=header原文\n",
        "#  - header 前的部分（若有）作为独立块保留（页码/其他为空）\n",
        "#  - 如果原代码块被拆分，则其原记录被替换为拆分后的一系列记录（顺序保留）\n",
        "# 支持半角与全角数字\n",
        "# ---------------------------\n",
        "page_header_re = re.compile(r'^\\s*第\\s*([0-9０-９]+)\\s*[页頁]\\s*', flags=re.UNICODE)\n",
        "\n",
        "def to_ascii_digits(s):\n",
        "    # 将全角数字转为半角\n",
        "    trans = str.maketrans('０１２３４５６７８９', '0123456789')\n",
        "    return s.translate(trans)\n",
        "\n",
        "new_rows = []\n",
        "for idx, row in df_mid.iterrows():\n",
        "    block = str(row['代码块'])\n",
        "    lines = block.splitlines()\n",
        "    if not lines:\n",
        "        new_rows.append({'代码块': block, '页码': row['页码'], '类别': row['类别'], '其他': row['其他']})\n",
        "        continue\n",
        "\n",
        "    i = 0\n",
        "    pending_segments = []  # list of dicts {text, page(None or num), other}\n",
        "    while i < len(lines):\n",
        "        ln = lines[i]\n",
        "        compact = re.sub(r'\\s+', '', ln)  # remove whitespace\n",
        "        m = page_header_re.match(compact)\n",
        "        if m:\n",
        "            num_raw = m.group(1)\n",
        "            num = to_ascii_digits(num_raw)\n",
        "            # collect content lines after header until next header or EOF\n",
        "            j = i + 1\n",
        "            seg_lines = []\n",
        "            while j < len(lines):\n",
        "                comp2 = re.sub(r'\\s+', '', lines[j])\n",
        "                if page_header_re.match(comp2):\n",
        "                    break\n",
        "                seg_lines.append(lines[j])\n",
        "                j += 1\n",
        "            seg_text = '\\n'.join(seg_lines).strip()\n",
        "            # create segment with page and other=header original text\n",
        "            pending_segments.append({'代码块': seg_text, '页码': num, '类别': '', '其他': ln.strip()})\n",
        "            i = j  # continue from next header or EOF\n",
        "        else:\n",
        "            # lines until next header go into a normal segment (no page)\n",
        "            j = i\n",
        "            seg_lines = []\n",
        "            while j < len(lines):\n",
        "                comp2 = re.sub(r'\\s+', '', lines[j])\n",
        "                if page_header_re.match(comp2):\n",
        "                    break\n",
        "                seg_lines.append(lines[j])\n",
        "                j += 1\n",
        "            seg_text = '\\n'.join(seg_lines).strip()\n",
        "            if seg_text:\n",
        "                pending_segments.append({'代码块': seg_text, '页码': '', '类别': '', '其他': ''})\n",
        "            i = j\n",
        "    # If we didn't find any header, pending_segments will contain the same block as single entry\n",
        "    # If the original block had no header (pending_segments len == 1 and page empty and other empty), treat as original\n",
        "    # Append pending_segments to new_rows\n",
        "    if pending_segments:\n",
        "        # if the original had no header and pending_segments is single identical part, preserve original (no deletion necessary)\n",
        "        # but spec says if original is split (has header) delete original and replace — that's what we do: simply add pending_segments\n",
        "        # If original had no header at all, pending_segments length likely 1 with same content, we still add it\n",
        "        for seg in pending_segments:\n",
        "            # if segment text is empty, skip it\n",
        "            if seg['代码块'] is None:\n",
        "                continue\n",
        "            seg_text = seg['代码块'].strip()\n",
        "            if seg_text == '':\n",
        "                # If blank, still might need to create a placeholder with page/other (if page exists)\n",
        "                if seg['页码'] or seg['其他']:\n",
        "                    new_rows.append({'代码块': '', '页码': seg['页码'], '类别': '', '其他': seg['其他']})\n",
        "                continue\n",
        "            new_rows.append({'代码块': seg_text, '页码': seg['页码'], '类别': '', '其他': seg['其他']})\n",
        "    else:\n",
        "        # fallback: keep original whole block\n",
        "        new_rows.append({'代码块': block.strip(), '页码': row['页码'], '类别': '', '其他': row['其他']})\n",
        "\n",
        "# 构建新的中间表（已拆分）\n",
        "df_mid = pd.DataFrame(new_rows, columns=['代码块', '页码', '类别', '其他'])\n",
        "\n",
        "# ---------------------------\n",
        "# 根据代码块内容判断 类别（覆盖规则见说明）\n",
        "# 判定规则（按你最新描述）：\n",
        "# 1) 如果 代码块 同时包含 '【原文】' 字样 且 包含日语假名（平假名或片假名） => '逐行解析'\n",
        "# 2) 否则若 代码块 包含日语假名 OR 全为英文字母（忽略标点空格） => '原文'\n",
        "# 3) 否则 => '中文翻译'\n",
        "# 但若 '其他' 列中包含关键词，则以 '其他' 的关键词覆盖类别：\n",
        "#    若 '其他' 含 '原文' => '原文'; 含 '翻译' => '中文翻译'; 含 '逐行' => '逐行解析'\n",
        "# ---------------------------\n",
        "cn_re = re.compile(r'[\\u4e00-\\u9fff]')\n",
        "jp_re = re.compile(r'[\\u3040-\\u30ff\\u31f0-\\u31ff]')  # hiragana, katakana ranges\n",
        "# ASCII letter check: consider \"全是英文字母\" -> after removing spaces and punctuation, remaining are A-Za-z\n",
        "import re\n",
        "\n",
        "# 日语平假名和片假名范围\n",
        "jp_re = re.compile(r'[\\u3040-\\u309F\\u30A0-\\u30FF\\u31F0-\\u31FF]')\n",
        "\n",
        "# 判断是否全为 ASCII 英文字母、空格和常见标点\n",
        "ascii_letters_re = re.compile(r'^[A-Za-z\\s!\"#$%&\\'()*+,\\-./:;<=>?@[\\\\\\]^_`{|}~]+$')\n",
        "\n",
        "def is_all_ascii_letters(text):\n",
        "    # 去除换行等控制字符\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return False\n",
        "    return bool(ascii_letters_re.fullmatch(text))\n",
        "\n",
        "# 测试：\n",
        "print(is_all_ascii_letters(\"This is a test, with punctuation!\"))  # True\n",
        "print(is_all_ascii_letters(\"これはテストです\"))  # False\n",
        "\n",
        "\n",
        "def detect_kind(block_text, other_text):\n",
        "    txt = block_text or ''\n",
        "    other = other_text or ''\n",
        "    other_low = other.lower()\n",
        "    # 优先检查“其他”列覆盖关键词\n",
        "    if any(k in other_low for k in ['逐行', '逐行解析', '逐行詳細解析']):\n",
        "        return '逐行解析'\n",
        "    if '翻' in other_low or '翻译' in other_low or '翻譯' in other_low:\n",
        "        return '中文翻译'\n",
        "    if '原文' in other_low:\n",
        "        return '原文'\n",
        "\n",
        "    # 主要判定逻辑修改如下：\n",
        "    #has_marker_gen = ('【原文】' in txt)\n",
        "    has_marker_fa = ('【发音】' in txt)\n",
        "    has_jp = bool(jp_re.search(txt))\n",
        "\n",
        "    #if (has_marker_gen and has_marker_fa) or ('逐行' in txt):\n",
        "    if has_marker_fa or ('逐行' in txt):\n",
        "        return '逐行解析'\n",
        "    if has_jp or is_all_ascii_letters(txt):\n",
        "        return '原文'\n",
        "\n",
        "    return '中文翻译'\n",
        "\n",
        "# Apply detection\n",
        "kinds = []\n",
        "for idx, row in df_mid.iterrows():\n",
        "    k = detect_kind(str(row['代码块']), str(row['其他']))\n",
        "    kinds.append(k)\n",
        "df_mid['类别'] = kinds\n",
        "\n",
        "# ---------------------------\n",
        "# 合并连续的 “逐行解析” 代码块（合并所有连续块）\n",
        "# ---------------------------\n",
        "merged_rows = []\n",
        "i = 0\n",
        "n = len(df_mid)\n",
        "while i < n:\n",
        "    row = df_mid.iloc[i]\n",
        "    if row['类别'] == '逐行解析':\n",
        "        segs = [str(row['代码块']).strip()]\n",
        "        pages = [str(row['页码']).strip()] if str(row['页码']).strip() else []\n",
        "        others = [str(row['其他']).strip()] if str(row['其他']).strip() else []\n",
        "        j = i + 1\n",
        "        while j < n and df_mid.iloc[j]['类别'] == '逐行解析':\n",
        "            segs.append(str(df_mid.iloc[j]['代码块']).strip())\n",
        "            if str(df_mid.iloc[j]['页码']).strip():\n",
        "                pages.append(str(df_mid.iloc[j]['页码']).strip())\n",
        "            if str(df_mid.iloc[j]['其他']).strip():\n",
        "                others.append(str(df_mid.iloc[j]['其他']).strip())\n",
        "            j += 1\n",
        "        merged_text = '\\n'.join([s for s in segs if s])\n",
        "        merged_page = pages[0] if pages else ''\n",
        "        merged_other = '\\n'.join([o for o in others if o])\n",
        "        merged_rows.append({'代码块': merged_text, '页码': merged_page, '类别': '逐行解析', '其他': merged_other})\n",
        "        i = j\n",
        "    else:\n",
        "        merged_rows.append({'代码块': row['代码块'], '页码': row['页码'], '类别': row['类别'], '其他': row['其他']})\n",
        "        i += 1\n",
        "\n",
        "df_mid = pd.DataFrame(merged_rows, columns=['代码块', '页码', '类别', '其他'])\n",
        "\n",
        "# ---------------------------\n",
        "# 基于中间表生成最终三列表：原文 / 中文翻译 / 逐行解析\n",
        "# 规则：遍历中间表，遇到 类别 == '原文' 时创建一行，尝试把其后紧跟的 翻译/解析 填入对应列\n",
        "# 注意：如果翻译或解析不存在，则为空。之后去重合并相同原文（见下）\n",
        "# ---------------------------\n",
        "final_rows = []\n",
        "i = 0\n",
        "n = len(df_mid)\n",
        "while i < n:\n",
        "    row = df_mid.iloc[i]\n",
        "    if row['类别'] == '原文':\n",
        "        orig = str(row['代码块']).strip()\n",
        "        trans = ''\n",
        "        analysis = ''\n",
        "        # look ahead\n",
        "        if i + 1 < n and df_mid.iloc[i+1]['类别'] == '中文翻译':\n",
        "            trans = str(df_mid.iloc[i+1]['代码块']).strip()\n",
        "            # maybe next is 逐行解析\n",
        "            if i + 2 < n and df_mid.iloc[i+2]['类别'] == '逐行解析':\n",
        "                analysis = str(df_mid.iloc[i+2]['代码块']).strip()\n",
        "                i += 2\n",
        "            else:\n",
        "                i += 1\n",
        "        elif i + 1 < n and df_mid.iloc[i+1]['类别'] == '逐行解析':\n",
        "            analysis = str(df_mid.iloc[i+1]['代码块']).strip()\n",
        "            i += 1\n",
        "        final_rows.append({'原文': orig, '中文翻译': trans, '逐行解析': analysis})\n",
        "    i += 1\n",
        "\n",
        "df_final = pd.DataFrame(final_rows, columns=['原文', '中文翻译', '逐行解析'])\n",
        "\n",
        "# ---------------------------\n",
        "# 对 df_final 按 '原文' 去重，保留一条，并取最长的 翻译/解析 字段\n",
        "# ---------------------------\n",
        "def choose_longest(series):\n",
        "    vals = [str(x) for x in series if str(x).strip()]\n",
        "    return max(vals, key=len) if vals else ''\n",
        "\n",
        "if not df_final.empty:\n",
        "    df_grouped = df_final.groupby('原文', as_index=False).agg({\n",
        "        '中文翻译': choose_longest,\n",
        "        '逐行解析': choose_longest\n",
        "    })\n",
        "else:\n",
        "    df_grouped = df_final.copy()\n",
        "\n",
        "# ---------------------------\n",
        "# 导出中间表和最终表\n",
        "# ---------------------------\n",
        "df_mid.to_excel(OUTPUT_MID_EXCEL, index=False)\n",
        "df_grouped.to_excel(OUTPUT_FINAL_EXCEL, index=False)\n",
        "\n",
        "print(\"完成。\")\n",
        "print(\"中间表（代码块、页码、类别、其他）保存为:\", OUTPUT_MID_EXCEL)\n",
        "print(\"最终表（原文、中文翻译、逐行解析）保存为:\", OUTPUT_FINAL_EXCEL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOlOz8Nr4sXa",
        "outputId": "94d83cb8-d06a-4a3b-852e-befeab523ede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n",
            "完成。\n",
            "中间表（代码块、页码、类别、其他）保存为: /content/drive/MyDrive/codeblocks_with_meta.xlsx\n",
            "最终表（原文、中文翻译、逐行解析）保存为: /content/drive/MyDrive/final_original_translation_analysis.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4 lxml pandas openpyxl\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmwdTexX1kve",
        "outputId": "fa75fc50-7761-4453-f0cf-52d2030ef292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 运行前请确保安装依赖（Colab可执行）\n",
        "# !pip install beautifulsoup4 lxml pandas openpyxl\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# ---------------------------\n",
        "# 配置：将下面路径替换为你的文件路径\n",
        "# ---------------------------\n",
        "#HTML_FILE_PATH = '/content/drive/MyDrive/chatgpt_export0.html'      # <- 修改\n",
        "#HTML_FILE_PATH = '/content/drive/MyDrive/chatgpt_export.html'      # <- 修改\n",
        "HTML_FILE_PATH = '/content/drive/MyDrive/chatgpt_export180_350.html'      # <- 修改\n",
        "#HTML_FILE_PATH = '/content/drive/MyDrive/chatgpt_export431_770.html'      # <- 修改\n",
        "OUTPUT_MID_MERGED_EXCEL = '/content/drive/MyDrive/codeblocks_with_meta_merged.xlsx'  # 中间表输出\n",
        "OUTPUT_MID_EXCEL = '/content/drive/MyDrive/codeblocks_with_meta.xlsx'  # 中间表输出\n",
        "OUTPUT_FINAL_EXCEL = '/content/drive/MyDrive/final_original_translation_analysis.xlsx'  # 最终三列输出\n",
        "OUTPUT_FINAL_FORMAT_EXCEL = '/content/drive/MyDrive/final_original_translation_analysis_format.xlsx'  # 最终三列输出\n",
        "OUTPUT_FINAL_FORMAT_CSV = '/content/drive/MyDrive/final_original_translation_analysis_format.csv' # 最终三列输出\n",
        "\n",
        "# ---------------------------\n",
        "# 读取 HTML（容错：二进制读 -> utf-8 decode 忽略错误）\n",
        "# ---------------------------\n",
        "with open(HTML_FILE_PATH, 'rb') as f:\n",
        "    raw = f.read()\n",
        "html_text = raw.decode('utf-8', errors='ignore')\n",
        "\n",
        "# ---------------------------\n",
        "# 用 BeautifulSoup 解析\n",
        "# ---------------------------\n",
        "soup = BeautifulSoup(html_text, 'lxml')\n",
        "\n",
        "# ---------------------------\n",
        "# 识别页面中“代码块”的策略（按文档出现顺序）\n",
        "# 优先选择：<pre>、<code>、class含language-plaintext/whitespace-pre 的 <div> 等\n",
        "# ---------------------------\n",
        "def clean_pre_text(text):\n",
        "    \"\"\"\n",
        "    清理 <pre> 标签内的多余文字，例如开头的“plaintext”、“复制代码”等。\n",
        "    \"\"\"\n",
        "    remove_phrases = ['plaintext', '复制代码', 'コピーコード']\n",
        "    for phrase in remove_phrases:\n",
        "        text = text.replace(phrase, '')\n",
        "    # 去除开头结尾多余空白符\n",
        "    return text.lstrip('\\n\\r ').rstrip()\n",
        "\n",
        "def collect_code_blocks(soup):\n",
        "    blocks = []\n",
        "    # collect tags in document order by searching for these tag types\n",
        "    for tag in soup.find_all(True):\n",
        "        name = tag.name.lower()\n",
        "        if name in ('pre', 'code'):\n",
        "            text = tag.get_text()\n",
        "            if text and text.strip():\n",
        "                blocks.append(text)\n",
        "            continue\n",
        "        classes = tag.get('class') or []\n",
        "        classes_join = ' '.join(classes).lower() if classes else ''\n",
        "        if ('language-plaintext' in classes_join or 'whitespace-pre' in classes_join\n",
        "            or 'language-plain' in classes_join):\n",
        "            text = tag.get_text()\n",
        "            if text and text.strip():\n",
        "                blocks.append(text)\n",
        "            continue\n",
        "    return blocks\n",
        "\n",
        "code_blocks = collect_code_blocks(soup)\n",
        "\n",
        "# 如果没有识别到任何代码块，作为回退，提取所有 <div> 文本\n",
        "if not code_blocks:\n",
        "    for div in soup.find_all('div'):\n",
        "        t = div.get_text()\n",
        "        if t and t.strip():\n",
        "            code_blocks.append(t)\n",
        "\n",
        "# 构造中间 DataFrame：代码块、页码、类别、其他（初始化后三列为空）\n",
        "df_mid = pd.DataFrame({\n",
        "    '代码块': code_blocks,\n",
        "    '页码': [''] * len(code_blocks),\n",
        "    '类别': [''] * len(code_blocks),\n",
        "    '其他': [''] * len(code_blocks)\n",
        "})\n",
        "\n",
        "# ---------------------------\n",
        "# 检测并拆分含“第xx页” header 的代码块\n",
        "# 规则：\n",
        "# - 行内去空白，若行以 \"第<digits>页\" 或 \"第<digits>頁\" 开头视为 header\n",
        "# - header 之后内容作为新块，页码=xx，其他=header原文\n",
        "# - header 前的部分（若有）单独作为块，页码/其他为空\n",
        "# - 原代码块被拆分，则替换成拆分后多条记录\n",
        "# ---------------------------\n",
        "page_header_re = re.compile(r'^\\s*第\\s*([0-9０-９]+)\\s*[页頁]\\s*', flags=re.UNICODE)\n",
        "\n",
        "def to_ascii_digits(s):\n",
        "    trans = str.maketrans('０１２３４５６７８９', '0123456789')\n",
        "    return s.translate(trans)\n",
        "\n",
        "new_rows = []\n",
        "global_seq = 1  # 总顺序\n",
        "\n",
        "for idx, row in df_mid.iterrows():\n",
        "    block = clean_pre_text(str(row['代码块']))\n",
        "    lines = block.splitlines()\n",
        "    if not lines:\n",
        "        new_rows.append({'代码块': block, '页码': row['页码'], '类别': row['类别'], '其他': row['其他'],\n",
        "                         '顺序': global_seq, '子顺序': 1})\n",
        "        global_seq += 1\n",
        "        continue\n",
        "\n",
        "    i = 0\n",
        "    pending_segments = []\n",
        "    sub_seq = 1  # 子顺序\n",
        "\n",
        "    while i < len(lines):\n",
        "        ln = lines[i]\n",
        "        compact = re.sub(r'\\s+', '', ln)\n",
        "        m = page_header_re.match(compact)\n",
        "        if m:\n",
        "            num_raw = m.group(1)\n",
        "            num = to_ascii_digits(num_raw)\n",
        "            j = i + 1\n",
        "            seg_lines = []\n",
        "            while j < len(lines):\n",
        "                comp2 = re.sub(r'\\s+', '', lines[j])\n",
        "                if page_header_re.match(comp2):\n",
        "                    break\n",
        "                seg_lines.append(lines[j])\n",
        "                j += 1\n",
        "            seg_text = '\\n'.join(seg_lines).strip()\n",
        "            if seg_text:\n",
        "                pending_segments.append({'代码块': clean_pre_text(seg_text), '页码': num, '类别': '', '其他': ln.strip(),\n",
        "                                         '顺序': global_seq, '子顺序': sub_seq})\n",
        "                sub_seq += 1\n",
        "            i = j\n",
        "        else:\n",
        "            j = i\n",
        "            seg_lines = []\n",
        "            while j < len(lines):\n",
        "                comp2 = re.sub(r'\\s+', '', lines[j])\n",
        "                if page_header_re.match(comp2):\n",
        "                    break\n",
        "                seg_lines.append(lines[j])\n",
        "                j += 1\n",
        "            seg_text = '\\n'.join(seg_lines).strip()\n",
        "            if seg_text:\n",
        "                pending_segments.append({'代码块': clean_pre_text(seg_text), '页码': '', '类别': '', '其他': '',\n",
        "                                         '顺序': global_seq, '子顺序': sub_seq})\n",
        "                sub_seq += 1\n",
        "            i = j\n",
        "\n",
        "    if pending_segments:\n",
        "        for seg in pending_segments:\n",
        "            new_rows.append(seg)\n",
        "        global_seq += 1\n",
        "    else:\n",
        "        new_rows.append({'代码块': clean_pre_text(block.strip()), '页码': row['页码'], '类别': row['类别'], '其他': row['其他'],\n",
        "                         '顺序': global_seq, '子顺序': 1})\n",
        "        global_seq += 1\n",
        "\n",
        "df_mid = pd.DataFrame(new_rows, columns=['代码块', '页码', '类别', '其他', '顺序', '子顺序'])\n",
        "\n",
        "# ---------------------------\n",
        "# 类别判定\n",
        "# ---------------------------\n",
        "jp_re = re.compile(r'[\\u3040-\\u309F\\u30A0-\\u30FF\\u31F0-\\u31FF]')  # 日语平假名片假名\n",
        "ascii_letters_re = re.compile(r'^[A-Za-z\\s!\"#$%&\\'()*+,\\-./:;<=>?@[\\\\\\]^_`{|}~]+$')\n",
        "\n",
        "def is_all_ascii_letters(text):\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return False\n",
        "    return bool(ascii_letters_re.fullmatch(text))\n",
        "\n",
        "def detect_kind(block_text, other_text):\n",
        "    txt = block_text or ''\n",
        "    other = other_text or ''\n",
        "    other_low = other.lower()\n",
        "    if any(k in other_low for k in ['逐行', '逐行解析', '逐行詳細解析']):\n",
        "        return '逐行解析'\n",
        "    if '翻' in other_low or '翻译' in other_low or '翻譯' in other_low:\n",
        "        return '中文翻译'\n",
        "    if '原文' in other_low:\n",
        "        return '原文'\n",
        "    has_marker_fa = ('发音' in txt) or ('発音' in txt)\n",
        "    has_marker_Chinese = ('中文翻译' in txt)\n",
        "\n",
        "    has_jp = bool(jp_re.search(txt))\n",
        "    if has_marker_Chinese:\n",
        "        return '中文翻译'\n",
        "\n",
        "    if has_marker_fa or ('逐行' in txt):\n",
        "        return '逐行解析'\n",
        "\n",
        "    if has_jp or is_all_ascii_letters(txt) or (' 原文' in txt):\n",
        "        return '原文'\n",
        "\n",
        "\n",
        "    return '中文翻译'\n",
        "\n",
        "kinds = []\n",
        "for idx, row in df_mid.iterrows():\n",
        "    k = detect_kind(str(row['代码块']), str(row['其他']))\n",
        "    kinds.append(k)\n",
        "df_mid['类别'] = kinds\n",
        "\n",
        "# ---------------------------\n",
        "# 合并连续“逐行解析”代码块，顺序取最小，子顺序取最小\n",
        "# ---------------------------\n",
        "merged_rows = []\n",
        "i = 0\n",
        "n = len(df_mid)\n",
        "while i < n:\n",
        "    row = df_mid.iloc[i]\n",
        "    if row['类别'] == '逐行解析':\n",
        "        segs = [str(row['代码块']).strip()]\n",
        "        pages = [str(row['页码']).strip()] if str(row['页码']).strip() else []\n",
        "        others = [str(row['其他']).strip()] if str(row['其他']).strip() else []\n",
        "        seqs = [row['顺序']]\n",
        "        subs = [row['子顺序']]\n",
        "        j = i + 1\n",
        "        while j < n and df_mid.iloc[j]['类别'] == '逐行解析':\n",
        "            segs.append(str(df_mid.iloc[j]['代码块']).strip())\n",
        "            if str(df_mid.iloc[j]['页码']).strip():\n",
        "                pages.append(str(df_mid.iloc[j]['页码']).strip())\n",
        "            if str(df_mid.iloc[j]['其他']).strip():\n",
        "                others.append(str(df_mid.iloc[j]['其他']).strip())\n",
        "            seqs.append(df_mid.iloc[j]['顺序'])\n",
        "            subs.append(df_mid.iloc[j]['子顺序'])\n",
        "            j += 1\n",
        "        merged_text = '\\n'.join([s for s in segs if s])\n",
        "        merged_page = pages[0] if pages else ''\n",
        "        merged_other = '\\n'.join([o for o in others if o])\n",
        "        merged_seq = min(seqs)\n",
        "        merged_sub = min(subs)\n",
        "        merged_rows.append({'代码块': merged_text, '页码': merged_page, '类别': '逐行解析',\n",
        "                            '其他': merged_other, '顺序': merged_seq, '子顺序': merged_sub})\n",
        "        i = j\n",
        "    else:\n",
        "        merged_rows.append({'代码块': row['代码块'], '页码': row['页码'], '类别': row['类别'],\n",
        "                            '其他': row['其他'], '顺序': row['顺序'], '子顺序': row['子顺序']})\n",
        "        i += 1\n",
        "df_mid = pd.DataFrame(merged_rows, columns=['代码块', '页码', '类别', '其他', '顺序', '子顺序'])\n",
        "\n",
        "# ...完成所有拆分、类别判定和合并代码块后\n",
        "\n",
        "# 删除除顺序和子顺序外内容重复的记录，只保留最后出现的\n",
        "cols_to_check = ['代码块', '页码', '类别', '其他']\n",
        "df_mid = df_mid.drop_duplicates(subset=cols_to_check, keep='last').reset_index(drop=True)\n",
        "\n",
        "df_mid.to_excel(OUTPUT_MID_MERGED_EXCEL, index=False)\n",
        "\n",
        "# 然后继续后续处理，比如生成最终表...\n",
        "\n",
        "# ---------------------------\n",
        "# 基于中间表生成最终三列表：原文 / 中文翻译 / 逐行解析\n",
        "# 规则：\n",
        "# 遍历中间表，遇到 类别 == '原文' 时创建一行，\n",
        "# 尝试把其后紧跟的 翻译/解析 填入对应列\n",
        "# 注意：如果翻译或解析不存在，则为空。\n",
        "# 之后对相同原文去重，保留顺序最早（顺序最小）的记录，\n",
        "# 翻译、逐行解析保留最长的内容\n",
        "# ---------------------------\n",
        "\n",
        "final_rows = []\n",
        "i = 0\n",
        "n = len(df_mid)\n",
        "\n",
        "while i < n:\n",
        "    row = df_mid.iloc[i]\n",
        "    if row['类别'] == '原文':\n",
        "        orig = str(row['代码块']).strip()\n",
        "        trans = ''\n",
        "        analysis = ''\n",
        "        orig_seq = row['顺序']\n",
        "        orig_sub = row['子顺序']\n",
        "\n",
        "        # 向后寻找“中文翻译”或“逐行解析”\n",
        "        if i + 1 < n and df_mid.iloc[i + 1]['类别'] == '中文翻译':\n",
        "            trans = str(df_mid.iloc[i + 1]['代码块']).strip()\n",
        "            if i + 2 < n and df_mid.iloc[i + 2]['类别'] == '逐行解析':\n",
        "                analysis = str(df_mid.iloc[i + 2]['代码块']).strip()\n",
        "                i += 2\n",
        "            else:\n",
        "                i += 1\n",
        "        elif i + 1 < n and df_mid.iloc[i + 1]['类别'] == '逐行解析':\n",
        "            analysis = str(df_mid.iloc[i + 1]['代码块']).strip()\n",
        "            i += 1\n",
        "\n",
        "        final_rows.append({\n",
        "            '原文': orig,\n",
        "            '中文翻译': trans,\n",
        "            '逐行解析': analysis,\n",
        "            '顺序': orig_seq,\n",
        "            '子顺序': orig_sub\n",
        "        })\n",
        "    i += 1\n",
        "\n",
        "df_final = pd.DataFrame(final_rows, columns=['原文', '中文翻译', '逐行解析', '顺序', '子顺序'])\n",
        "\n",
        "# ---------------------------\n",
        "# 按“原文”去重，保留顺序最早的记录，\n",
        "# 翻译和逐行解析取最长文本\n",
        "# ---------------------------\n",
        "def choose_longest(series):\n",
        "    vals = [str(x) for x in series if str(x).strip()]\n",
        "    return max(vals, key=len) if vals else ''\n",
        "\n",
        "if not df_final.empty:\n",
        "    # 先排序，保证顺序最小优先\n",
        "    df_final = df_final.sort_values(by=['顺序', '子顺序'])\n",
        "    # 按“原文”聚合，保留第一条记录的顺序和子顺序，合并最长文本\n",
        "    df_grouped = df_final.groupby('原文', as_index=False).agg({\n",
        "        '顺序': 'first',\n",
        "        '子顺序': 'first',\n",
        "        '中文翻译': choose_longest,\n",
        "        '逐行解析': choose_longest\n",
        "    })\n",
        "    # 结果再按顺序子顺序排序\n",
        "    df_grouped = df_grouped.sort_values(by=['顺序', '子顺序']).reset_index(drop=True)\n",
        "else:\n",
        "    df_grouped = df_final.copy()\n",
        "\n",
        "# ---------------------------\n",
        "# 导出中间表和最终表\n",
        "# ---------------------------\n",
        "df_mid.to_excel(OUTPUT_MID_EXCEL, index=False)\n",
        "df_grouped[['原文', '中文翻译', '逐行解析']].to_excel(OUTPUT_FINAL_EXCEL, index=False)\n",
        "\n",
        "\n",
        "# 删除“中文翻译”和“逐行解析”两列同时为空的记录\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# 假设 df_final 已经生成，含列 ['原文','中文翻译','逐行解析', ...]\n",
        "# 如果你的 DataFrame 叫 df_grouped 或其它，把名字替换过来即可\n",
        "\n",
        "def normalize_cell_to_NA(x):\n",
        "    \"\"\"\n",
        "    把各种“空”形式规范为 pd.NA：\n",
        "    - None/NaN/''/只含空白/全角空格/'nan'/'None'/'null'/'-' 等 -> pd.NA\n",
        "    - 其它字符串会去除首尾空白并返回\n",
        "    \"\"\"\n",
        "    if pd.isna(x):\n",
        "        return pd.NA\n",
        "    s = str(x)\n",
        "    # 去除常规空白与全角空格（两端）\n",
        "    s = re.sub(r'^[\\s\\u3000]+|[\\s\\u3000]+$', '', s)\n",
        "    # 如果变成空字符串或常见占位字符串，视为缺失\n",
        "    if s == '' or s.lower() in ('nan', 'none', 'null', '-'):\n",
        "        return pd.NA\n",
        "    return s\n",
        "\n",
        "def drop_rows_both_translation_empty(df, col1='中文翻译', col2='逐行解析', inplace=False):\n",
        "    \"\"\"\n",
        "    规范化 col1,col2 并删除同时为空的行\n",
        "    返回 (df_new, removed_count)\n",
        "    \"\"\"\n",
        "    if not inplace:\n",
        "        df = df.copy()\n",
        "\n",
        "    # 统计删除前数量\n",
        "    before = len(df)\n",
        "\n",
        "    # 规范化两列（如果没有列会抛出，确保列名正确）\n",
        "    df[[col1, col2]] = df[[col1, col2]].applymap(normalize_cell_to_NA)\n",
        "\n",
        "    # debug: 看看到底哪些行被识别为空（打印样例）\n",
        "    mask_both_empty = df[col1].isna() | df[col2].isna()\n",
        "    sample = df[mask_both_empty].head(10)\n",
        "    if not sample.empty:\n",
        "        print(f\"示例：这 {len(sample)} 行会被删除（显示前 10 行）：\")\n",
        "        print(sample[['原文', col1, col2]].to_string(index=False))\n",
        "    else:\n",
        "        print(\"没有检测到同时为空的行（前 10 个样例为空）。\")\n",
        "\n",
        "    # 执行删除（两列同时为空）\n",
        "    df = df[~mask_both_empty].reset_index(drop=True)\n",
        "    after = len(df)\n",
        "    removed = before - after\n",
        "    print(f\"已删除 {removed} 行；剩余 {after} 行（原始 {before} 行）。\")\n",
        "    return df, removed\n",
        "\n",
        "# 用法示例（假设 df_final 是目标 DataFrame）：\n",
        "df_final, removed_count = drop_rows_both_translation_empty(df_grouped[['原文', '中文翻译', '逐行解析']])\n",
        "# 如果你想就地修改：\n",
        "#df_final, _ = drop_rows_both_translation_empty(df_final)\n",
        "\n",
        "# 如果还需要把结果写回 Excel：\n",
        "# df_final.to_excel('final_filtered.xlsx', index=False)\n",
        "#df_final = df_grouped[['原文', '中文翻译', '逐行解析']].dropna(subset=['中文翻译', '逐行解析'], how='all')\n",
        "df_final.to_excel(OUTPUT_FINAL_FORMAT_EXCEL, index=False)\n",
        "\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def add_page_and_section(df, col_original='原文'):\n",
        "    \"\"\"\n",
        "    在 DataFrame 第一列前添加两列：'页码'（提取自'原文'列）和'section'（空字符串）\n",
        "    \"\"\"\n",
        "    # 提取页码\n",
        "    def extract_page(s):\n",
        "        if pd.isna(s):\n",
        "            return ''\n",
        "        m = re.search(r'第\\s*(\\d+)\\s*页', str(s))\n",
        "        if m:\n",
        "            return m.group(1)\n",
        "        return ''\n",
        "\n",
        "    df = df.copy()\n",
        "    page_vals = df[col_original].apply(extract_page)\n",
        "    section_vals = [''] * len(df)\n",
        "\n",
        "    # 在第一列前插入两列\n",
        "    df.insert(0, 'section', section_vals)\n",
        "    df.insert(0, '页码', page_vals)\n",
        "\n",
        "    return df\n",
        "\n",
        "# 用法示例：\n",
        "# 假设 df_final 是你的 DataFrame\n",
        "df_final = add_page_and_section(df_final)\n",
        "#df_final.to_csv(OUTPUT_FINAL_FORMAT_CSV, index=False, encoding='utf-8-sig')\n",
        "\n",
        "#def keep_longest_by_page(df, col_page='页码', col_target='逐行解析'):\n",
        "def keep_longest_by_page(df, col_page='原文', col_target='逐行解析'):\n",
        "    \"\"\"\n",
        "    按页码分组，只保留逐行解析列里内容最长的记录\n",
        "    如果逐行解析为空，保留组里的第一条\n",
        "    \"\"\"\n",
        "    def pick_longest(group):\n",
        "        # 找到非空的最长逐行解析\n",
        "        non_empty = group[group[col_target].astype(str).str.strip() != '']\n",
        "        if len(non_empty) > 0:\n",
        "            # 选最长的那条\n",
        "            idx_max = non_empty[col_target].astype(str).str.len().idxmax()\n",
        "            return group.loc[[idx_max]]\n",
        "        else:\n",
        "            # 全部为空，保留第一条\n",
        "            return group.iloc[[0]]\n",
        "\n",
        "    return pd.concat([pick_longest(g) for _, g in df.groupby(col_page)], ignore_index=True)\n",
        "\n",
        "# 用法示例：\n",
        "df_final = keep_longest_by_page(df_final, col_page='页码', col_target='逐行解析')\n",
        "df_final.to_csv(OUTPUT_FINAL_FORMAT_CSV, index=False, encoding='utf-8-sig')\n",
        "df_final.to_excel(OUTPUT_FINAL_EXCEL, index=False)\n",
        "\n",
        "print(\"完成。\")\n",
        "print(\"中间表（代码块、页码、类别、其他、顺序、子顺序）保存为:\", OUTPUT_MID_EXCEL)\n",
        "print(\"最终表（原文、中文翻译、逐行解析）保存为:\", OUTPUT_FINAL_EXCEL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHPQvHIaC92L",
        "outputId": "95378f34-e23c-40a9-f1cb-f177a3df4a15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1833413538.py:365: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df[[col1, col2]] = df[[col1, col2]].applymap(normalize_cell_to_NA)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "示例：这 10 行会被删除（显示前 10 行）：\n",
            "                                                                                                                                                                                                                                                                                                                                 原文 中文翻译 逐行解析\n",
            "                                                                                                                                         OK\\n🟩 第27页内容：多要素認証 (Multi Factor Authentication - MFA)• MFA = パスワード(you know) + セキュリティデバイス(you have)• MFA の主なメリット: もしパスワードが盗まれたりハックされても、アカウントが保 護されます Alice + Password => Successful login <NA> <NA>\n",
            "                                                                                                                 🟩 第28页内容：\\nAWS で使えるMFA デバイス バーチャルMFA デバイス Google Authenticator (スマホのみ) Authy (様々なデバイス) Universal 2nd Factor (U2F) Security Key YubiKey by Yubico (3rd party) 一つのデバイスで複数トークンをサポート 一つのセキュリティキーで、複数の ルート・IAM ユーザをサポート <NA> <NA>\n",
            "                                                                                                                                                🟩 第29页内容：\\nAWS で使えるMFA デバイス(cont.) Hardware Key Fob MFA Device Provided by Gemalto (3rd party) Hardware Key Fob MFA Device for AWS GovCloud (US) Provided by SurePassID (3rd party) <NA> <NA>\n",
            "     🟩 第30页内容：\\nAWS へのアクセス方法\\n• AWS へアクセスする3 つの選択肢:\\n• AWS マネジメントコンソール(パスワード+ MFA で保護)\\n• AWS Command Line Interface (CLI): アクセスキーで保護\\n• AWS Software Developer Kit (SDK) - コード向け: アクセスキーで保護\\n• アクセスキーは、AWS マネジメントコンソールで生成\\n• 利用者が、自身のアクセスキーを管理します\\n• パスワード同様、アクセスキーも秘密にしてください 共有しないこと!\\n• アクセスキーID ~= ユーザ名\\n• シークレットアクセスキー~= パスワード <NA> <NA>\n",
            "                                                                                                                                                                                         🟩 第31页内容：\\n例: アクセスキー\\n• アクセスキーID: AKIASK4E37PV4983d6C\\n• シークレットアクセスキー: AZPN3zojWozWCndIjhB0Unh8239a1bzbzO5fqqkZq\\n• 忘れないで! : アクセスキーを共有しない! <NA> <NA>\n",
            "                                                                          🟩 第32页内容：  \\nサービス向けIAM ロール  \\n• AWS サービスのなかには、 あなたの代わりにアクショ ンを実施する必要があるも のがあります  \\n• そのために、IAM ロールを 使ってAWS サービスに権限 を割り当てます  \\n• 一般的なロール:  \\n• EC2 インスタンスロール  \\n• Lambda ファンクションロール  \\n• CloudFormation 向けのロール EC2 インスタンス (仮想サーバ) IAM ロール 各種AWS サービスに アクセス <NA> <NA>\n",
            "                                                                                                                                     🟩 第33页内容：\\nIAM セキュリティツール\\n• IAM Credentials Report (アカウントレベル)\\n• すべてのアカウントのユーザや認証情報の状態をレポート\\n• IAM Access Advisor (ユーザレベル)\\n• ユーザに許可されているサービスと、それぞれのサービスに最後にア クセスしたのがいつかが見られます\\n• 権限を見直すのに使えます <NA> <NA>\n",
            "🟩 第34页内容：\\nIAM ガイドライン & ベストプラクティス\\n• AWS アカウントのセットアップ以外では、ルートアカウントを使わ ないようにしましょう\\n• 1 物理ユーザ= 1 AWS ユーザ\\n• ユーザをグループに割り当て、権限をグループに割り当てましょう\\n• 強いパスワードポリシーを設定しましょう\\n• 多要素認証(MFA)を使い、できれば必須にしましょう\\n• AWS サービスに権限を割り当てるためにロールを使います\\n• CLI /SDK のアクセスではアクセスキーを使います\\n• IAM Credentials Report で権限を管理しましょう\\n• 決して、IAM ユーザやアクセスキーを共有しないでください <NA> <NA>\n",
            "                                                      🟩 第35页内容：\\nIAM Section – サマリー\\n• ユーザ: 物理ユーザに対応、パスワードを持つ\\n• グループ: ユーザを含む（グループの入れ子は不可）\\n• ポリシー: ユーザ、グループの権限を書くJSON ドキュメント\\n• ロール: EC2 インスタンスやAWS サービスなどで利用\\n• セキュリティ: 多要素認証(MFA) + パスワードポリシー\\n• アクセスキー: CLI, SDK でアクセスする時に利用\\n• 監査: IAM Credential Reports & IAM Access Advisor <NA> <NA>\n",
            "             🟩 第37页内容：\\nAmazon EC2\\n• EC2 は、AWS のとてもよく使われるサービスのひとつ\\n• EC2 = Elastic Compute Cloud = 計算機インフラの時間貸し (Infrastructure as a Service)\\n• さまざまな機能から構成:\\n• 仮想マシンの時間貸し(EC2)\\n• 仮想ディスクへのデータ保存(EBS)\\n• アクセス負荷を複数のマシンに分散(ELB: Elastic Load Balancer)\\n• 負荷に応じてマシン数を自動で増減(ASG: Auto-scaling Group)\\n• EC2 を理解することは、”クラウド” を理解する基礎 <NA> <NA>\n",
            "已删除 252 行；剩余 277 行（原始 529 行）。\n",
            "完成。\n",
            "中间表（代码块、页码、类别、其他、顺序、子顺序）保存为: /content/drive/MyDrive/codeblocks_with_meta.xlsx\n",
            "最终表（原文、中文翻译、逐行解析）保存为: /content/drive/MyDrive/final_original_translation_analysis.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# ===== 文本清理函数 =====\n",
        "def remove_html_tags(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    soup = BeautifulSoup(str(text), \"html.parser\")\n",
        "    return soup.get_text(separator=\" \")\n",
        "\n",
        "def clean_text(text):\n",
        "    text = remove_html_tags(text)\n",
        "    text = re.sub(r\"^\\s*(🟩\\s*)?第\\d+页内容[:：]\", \"\", text)\n",
        "    text = text.replace(\"【原文】\", \"\")\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# ===== 读取数据 =====\n",
        "csv_file = \"/content/drive/MyDrive/aws_pages.csv\"\n",
        "xlsx_file = \"/content/drive/MyDrive/anki_cleaned.xlsx\"\n",
        "\n",
        "df_csv = pd.read_csv(csv_file, usecols=[\"页码\", \"原文\"])\n",
        "df_xlsx = pd.read_excel(xlsx_file, usecols=[\"原文\", \"中文翻译\", \"逐行解析\"])\n",
        "\n",
        "# 转换页码为数字\n",
        "df_csv[\"页码\"] = pd.to_numeric(df_csv[\"页码\"], errors=\"coerce\")\n",
        "\n",
        "# 清理\n",
        "df_csv[\"原文_clean\"] = df_csv[\"原文\"].apply(clean_text)\n",
        "df_xlsx[\"原文_clean\"] = df_xlsx[\"原文\"].apply(clean_text)\n",
        "\n",
        "# 初始化新 DataFrame\n",
        "df_new = pd.DataFrame({\n",
        "    \"页码\": [None] * len(df_xlsx),\n",
        "    \"section\": [\"\" for _ in range(len(df_xlsx))],\n",
        "    \"原文\": df_xlsx[\"原文\"],\n",
        "    \"中文翻译\": df_xlsx[\"中文翻译\"],\n",
        "    \"逐行解析\": df_xlsx[\"逐行解析\"]\n",
        "})\n",
        "\n",
        "# 匹配\n",
        "for idx, row in df_xlsx.iterrows():\n",
        "    target_text = row[\"原文_clean\"]\n",
        "    if not target_text:\n",
        "        continue\n",
        "\n",
        "    best_score = 0\n",
        "    best_page = None\n",
        "\n",
        "    for _, csv_row in df_csv.iterrows():\n",
        "        score = SequenceMatcher(None, target_text, csv_row[\"原文_clean\"]).ratio()\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_page = csv_row[\"页码\"]\n",
        "\n",
        "    # 只在相似度达到阈值时才填充\n",
        "    if best_score > 0.75:\n",
        "        df_new.at[idx, \"页码\"] = best_page\n",
        "\n",
        "# 保存\n",
        "output_file = \"/content/drive/MyDrive/similar_check.xlsx\"\n",
        "df_new.to_excel(output_file, index=False)\n",
        "print(f\"✅ similar_check.xlsx 已生成: {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GjlkqbU_Osw",
        "outputId": "d7dde7c6-2fe8-4bed-9a7a-417692fbeb69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ similar_check.xlsx 已生成: /content/drive/MyDrive/similar_check.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#anki_csv_file = \"/content/drive/MyDrive/similar_check_for_anki.csv\"\n",
        "anki_csv_file = \"/content/drive/MyDrive/AWS_Anki.txt\"\n",
        "\n",
        "# ========== 导出成 Anki 可用 CSV ==========\n",
        "#anki_csv_file = \"/content/drive/MyDrive/anki_import.txt\"\n",
        "\n",
        "# 按 Anki 需求：用制表符分隔，不要表头，不要索引\n",
        "df_new.to_csv(\n",
        "    anki_csv_file,\n",
        "    sep=\"\\t\",\n",
        "    index=False,\n",
        "    header=False,\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "print(f\"✅ 已导出可直接导入 Anki 的 txt 文件：{anki_csv_file}\")\n",
        "#df_new.to_csv(anki_csv_file, index=False, encoding=\"utf-8-sig\")\n",
        "#print(f\"已生成 Anki 导入 CSV：{anki_csv_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPVhO0iDF5Cm",
        "outputId": "804f1c59-eb4e-41ca-e818-d4d33f26b3dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 已导出可直接导入 Anki 的 txt 文件：/content/drive/MyDrive/AWS_Anki.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "#anki_txt_file = \"/content/drive/MyDrive/AWS_Anki.txt\"\n",
        "anki_txt_file = \"/content/drive/MyDrive/AWS_Anki_total.txt\"\n",
        "output_excel = \"/content/drive/MyDrive/anki_cleaned.xlsx\"\n",
        "output_csv = \"/content/drive/MyDrive/anki_cleaned.csv\"\n",
        "output_original_csv = \"/content/drive/MyDrive/anki_original_cleaned.csv\"\n",
        "output_translation_csv = \"/content/drive/MyDrive/anki_translation_cleaned.csv\"\n",
        "output_original_translation_csv = \"/content/drive/MyDrive/anki_original_translation_cleaned.csv\"\n",
        "\n",
        "def remove_html_tags(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    soup = BeautifulSoup(str(text), \"html.parser\")\n",
        "    return soup.get_text(separator=\" \")  # 用空格替换换行等\n",
        "\n",
        "def remove_invisible_chars(text):\n",
        "    invisible_chars = [\n",
        "        '\\u200b',  # 零宽空格\n",
        "        '\\ufeff',  # BOM\n",
        "        '\\u3000',  # 全角空格\n",
        "    ]\n",
        "    for ch in invisible_chars:\n",
        "        text = text.replace(ch, '')\n",
        "    return text\n",
        "\n",
        "import unicodedata\n",
        "\n",
        "def normalize_text(text):\n",
        "    # NFKC标准化，统一全角半角、特殊字符等\n",
        "    text = unicodedata.normalize('NFKC', text)\n",
        "    # 去除多余空格（词间只留一个空格）\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # 去除空格前后的特殊字符空格\n",
        "    text = re.sub(r'\\s+([、。！？])', r'\\1', text)\n",
        "    return text\n",
        "\n",
        "def clean_original(text):\n",
        "    text = remove_html_tags(text)\n",
        "    text = re.sub(r\"(🟩\\s*)?第\\d+页(?:内容[:：]| 原文\\s*)\", \"\", text)\n",
        "    text = text.replace(\"【原文】\", \"\")\n",
        "    text = remove_invisible_chars(text)\n",
        "    text = normalize_text(text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "def clean_text_field(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = remove_html_tags(text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def similar(a, b):\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "def merge_similar_rows(group, threshold=0.85):\n",
        "    rows = group.copy()\n",
        "    texts = rows[\"原文_clean\"].tolist()\n",
        "    n = len(texts)\n",
        "    labels = [-1] * n\n",
        "    current_label = 0\n",
        "    for i in range(n):\n",
        "        if labels[i] == -1:\n",
        "            labels[i] = current_label\n",
        "            for j in range(i + 1, n):\n",
        "                if labels[j] == -1:\n",
        "                    if similar(texts[i], texts[j]) >= threshold:\n",
        "                        labels[j] = current_label\n",
        "            current_label += 1\n",
        "    rows[\"group_label\"] = labels\n",
        "\n",
        "    def select_longest_subgroup(subgroup):\n",
        "        max_len = subgroup[\"逐行解析_clean\"].str.len().max()\n",
        "        longest_rows = subgroup[subgroup[\"逐行解析_clean\"].str.len() == max_len]\n",
        "        return longest_rows.iloc[0]\n",
        "\n",
        "    result = rows.groupby(\"group_label\").apply(select_longest_subgroup)\n",
        "    return result.reset_index(drop=True)\n",
        "\n",
        "# 读取anki导出txt\n",
        "df = pd.read_csv(\n",
        "    anki_txt_file, sep=\"\\t\", header=None, usecols=[0,1,2,3,4],\n",
        "    names=[\"页码\", \"section\", \"原文\", \"中文翻译\", \"逐行解析\"]\n",
        ")\n",
        "df[\"页码\"] = pd.to_numeric(df[\"页码\"], errors=\"coerce\")\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "df[\"原文_clean\"] = df[\"原文\"].apply(clean_original)\n",
        "df[\"中文翻译_clean\"] = df[\"中文翻译\"].apply(clean_text_field)\n",
        "df[\"逐行解析_clean\"] = df[\"逐行解析\"].apply(clean_text_field)\n",
        "df[\"len_sum\"] = df[\"中文翻译_clean\"].str.len() + df[\"逐行解析_clean\"].str.len()\n",
        "\n",
        "# 按“页码+原文_clean”分组，保留len_sum最长的\n",
        "grouped = df.groupby([\"页码\", \"原文_clean\"])\n",
        "result_rows = []\n",
        "for name, group in grouped:\n",
        "    max_len = group[\"len_sum\"].max()\n",
        "    max_row = group[group[\"len_sum\"] == max_len].iloc[0]\n",
        "    result_rows.append(max_row)\n",
        "df_cleaned = pd.DataFrame(result_rows)\n",
        "\n",
        "# 删除同原文中逐行解析为空的（只保留非空版本）\n",
        "df_cleaned[\"逐行解析_is_empty\"] = df_cleaned[\"逐行解析_clean\"].str.strip() == \"\"\n",
        "mask_to_drop = (\n",
        "    df_cleaned.groupby(\"原文_clean\")[\"逐行解析_is_empty\"]\n",
        "    .transform(lambda x: x.any() and x.sum() < len(x))\n",
        "    & df_cleaned[\"逐行解析_is_empty\"]\n",
        ")\n",
        "df_cleaned = df_cleaned[~mask_to_drop].copy()\n",
        "df_cleaned = df_cleaned.drop(columns=[\"逐行解析_is_empty\"])\n",
        "\n",
        "# 合并相似原文（只在同一页码内合并）\n",
        "df_merged = df_cleaned.groupby(\"页码\", group_keys=False).apply(merge_similar_rows)\n",
        "\n",
        "# 检测同一页码下原文不同\n",
        "conflicts = df_merged.groupby(\"页码\")[\"原文_clean\"].nunique()\n",
        "conflict_pages = conflicts[conflicts > 1]\n",
        "if not conflict_pages.empty:\n",
        "    print(\"⚠️ 合并相似原文后，仍检测到同一页码下有多个不同原文：\")\n",
        "    for page, count in conflict_pages.items():\n",
        "        print(f\"  页码 {page} 有 {count} 条不同原文：\")\n",
        "        originals = df_merged[df_merged[\"页码\"] == page][\"原文_clean\"].unique()\n",
        "        for idx, orig in enumerate(originals, 1):\n",
        "            print(f\"    {idx}. {orig[:60]}{'...' if len(orig) > 60 else ''}\")\n",
        "\n",
        "df_merged = df_merged.sort_values(by=\"页码\", ascending=True, na_position=\"last\")\n",
        "\n",
        "# 假设最终的 DataFrame 是 df_merged\n",
        "\n",
        "cols_to_export = [\"页码\", \"section\", \"原文\", \"中文翻译\", \"逐行解析\"]\n",
        "\n",
        "df_merged.to_excel(output_excel, columns=cols_to_export, index=False)\n",
        "df_merged.to_csv(output_csv, columns=cols_to_export, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(f\"✅ 处理完成并已按页码排序，已导出到：\\n{output_excel}\\n{output_csv}\")\n",
        "\n",
        "def check_duplicates_and_empty_parsing(df):\n",
        "    # 检测重复页码\n",
        "    page_counts = df[\"页码\"].value_counts()\n",
        "    duplicate_pages = page_counts[page_counts > 1]\n",
        "\n",
        "    if not duplicate_pages.empty:\n",
        "        print(\"⚠️ 存在重复的页码：\")\n",
        "        for page, count in duplicate_pages.items():\n",
        "            print(f\"  页码 {page} 出现了 {count} 次\")\n",
        "    else:\n",
        "        print(\"✅ 没有重复的页码。\")\n",
        "\n",
        "    # 检测逐行解析为空\n",
        "    empty_parsing_rows = df[df[\"逐行解析\"].str.strip() == \"\"]\n",
        "    if not empty_parsing_rows.empty:\n",
        "        print(\"⚠️ 以下记录的‘逐行解析’字段为空：\")\n",
        "        print(empty_parsing_rows[[\"页码\", \"原文\"]])\n",
        "    else:\n",
        "        print(\"✅ 所有记录‘逐行解析’字段均非空。\")\n",
        "\n",
        "# 调用示例\n",
        "check_duplicates_and_empty_parsing(df_merged)\n",
        "\n",
        "# 假设最大页码为数据中的最大页码，也可以自己指定\n",
        "max_page = int(df_merged[\"页码\"].max())\n",
        "\n",
        "expected_pages = set(range(1, max_page + 1))\n",
        "existing_pages = set(df_merged[\"页码\"].dropna().astype(int).unique())\n",
        "\n",
        "missing_pages = expected_pages - existing_pages\n",
        "\n",
        "if missing_pages:\n",
        "    print(\"⚠️ 缺失的页码有：\")\n",
        "    print(sorted(missing_pages))\n",
        "else:\n",
        "    print(\"✅ 没有缺失页码。\")\n",
        "\n",
        "\n",
        "df_merged.to_csv(output_original_csv, columns=[\"原文\"], index=False, encoding=\"utf-8-sig\")\n",
        "df_merged.to_csv(output_translation_csv, columns=[\"中文翻译\"], index=False, encoding=\"utf-8-sig\")\n",
        "df_merged.to_csv(output_original_translation_csv, columns=[\"原文\", \"中文翻译\"], index=False, encoding=\"utf-8-sig\")"
      ],
      "metadata": {
        "id": "ZAlbScxMLI7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 安装依赖（Colab 环境）\n",
        "# ---------------------------\n",
        "!pip install -q chardet beautifulsoup4 markdown pygments\n",
        "\n",
        "# ---------------------------\n",
        "# 导入与配置\n",
        "# ---------------------------\n",
        "import os\n",
        "import chardet\n",
        "from bs4 import BeautifulSoup\n",
        "import markdown\n",
        "from markdown.extensions import fenced_code, tables, attr_list\n",
        "import html\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# 修改这里：\n",
        "# 输入文件夹（你的导出 html 文件放在这里），相对路径示例： \"drive/MyDrive/chatgpt_exports\"\n",
        "# 输出文件夹（生成的渲染 html 会放在这里）\n",
        "INPUT_FOLDER = \"/content/drive/MyDrive/chatgpt_exports\"   # <-- 修改为你 Drive 中的输入文件夹\n",
        "OUTPUT_FOLDER = \"/content/drive/MyDrive/chatgpt_html_rendered_v4\"  # <-- 修改为输出文件夹\n",
        "\n",
        "# ---------------------------\n",
        "# 挂载 Google Drive（如果还没挂载）\n",
        "# ---------------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "# ---------------------------\n",
        "# 工具函数：安全读取并检测编码\n",
        "# ---------------------------\n",
        "def read_file_with_encoding_detection(path):\n",
        "    with open(path, \"rb\") as f:\n",
        "        raw = f.read()\n",
        "    # chardet 检测\n",
        "    res = chardet.detect(raw)\n",
        "    enc = res.get(\"encoding\") or \"utf-8\"\n",
        "    try:\n",
        "        text = raw.decode(enc, errors=\"ignore\")\n",
        "    except Exception:\n",
        "        text = raw.decode(\"utf-8\", errors=\"ignore\")\n",
        "    return text, enc\n",
        "\n",
        "# ---------------------------\n",
        "# 主要解析逻辑（尽量保留原始 HTML / 内部样式）\n",
        "# ---------------------------\n",
        "def extract_messages_from_html(html_text):\n",
        "    \"\"\"\n",
        "    返回一个按顺序的消息列表，每条为 dict: {role: \"user\"|\"chatgpt\"|\"system\", html: \"<原始内部HTML>\", text: \"纯文本\"}\n",
        "    尽量使用多重启发式规则判定角色，优先保留原始内部 HTML（decode_contents）。\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html_text, \"html.parser\")  # 宽松解析\n",
        "\n",
        "    messages = []\n",
        "\n",
        "    # 常见导出里可能的容器选择器（尝试多种）\n",
        "    possible_selectors = [\n",
        "        {'attr':'class', 'values': ['assistant','assistant-message','message-row-assistant','chatgpt']},\n",
        "        {'attr':'class', 'values': ['user','user-message','message-row-user','from-user']},\n",
        "        {'attr':'id', 'values': ['assistant','user']}\n",
        "    ]\n",
        "    # 首先尝试找到明显带 class/id 的消息容器\n",
        "    found_nodes = []\n",
        "    for tag in soup.find_all(True):\n",
        "        classes = [c.lower() for c in (tag.get(\"class\") or [])]\n",
        "        tag_id = (tag.get(\"id\") or \"\").lower()\n",
        "        is_candidate = False\n",
        "        for s in possible_selectors:\n",
        "            if s['attr'] == 'class':\n",
        "                for v in s['values']:\n",
        "                    if v in classes:\n",
        "                        found_nodes.append(tag)\n",
        "                        is_candidate = True\n",
        "                        break\n",
        "            else:\n",
        "                if tag_id in s['values']:\n",
        "                    found_nodes.append(tag)\n",
        "                    is_candidate = True\n",
        "                    break\n",
        "        # 避 deep search too much here; we collect, filter below\n",
        "    # 如果没找到任何带标识节点，退回到遍历 body 的直接子节点\n",
        "    if not found_nodes:\n",
        "        # 遍历 body 的直接子标签（保持顺序）\n",
        "        body = soup.body or soup\n",
        "        for tag in body.find_all(recursive=False):\n",
        "            found_nodes.append(tag)\n",
        "\n",
        "    # 再对 found_nodes 做更严格的筛查与分段\n",
        "    for node in found_nodes:\n",
        "        # 提取纯文本（用于判别）和内部 HTML（用于保留样式）\n",
        "        text = node.get_text(separator=\"\\n\", strip=True)\n",
        "        inner_html = node.decode_contents()  # 保留 node 内部的原始 HTML\n",
        "\n",
        "        # 过滤无实质内容\n",
        "        if not text or len(text.strip()) == 0:\n",
        "            continue\n",
        "\n",
        "        # 判定角色：多条件\n",
        "        role = \"user\"  # 默认用户\n",
        "        classes = [c.lower() for c in (node.get(\"class\") or [])]\n",
        "        node_id = (node.get(\"id\") or \"\").lower()\n",
        "\n",
        "        # 优先依据 class / id 关键词\n",
        "        if any(\"assistant\" in c or \"chatgpt\" in c for c in classes) or \"assistant\" in node_id:\n",
        "            role = \"chatgpt\"\n",
        "        elif any(\"user\" in c or \"from-user\" in c for c in classes) or \"user\" in node_id:\n",
        "            role = \"user\"\n",
        "        # 如果文本很长或含代码、表格、Markdown 特征，倾向 chatgpt\n",
        "        elif len(text) > 200 or \"```\" in inner_html or \"<code\" in inner_html or \"<pre\" in inner_html or \"<table\" in inner_html:\n",
        "            role = \"chatgpt\"\n",
        "        # 含 \"assistant:\" 前缀\n",
        "        elif text.strip().lower().startswith(\"assistant:\") or text.strip().lower().startswith(\"chatgpt:\"):\n",
        "            role = \"chatgpt\"\n",
        "        # 含 \"user:\" 或以问句结尾倾向 user（短）\n",
        "        elif text.strip().endswith(\"?\") and len(text) < 200:\n",
        "            role = \"user\"\n",
        "        else:\n",
        "            # 若 node 在 DOM 中包含子节点里明显是回答/段落（heuristic）\n",
        "            # 保守处理：若节点下包含多段 <p> 或多行换行，倾向 chatgpt\n",
        "            if inner_html.count(\"<p\") >= 2 or inner_html.count(\"<br\") >= 3 or inner_html.count(\"\\n\") >= 4:\n",
        "                role = \"chatgpt\"\n",
        "            else:\n",
        "                role = \"user\"\n",
        "\n",
        "        # Push message (保留原始 inner_html)\n",
        "        messages.append({\"role\": role, \"html\": inner_html, \"text\": text})\n",
        "\n",
        "    # 最后尝试合并相邻同 role 的短块，避免碎片化\n",
        "    merged = []\n",
        "    for m in messages:\n",
        "        if merged and merged[-1][\"role\"] == m[\"role\"]:\n",
        "            # 合并 HTML 和 text（保留之间换行）\n",
        "            merged[-1][\"html\"] += \"<hr style='display:none;margin:0;padding:0;border:0;height:0;'>\\n\" + m[\"html\"]\n",
        "            merged[-1][\"text\"] += \"\\n\\n\" + m[\"text\"]\n",
        "        else:\n",
        "            merged.append(m)\n",
        "    return merged\n",
        "\n",
        "# ---------------------------\n",
        "# 生成交互式网页模板\n",
        "# ---------------------------\n",
        "def generate_output_html(messages, title=\"ChatGPT 会话 复原 v4\", generated_time=None):\n",
        "    if generated_time is None:\n",
        "        generated_time = datetime.utcnow().isoformat() + \"Z\"\n",
        "    # Header + core css/js（包含搜索、折叠、主题切换、代码高亮、MathJax）\n",
        "    header = f\"\"\"<!DOCTYPE html>\n",
        "<html lang=\"zh\">\n",
        "<head>\n",
        "<meta charset=\"utf-8\">\n",
        "<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\">\n",
        "<title>{html.escape(title)}</title>\n",
        "\n",
        "<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css\">\n",
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js\"></script>\n",
        "<script>window.addEventListener('load', ()=>{{hljs.highlightAll();}});</script>\n",
        "\n",
        "<!-- MathJax -->\n",
        "<script src=\"https://polyfill.io/v3/polyfill.min.js?features=es6\"></script>\n",
        "<script id=\"MathJax-script\" async\n",
        "  src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\">\n",
        "</script>\n",
        "\n",
        "<style>\n",
        ":root {{\n",
        "  --bg: #f7f7f8;\n",
        "  --user-bg: #e6e6e6;\n",
        "  --assistant-bg: #ffffff;\n",
        "  --assistant-accent: #0b93f6;\n",
        "  --text: #0b0b0b;\n",
        "  --muted: #555;\n",
        "}}\n",
        "html,body{{height:100%;}}\n",
        "body {{\n",
        "  margin:0;\n",
        "  font-family: Inter, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial, sans-serif;\n",
        "  background: var(--bg);\n",
        "  color: var(--text);\n",
        "  padding: 20px;\n",
        "}}\n",
        ".container {{\n",
        "  max-width:1100px;\n",
        "  margin:0 auto;\n",
        "}}\n",
        ".header-row {{\n",
        "  display:flex;align-items:center;justify-content:space-between;margin-bottom:14px;\n",
        "}}\n",
        ".toolbar {{\n",
        "  display:flex;gap:8px;align-items:center;\n",
        "}}\n",
        ".btn {{\n",
        "  padding:8px 10px;border-radius:8px;border:1px solid rgba(0,0,0,0.08);background:#fff;cursor:pointer;\n",
        "}}\n",
        ".search-input{{padding:8px;border-radius:8px;border:1px solid #ddd;min-width:220px;}}\n",
        "\n",
        ".chat-area {{ background:transparent;padding:8px 0; }}\n",
        "\n",
        ".message {{\n",
        "  display:block;\n",
        "  clear:both;\n",
        "  padding:14px 18px;\n",
        "  margin:10px 0;\n",
        "  border-radius:12px;\n",
        "  max-width:78%;\n",
        "  word-wrap:break-word;\n",
        "  box-shadow: 0 6px 18px rgba(16,24,40,0.04);\n",
        "}}\n",
        ".user {{\n",
        "  background:var(--user-bg);\n",
        "  float:left;\n",
        "}}\n",
        ".chatgpt {{\n",
        "  background:var(--assistant-bg);\n",
        "  float:right;\n",
        "  border-left:4px solid var(--assistant-accent);\n",
        "}}\n",
        "\n",
        ".meta {{ font-size:12px;color:var(--muted);margin-bottom:6px; }}\n",
        "\n",
        ".message pre {{\n",
        "  background:#f0f0f0;\n",
        "  padding:12px;border-radius:8px;overflow:auto;cursor:pointer;\n",
        "  max-height:220px;\n",
        "}}\n",
        ".message table{{ border-collapse: collapse; width:100%;}}\n",
        ".message table, .message th, .message td {{ border:1px solid #ccc; padding:6px;}}\n",
        ".message img{{ max-width:100%; display:block; margin:8px auto; border-radius:6px; }}\n",
        "\n",
        ".clearfix{{clear:both;}}\n",
        "\n",
        "/* night theme */\n",
        "body.night {{\n",
        "  --bg: #0b1020;\n",
        "  --user-bg: #151826;\n",
        "  --assistant-bg: #071127;\n",
        "  --assistant-accent: #2fa6ff;\n",
        "  --text: #e6eef6;\n",
        "  --muted: #9fb0c8;\n",
        "}}\n",
        ".header-info {{ font-size:12px;color:var(--muted); }}\n",
        ".controls-row {{ display:flex; gap:8px; align-items:center; }}\n",
        ".copy-btn {{ margin-left:8px; padding:4px 8px; font-size:12px; }}\n",
        "\n",
        ".search-highlight {{ background: rgba(255,230,100,0.6); padding:0 3px; border-radius:2px; }}\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "<div class=\"container\">\n",
        "  <div class=\"header-row\">\n",
        "    <div>\n",
        "      <h2 style=\"margin:0 0 6px 0;\">{html.escape(title)}</h2>\n",
        "      <div class=\"header-info\">生成时间: {generated_time} · 文件渲染自导出 HTML</div>\n",
        "    </div>\n",
        "    <div class=\"toolbar\">\n",
        "      <input class=\"search-input\" id=\"globalSearch\" placeholder=\"搜索会话（回车高亮）\" />\n",
        "      <button class=\"btn\" id=\"btnSearch\">搜索</button>\n",
        "      <button class=\"btn\" id=\"btnClearSearch\">清除</button>\n",
        "      <button class=\"btn\" id=\"btnCollapseAll\">折叠全部</button>\n",
        "      <button class=\"btn\" id=\"btnExpandAll\">展开全部</button>\n",
        "      <button class=\"btn\" id=\"btnToggleTheme\">夜间/白天</button>\n",
        "      <a class=\"btn\" id=\"downloadAll\" href=\"#\" download=\"chatgpt_session_rendered.html\">下载当前页面</a>\n",
        "    </div>\n",
        "  </div>\n",
        "\n",
        "  <div class=\"chat-area\" id=\"chatArea\">\n",
        "\"\"\"\n",
        "    # Messages insertion\n",
        "    body_parts = []\n",
        "    idx = 0\n",
        "    for m in messages:\n",
        "        idx += 1\n",
        "        role = m.get(\"role\",\"user\")\n",
        "        css_role = \"chatgpt\" if role==\"chatgpt\" else (\"user\" if role==\"user\" else \"user\")\n",
        "        # Sanitize minimal but keep HTML inside (we assume the user trusts these files)\n",
        "        inner_html = m.get(\"html\",\"\")\n",
        "        # ensure images lazy-load\n",
        "        inner_html = inner_html.replace(\"<img \", '<img loading=\"lazy\" ')\n",
        "        # wrap with meta and message container\n",
        "        html_msg = f'''\n",
        "    <div class=\"message {css_role}\" data-index=\"{idx}\">\n",
        "      <div class=\"meta\">{html.escape(role)} · 消息 #{idx}</div>\n",
        "      <div class=\"content\">{inner_html}</div>\n",
        "      <div style=\"margin-top:8px;\">\n",
        "        <button class=\"btn copy-btn\" onclick=\"copyMessage({idx})\">复制文字</button>\n",
        "        <button class=\"btn copy-btn\" onclick=\"copyHTML({idx})\">复制 HTML</button>\n",
        "      </div>\n",
        "    </div>\n",
        "    <div class=\"clearfix\"></div>\n",
        "'''\n",
        "        body_parts.append(html_msg)\n",
        "\n",
        "    footer = \"\"\"\n",
        "  </div> <!-- chat-area -->\n",
        "</div> <!-- container -->\n",
        "\n",
        "<script>\n",
        "function qs(sel){return document.querySelector(sel);}\n",
        "function qsa(sel){return Array.from(document.querySelectorAll(sel));}\n",
        "\n",
        "// copy helpers\n",
        "function copyToClipboard(text){\n",
        "  navigator.clipboard.writeText(text).then(()=>{ alert('已复制到剪贴板'); },(e)=>{ alert('复制失败：'+e); });\n",
        "}\n",
        "function copyMessage(idx){\n",
        "  const el = document.querySelector('.message[data-index=\"'+idx+'\"] .content');\n",
        "  copyToClipboard(el.innerText.trim());\n",
        "}\n",
        "function copyHTML(idx){\n",
        "  const el = document.querySelector('.message[data-index=\"'+idx+'\"] .content');\n",
        "  copyToClipboard(el.innerHTML.trim());\n",
        "}\n",
        "\n",
        "// collapse/expand behavior for pre and tables and large images\n",
        "function setUpCollapsibles(){\n",
        "  qsa('.message pre').forEach(pre=>{\n",
        "    pre.style.maxHeight = '220px';\n",
        "    pre.style.overflow = 'hidden';\n",
        "    pre.dataset.collapsed = '1';\n",
        "    pre.addEventListener('click', ()=>{\n",
        "      if(pre.dataset.collapsed==='1'){ pre.style.maxHeight='none'; pre.dataset.collapsed='0'; }\n",
        "      else { pre.style.maxHeight='220px'; pre.dataset.collapsed='1'; }\n",
        "    });\n",
        "  });\n",
        "  qsa('.message table').forEach(tbl=>{\n",
        "    const wrapper = document.createElement('div');\n",
        "    wrapper.style.maxHeight = '220px';\n",
        "    wrapper.style.overflow = 'hidden';\n",
        "    wrapper.style.cursor = 'pointer';\n",
        "    tbl.parentNode.insertBefore(wrapper, tbl);\n",
        "    wrapper.appendChild(tbl);\n",
        "    wrapper.dataset.collapsed = '1';\n",
        "    wrapper.addEventListener('click', ()=>{\n",
        "      if(wrapper.dataset.collapsed==='1'){ wrapper.style.maxHeight='none'; wrapper.dataset.collapsed='0'; }\n",
        "      else { wrapper.style.maxHeight='220px'; wrapper.dataset.collapsed='1'; }\n",
        "    });\n",
        "  });\n",
        "  qsa('.message img').forEach(img=>{\n",
        "    // large images: set max-height and allow click to toggle\n",
        "    img.style.maxHeight = '360px';\n",
        "    img.style.objectFit = 'contain';\n",
        "    img.style.cursor = 'pointer';\n",
        "    img.dataset.collapsed = '1';\n",
        "    img.addEventListener('click', ()=>{\n",
        "      if(img.dataset.collapsed==='1'){ img.style.maxHeight='none'; img.dataset.collapsed='0'; }\n",
        "      else { img.style.maxHeight='360px'; img.dataset.collapsed='1'; }\n",
        "    });\n",
        "  });\n",
        "}\n",
        "\n",
        "// global collapse/expand\n",
        "function collapseAll(){\n",
        "  qsa('.message pre').forEach(pre=>{ pre.style.maxHeight='220px'; pre.dataset.collapsed='1'; });\n",
        "  qsa('.message table').forEach(wrapper=>{ if(wrapper.parentNode && wrapper.parentNode.querySelector('div')){ /* skip */ }});\n",
        "  qsa('.message img').forEach(img=>{ img.style.maxHeight='360px'; img.dataset.collapsed='1'; });\n",
        "}\n",
        "function expandAll(){\n",
        "  qsa('.message pre').forEach(pre=>{ pre.style.maxHeight='none'; pre.dataset.collapsed='0'; });\n",
        "  qsa('.message table').forEach(tbl=>{ if(tbl.parentNode) tbl.parentNode.style.maxHeight='none'; });\n",
        "  qsa('.message img').forEach(img=>{ img.style.maxHeight='none'; img.dataset.collapsed='0'; });\n",
        "}\n",
        "\n",
        "// search\n",
        "function clearHighlights(){\n",
        "  qsa('.search-highlight').forEach(el=>{\n",
        "    const parent = el.parentNode;\n",
        "    parent.replaceChild(document.createTextNode(el.textContent), el);\n",
        "  });\n",
        "}\n",
        "function highlightSearch(term){\n",
        "  if(!term) return;\n",
        "  clearHighlights();\n",
        "  const re = new RegExp(term.replace(/[-/\\\\^$*+?.()|[\\\\]{}]/g, '\\\\\\\\$&'), 'ig');\n",
        "  qsa('.message .content').forEach(node=>{\n",
        "    // walk text nodes and replace matches with span\n",
        "    function walk(n){\n",
        "      if(n.nodeType === 3){\n",
        "        const match = n.nodeValue.match(re);\n",
        "        if(match){\n",
        "          const span = document.createElement('span');\n",
        "          span.innerHTML = n.nodeValue.replace(re, function(m){ return '<span class=\"search-highlight\">'+m+'</span>'; });\n",
        "          n.parentNode.replaceChild(span, n);\n",
        "        }\n",
        "      } else {\n",
        "        for(let i=0;i<n.childNodes.length;i++){ walk(n.childNodes[i]); }\n",
        "      }\n",
        "    }\n",
        "    walk(node);\n",
        "  });\n",
        "}\n",
        "\n",
        "// download current page as file\n",
        "document.getElementById('downloadAll').addEventListener('click', function(e){\n",
        "  const blob = new Blob([document.documentElement.outerHTML], {type:'text/html'});\n",
        "  const url = URL.createObjectURL(blob);\n",
        "  this.href = url;\n",
        "  setTimeout(()=>URL.revokeObjectURL(url), 10000);\n",
        "});\n",
        "\n",
        "// toolbar actions\n",
        "document.getElementById('btnCollapseAll').addEventListener('click', collapseAll);\n",
        "document.getElementById('btnExpandAll').addEventListener('click', expandAll);\n",
        "document.getElementById('btnToggleTheme').addEventListener('click', ()=>{\n",
        "  document.body.classList.toggle('night');\n",
        "});\n",
        "document.getElementById('btnSearch').addEventListener('click', ()=>{\n",
        "  const q = document.getElementById('globalSearch').value.trim();\n",
        "  clearHighlights();\n",
        "  if(q) highlightSearch(q);\n",
        "});\n",
        "document.getElementById('btnClearSearch').addEventListener('click', ()=>{\n",
        "  document.getElementById('globalSearch').value='';\n",
        "  clearHighlights();\n",
        "});\n",
        "document.getElementById('globalSearch').addEventListener('keydown', function(e){\n",
        "  if(e.key === 'Enter'){ document.getElementById('btnSearch').click(); }\n",
        "});\n",
        "\n",
        "// init\n",
        "setUpCollapsibles();\n",
        "</script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "    return header + \"\\n\".join(body_parts) + footer\n",
        "\n",
        "# ---------------------------\n",
        "# 处理文件夹中所有 .html 文件\n",
        "# ---------------------------\n",
        "def process_folder(input_dir, output_dir):\n",
        "    input_dir = Path(input_dir)\n",
        "    output_dir = Path(output_dir)\n",
        "    files = sorted([p for p in input_dir.iterdir() if p.suffix.lower() == \".html\"])\n",
        "    if not files:\n",
        "        print(\"未找到任何 .html 文件，检查 INPUT_FOLDER 路径是否正确：\", input_dir)\n",
        "        return\n",
        "    print(f\"发现 {len(files)} 个 HTML 文件，开始处理...\")\n",
        "    for p in files:\n",
        "        try:\n",
        "            print(\"处理：\", p.name)\n",
        "            txt, enc = read_file_with_encoding_detection(str(p))\n",
        "            msgs = extract_messages_from_html(txt)\n",
        "            out_html = generate_output_html(msgs, title=f\"Rendered - {p.name}\", generated_time=datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%SZ\"))\n",
        "            out_name = p.stem + \"_rendered_v4.html\"\n",
        "            out_path = output_dir / out_name\n",
        "            with open(out_path, \"w\", encoding=\"utf-8\") as fw:\n",
        "                fw.write(out_html)\n",
        "            print(\"已生成：\", out_path)\n",
        "        except Exception as e:\n",
        "            print(\"处理失败：\", p.name, e)\n",
        "\n",
        "# ---------------------------\n",
        "# 运行\n",
        "# ---------------------------\n",
        "process_folder(INPUT_FOLDER, OUTPUT_FOLDER)\n",
        "print(\"全部完成。生成文件位于：\", OUTPUT_FOLDER)"
      ],
      "metadata": {
        "id": "UfzfNtAfHqxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "708a2df2-1b16-425e-96e6-292e119b1c75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "发现 1 个 HTML 文件，开始处理...\n",
            "处理： 解释下文意思.html\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3456665724.py:445: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  out_html = generate_output_html(msgs, title=f\"Rendered - {p.name}\", generated_time=datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%SZ\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "已生成： /content/drive/MyDrive/chatgpt_html_rendered_v4/解释下文意思_rendered_v4.html\n",
            "全部完成。生成文件位于： /content/drive/MyDrive/chatgpt_html_rendered_v4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import chardet\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import files\n",
        "\n",
        "# ============= 配置区 =============\n",
        "# Google Drive 挂载后的文件夹路径\n",
        "input_folder = \"/content/drive/MyDrive/chatgpt_exports\"\n",
        "# 输出文件名\n",
        "output_file = \"chatgpt_only_fullscreen.html\"\n",
        "# =================================\n",
        "\n",
        "# 收集所有 ChatGPT 消息\n",
        "chatgpt_messages = []\n",
        "\n",
        "# 遍历文件夹中的所有文件\n",
        "for filename in os.listdir(input_folder):\n",
        "    file_path = os.path.join(input_folder, filename)\n",
        "\n",
        "    if not os.path.isfile(file_path):\n",
        "        continue\n",
        "\n",
        "    # 自动检测编码\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        raw_data = f.read()\n",
        "    result = chardet.detect(raw_data)\n",
        "    encoding = result['encoding'] or 'utf-8'\n",
        "\n",
        "    # 解码并用 BeautifulSoup 解析\n",
        "    text = raw_data.decode(encoding, errors=\"ignore\")\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "\n",
        "    # 遍历所有标签，筛选 ChatGPT 回复\n",
        "    for tag in soup.find_all(True):\n",
        "        tag_text = tag.get_text(strip=True)\n",
        "        if not tag_text:\n",
        "            continue\n",
        "\n",
        "        # 用户输入一般很短，排除掉\n",
        "        if len(tag_text) < 30:\n",
        "            continue\n",
        "\n",
        "        # ChatGPT 回复通常较长，或者 class 里含有 chatgpt/assistant\n",
        "        tag_classes = tag.get(\"class\", [])\n",
        "        if any(\"assistant\" in c.lower() or \"chatgpt\" in c.lower() for c in tag_classes) or len(tag_text) > 80:\n",
        "            chatgpt_messages.append(str(tag))\n",
        "\n",
        "# 去重，保持顺序\n",
        "seen = set()\n",
        "unique_messages = []\n",
        "for msg in chatgpt_messages:\n",
        "    if msg not in seen:\n",
        "        unique_messages.append(msg)\n",
        "        seen.add(msg)\n",
        "\n",
        "# 生成 HTML 页面（全屏显示 ChatGPT 回复）\n",
        "html_template = f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"zh\">\n",
        "<head>\n",
        "  <meta charset=\"UTF-8\">\n",
        "  <title>ChatGPT Only</title>\n",
        "  <style>\n",
        "    body {{\n",
        "      font-family: Arial, sans-serif;\n",
        "      margin: 0;\n",
        "      padding: 20px;\n",
        "      background: #f9f9f9;\n",
        "    }}\n",
        "    .chatgpt-message {{\n",
        "      width: 100%;\n",
        "      background: #ffffff;\n",
        "      border-radius: 10px;\n",
        "      padding: 20px;\n",
        "      margin: 15px 0;\n",
        "      box-shadow: 0 2px 6px rgba(0,0,0,0.1);\n",
        "      white-space: pre-wrap;\n",
        "    }}\n",
        "  </style>\n",
        "</head>\n",
        "<body>\n",
        "  {\"\".join(f'<div class=\"chatgpt-message\">{msg}</div>' for msg in unique_messages)}\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# 保存到文件\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(html_template)\n",
        "\n",
        "# 提供下载\n",
        "files.download(output_file)\n",
        "print(f\"✅ 已生成并打包 ChatGPT 回复，文件名: {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "2bhejVikCvtY",
        "outputId": "159da31c-f8df-4cf1-ecaf-55fb9efef0c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b00a501e-2697-436d-a6f2-f1e6dc170472\", \"chatgpt_only_fullscreen.html\", 313129698)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 已生成并打包 ChatGPT 回复，文件名: chatgpt_only_fullscreen.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import chardet\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import files\n",
        "\n",
        "# ============= 配置区 =============\n",
        "input_folder = \"/content/drive/MyDrive/chatgpt_exports\"\n",
        "output_file = \"chatgpt_only_clean.html\"\n",
        "# =================================\n",
        "\n",
        "chatgpt_messages = []\n",
        "\n",
        "for filename in os.listdir(input_folder):\n",
        "    file_path = os.path.join(input_folder, filename)\n",
        "\n",
        "    if not os.path.isfile(file_path):\n",
        "        continue\n",
        "\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        raw_data = f.read()\n",
        "    result = chardet.detect(raw_data)\n",
        "    encoding = result['encoding'] or 'utf-8'\n",
        "    text = raw_data.decode(encoding, errors=\"ignore\")\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "\n",
        "    for tag in soup.find_all(True):\n",
        "        tag_text = tag.get_text(strip=True)\n",
        "        if not tag_text:\n",
        "            continue\n",
        "        if len(tag_text) < 30:  # 短的，跳过（用户输入）\n",
        "            continue\n",
        "\n",
        "        tag_classes = tag.get(\"class\", [])\n",
        "        if any(\"assistant\" in c.lower() or \"chatgpt\" in c.lower() for c in tag_classes) or len(tag_text) > 80:\n",
        "            # 去掉图片和 script\n",
        "            for img in tag.find_all(\"img\"):\n",
        "                img.decompose()\n",
        "            for script in tag.find_all(\"script\"):\n",
        "                script.decompose()\n",
        "            for style in tag.find_all(\"style\"):\n",
        "                style.decompose()\n",
        "\n",
        "            # 只保留纯净 HTML 结构\n",
        "            chatgpt_messages.append(tag.get_text())\n",
        "\n",
        "# 去重\n",
        "seen = set()\n",
        "unique_messages = []\n",
        "for msg in chatgpt_messages:\n",
        "    if msg not in seen:\n",
        "        unique_messages.append(msg)\n",
        "        seen.add(msg)\n",
        "\n",
        "# 生成精简 HTML\n",
        "html_template = f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"zh\">\n",
        "<head>\n",
        "  <meta charset=\"UTF-8\">\n",
        "  <title>ChatGPT Only Clean</title>\n",
        "  <style>\n",
        "    body {{\n",
        "      font-family: Arial, sans-serif;\n",
        "      margin: 0;\n",
        "      padding: 20px;\n",
        "      background: #f9f9f9;\n",
        "    }}\n",
        "    .chatgpt-message {{\n",
        "      width: 100%;\n",
        "      background: #ffffff;\n",
        "      border-radius: 10px;\n",
        "      padding: 20px;\n",
        "      margin: 15px 0;\n",
        "      box-shadow: 0 2px 6px rgba(0,0,0,0.1);\n",
        "      white-space: pre-wrap;\n",
        "    }}\n",
        "  </style>\n",
        "</head>\n",
        "<body>\n",
        "  {\"\".join(f'<div class=\"chatgpt-message\">{msg}</div>' for msg in unique_messages)}\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(html_template)\n",
        "\n",
        "files.download(output_file)\n",
        "print(f\"✅ 已生成精简版 ChatGPT 回复，文件名: {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "byfLK0bWEUis",
        "outputId": "4f63545b-502d-4002-a998-8f4c4e2e0b90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0eab413c-63d8-4511-b34a-5bc35ddd30c5\", \"chatgpt_only_clean.html\", 68025007)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 已生成精简版 ChatGPT 回复，文件名: chatgpt_only_clean.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 安装依赖（Colab 环境）\n",
        "# ---------------------------\n",
        "!pip install -q chardet beautifulsoup4 markdown pygments\n",
        "\n",
        "# ---------------------------\n",
        "# 导入与配置\n",
        "# ---------------------------\n",
        "import os\n",
        "import chardet\n",
        "from bs4 import BeautifulSoup\n",
        "import html\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from google.colab import drive, files\n",
        "\n",
        "# 修改这里\n",
        "INPUT_FOLDER = \"/content/drive/MyDrive/chatgpt_exports\"   # 输入文件夹\n",
        "OUTPUT_FOLDER = \"/content/drive/MyDrive/chatgpt_html_rendered_clean\"  # 输出文件夹\n",
        "\n",
        "# ---------------------------\n",
        "# 挂载 Google Drive\n",
        "# ---------------------------\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "# ---------------------------\n",
        "# 工具函数：安全读取并检测编码\n",
        "# ---------------------------\n",
        "def read_file_with_encoding_detection(path):\n",
        "    with open(path, \"rb\") as f:\n",
        "        raw = f.read()\n",
        "    res = chardet.detect(raw)\n",
        "    enc = res.get(\"encoding\") or \"utf-8\"\n",
        "    try:\n",
        "        text = raw.decode(enc, errors=\"ignore\")\n",
        "    except Exception:\n",
        "        text = raw.decode(\"utf-8\", errors=\"ignore\")\n",
        "    return text, enc\n",
        "\n",
        "# ---------------------------\n",
        "# 提取消息：只保留 ChatGPT\n",
        "# ---------------------------\n",
        "def extract_chatgpt_messages(html_text):\n",
        "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
        "    messages = []\n",
        "\n",
        "    for tag in soup.find_all(True):\n",
        "        text = tag.get_text(separator=\"\\n\", strip=True)\n",
        "        if not text:\n",
        "            continue\n",
        "\n",
        "        classes = [c.lower() for c in (tag.get(\"class\") or [])]\n",
        "        node_id = (tag.get(\"id\") or \"\").lower()\n",
        "        inner_html = tag.decode_contents()\n",
        "\n",
        "        # 判断是否 ChatGPT\n",
        "        is_chatgpt = False\n",
        "        if any(\"assistant\" in c or \"chatgpt\" in c for c in classes):\n",
        "            is_chatgpt = True\n",
        "        elif \"assistant\" in node_id:\n",
        "            is_chatgpt = True\n",
        "        elif len(text) > 200 or \"```\" in inner_html or \"<pre\" in inner_html:\n",
        "            is_chatgpt = True\n",
        "\n",
        "        if is_chatgpt:\n",
        "            messages.append({\"html\": inner_html, \"text\": text})\n",
        "\n",
        "    return messages\n",
        "\n",
        "# ---------------------------\n",
        "# 生成 HTML（ChatGPT 全屏显示）\n",
        "# ---------------------------\n",
        "def generate_output_html(messages, title=\"ChatGPT 会话（只保留回答，全屏）\"):\n",
        "    header = f\"\"\"<!DOCTYPE html>\n",
        "<html lang=\"zh\">\n",
        "<head>\n",
        "<meta charset=\"utf-8\">\n",
        "<title>{html.escape(title)}</title>\n",
        "<style>\n",
        "body {{\n",
        "  font-family: Arial, sans-serif;\n",
        "  margin: 0;\n",
        "  padding: 20px;\n",
        "  background: #f9f9f9;\n",
        "  color: #222;\n",
        "}}\n",
        ".chatgpt-message {{\n",
        "  width: 100%;   /* ✅ 全屏宽度 */\n",
        "  background: #ffffff;\n",
        "  border-radius: 10px;\n",
        "  padding: 20px;\n",
        "  margin: 20px 0;\n",
        "  box-shadow: 0 2px 6px rgba(0,0,0,0.1);\n",
        "  line-height: 1.6;\n",
        "  word-wrap: break-word;\n",
        "}}\n",
        ".chatgpt-message pre {{\n",
        "  background: #f0f0f0;\n",
        "  padding: 10px;\n",
        "  border-radius: 6px;\n",
        "  overflow-x: auto;\n",
        "}}\n",
        ".chatgpt-message table {{\n",
        "  border-collapse: collapse;\n",
        "  width: 100%;\n",
        "}}\n",
        ".chatgpt-message table, .chatgpt-message th, .chatgpt-message td {{\n",
        "  border: 1px solid #ccc;\n",
        "  padding: 6px;\n",
        "}}\n",
        ".chatgpt-message img {{\n",
        "  max-width: 100%;\n",
        "  display: block;\n",
        "  margin: 10px auto;\n",
        "}}\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "<h2>{html.escape(title)}</h2>\n",
        "\"\"\"\n",
        "\n",
        "    footer = \"</body></html>\"\n",
        "\n",
        "    body_html = \"\"\n",
        "    for m in messages:\n",
        "        body_html += f'<div class=\"chatgpt-message\">{m[\"html\"]}</div>\\n'\n",
        "\n",
        "    return header + body_html + footer\n",
        "\n",
        "# ---------------------------\n",
        "# 主逻辑：处理文件夹内所有导出文件\n",
        "# ---------------------------\n",
        "for filename in os.listdir(INPUT_FOLDER):\n",
        "    if not filename.endswith(\".html\"):\n",
        "        continue\n",
        "    file_path = os.path.join(INPUT_FOLDER, filename)\n",
        "    text, enc = read_file_with_encoding_detection(file_path)\n",
        "    msgs = extract_chatgpt_messages(text)\n",
        "    out_html = generate_output_html(msgs, title=f\"{filename} - ChatGPT 回答\")\n",
        "\n",
        "    out_path = os.path.join(OUTPUT_FOLDER, filename.replace(\".html\", \"_clean.html\"))\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(out_html)\n",
        "    print(f\"✅ 已处理: {filename} -> {out_path}\")\n",
        "\n",
        "# ---------------------------\n",
        "# 打包并下载\n",
        "# ---------------------------\n",
        "import shutil\n",
        "shutil.make_archive(\"/content/chatgpt_only_clean\", 'zip', OUTPUT_FOLDER)\n",
        "files.download(\"/content/chatgpt_only_clean.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "wZsQPDj6FVQy",
        "outputId": "c46e2df9-9e5a-48f6-e908-88b350ac22f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ 已处理: 解释下文意思.html -> /content/drive/MyDrive/chatgpt_html_rendered_clean/解释下文意思_clean.html\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5ff6b0b5-5400-465e-95f8-503d5b20b1e7\", \"chatgpt_only_clean.zip\", 41914253)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------------------------\n",
        "# 精简且稳定的 Colab 脚本：只保留 ChatGPT 回复（全屏），去除垃圾与内联图片\n",
        "# 直接复制到 Google Colab 执行\n",
        "# ---------------------------\n",
        "\n",
        "# 安装依赖（Colab）\n",
        "!pip install -q chardet beautifulsoup4\n",
        "\n",
        "# ---------------------------\n",
        "# 导入\n",
        "# ---------------------------\n",
        "import os, re, shutil\n",
        "import chardet\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "import html\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from google.colab import drive, files\n",
        "\n",
        "# ---------------------------\n",
        "# 配置（请按需修改）\n",
        "# ---------------------------\n",
        "INPUT_FOLDER = \"/content/drive/MyDrive/chatgpt_exports\"   # <- 修改为你 Drive 的输入文件夹\n",
        "OUTPUT_FOLDER = \"/content/chatgpt_html_clean_outputs\"     # 输出到此临时目录（会被打包并下载）\n",
        "REMOVE_IMAGES = True   # 是否删除所有图片（True 可显著减小体积）\n",
        "MIN_ASSISTANT_LENGTH = 120  # 判定为 assistant 的最小纯文本长度阈值（可调整）\n",
        "\n",
        "# ---------------------------\n",
        "# 挂载 Google Drive\n",
        "# ---------------------------\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "# ---------------------------\n",
        "# 读取并检测编码（并修剪开头垃圾）\n",
        "# ---------------------------\n",
        "def read_file_with_encoding_detection(path):\n",
        "    with open(path, \"rb\") as f:\n",
        "        raw = f.read()\n",
        "    res = chardet.detect(raw)\n",
        "    enc = res.get(\"encoding\") or \"utf-8\"\n",
        "    try:\n",
        "        text = raw.decode(enc, errors=\"ignore\")\n",
        "    except Exception:\n",
        "        text = raw.decode(\"utf-8\", errors=\"ignore\")\n",
        "    # **去掉开头的非 HTML 元数据 / 垃圾**：从第一个 '<' 开始\n",
        "    first_lt = text.find('<')\n",
        "    if 0 <= first_lt < 2000:\n",
        "        text = text[first_lt:]\n",
        "    # 也去掉文件末尾可能的奇怪控制字符\n",
        "    text = text.strip('\\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\x0b\\x0c\\x0e\\x0f')\n",
        "    return text, enc\n",
        "\n",
        "# ---------------------------\n",
        "# HTML 片段安全清洗：只保留允许的标签和少量属性\n",
        "# ---------------------------\n",
        "def sanitize_fragment(fragment_html, remove_images=True):\n",
        "    allowed_tags = {\n",
        "        'p':[], 'br':[], 'pre':[], 'code':[], 'blockquote':[],\n",
        "        'ul':[], 'ol':[], 'li':[], 'strong':[], 'b':[], 'em':[], 'i':[],\n",
        "        'a':['href','title','target','rel'], 'table':[], 'thead':[], 'tbody':[],\n",
        "        'tr':[], 'th':[], 'td':[]\n",
        "    }\n",
        "    soup = BeautifulSoup(fragment_html, \"html.parser\")\n",
        "    # remove scripts, styles, comments\n",
        "    for s in soup(['script','style']):\n",
        "        s.decompose()\n",
        "    for c in soup.find_all(string=lambda text:isinstance(text, Comment)):\n",
        "        c.extract()\n",
        "    # images: remove base64 or all images\n",
        "    for img in soup.find_all('img'):\n",
        "        src = (img.get('src') or '').strip()\n",
        "        if remove_images or src.lower().startswith('data:'):\n",
        "            img.decompose()\n",
        "        else:\n",
        "            # keep image but lazy-load and drop inline attrs\n",
        "            img_attrs = {}\n",
        "            if src:\n",
        "                img_attrs['src'] = src\n",
        "            if img.get('alt'):\n",
        "                img_attrs['alt'] = img.get('alt')\n",
        "            img_attrs['loading'] = 'lazy'\n",
        "            img.attrs = img_attrs\n",
        "    # sanitize tags: unwrap disallowed tags, keep allowed tags and allowed attrs only\n",
        "    for tag in list(soup.find_all(True)):\n",
        "        if tag.name not in allowed_tags:\n",
        "            tag.unwrap()\n",
        "        else:\n",
        "            # keep only allowed attributes (and sanitize href)\n",
        "            keep = allowed_tags.get(tag.name, [])\n",
        "            attrs = dict(tag.attrs)\n",
        "            for k in list(attrs.keys()):\n",
        "                if k not in keep:\n",
        "                    del tag.attrs[k]\n",
        "            if tag.name == 'a' and tag.has_attr('href'):\n",
        "                href = tag['href'].strip()\n",
        "                # remove javascript: or data: hrefs\n",
        "                if href.lower().startswith('javascript:') or href.lower().startswith('data:'):\n",
        "                    del tag.attrs['href']\n",
        "                else:\n",
        "                    # keep absolute/relative links\n",
        "                    tag.attrs['href'] = href\n",
        "                    tag.attrs['target'] = '_blank'\n",
        "                    tag.attrs['rel'] = 'noopener noreferrer'\n",
        "    # collapse excessive whitespace\n",
        "    cleaned = str(soup)\n",
        "    cleaned = re.sub(r'\\n{3,}', '\\n\\n', cleaned)\n",
        "    return cleaned\n",
        "\n",
        "# ---------------------------\n",
        "# 更严格地只提取 ChatGPT / assistant 段落\n",
        "# ---------------------------\n",
        "def extract_chatgpt_messages(html_text):\n",
        "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
        "    # 预先去掉 scripts/styles/comments\n",
        "    for s in soup(['script','style']):\n",
        "        s.decompose()\n",
        "    for c in soup.find_all(string=lambda text:isinstance(text, Comment)):\n",
        "        c.extract()\n",
        "    messages = []\n",
        "    # Candidate nodes: 优先查找带 class/id 与 message 相关的节点，但若找不到再遍历所有 block-level 标签\n",
        "    # 寻找明显包含 assistant 的节点\n",
        "    candidates = []\n",
        "    # 先找类名/id里显式含 assistant/chatgpt 的节点\n",
        "    for tag in soup.find_all(True):\n",
        "        classes = [c.lower() for c in (tag.get('class') or [])]\n",
        "        tid = (tag.get('id') or '').lower()\n",
        "        if any('assistant' in c or 'chatgpt' in c for c in classes) or 'assistant' in tid or 'chatgpt' in tid:\n",
        "            candidates.append(tag)\n",
        "    # 如果没有显式标识，就找 message-like 的容器（class 包含 message/msg/row）\n",
        "    if not candidates:\n",
        "        for tag in soup.find_all(True):\n",
        "            classes = [c.lower() for c in (tag.get('class') or [])]\n",
        "            if any(x in ' '.join(classes) for x in ['message','msg','row','bubble']):\n",
        "                candidates.append(tag)\n",
        "    # 如果还为空，退回到 body 下的直接块级子节点\n",
        "    if not candidates:\n",
        "        body = soup.body or soup\n",
        "        for tag in body.find_all(recursive=False):\n",
        "            candidates.append(tag)\n",
        "\n",
        "    # 对候选节点逐一判定是否为 assistant\n",
        "    for node in candidates:\n",
        "        text = node.get_text(separator=\"\\n\", strip=True)\n",
        "        if not text or len(text.strip())==0:\n",
        "            continue\n",
        "        # 检查祖先是否标记为 user（若是则跳过）\n",
        "        is_user_ancestor = False\n",
        "        anc = node\n",
        "        while anc.parent and anc.parent != soup:\n",
        "            anc = anc.parent\n",
        "            anc_classes = [c.lower() for c in (anc.get('class') or [])]\n",
        "            anc_id = (anc.get('id') or '').lower()\n",
        "            if any('user' in c or 'from-user' in c or 'you' in c for c in anc_classes) or 'user' in anc_id:\n",
        "                is_user_ancestor = True\n",
        "                break\n",
        "        if is_user_ancestor:\n",
        "            continue\n",
        "        classes = [c.lower() for c in (node.get('class') or [])]\n",
        "        node_id = (node.get('id') or '').lower()\n",
        "        inner_html = node.decode_contents()\n",
        "        # 判定规则（严格，只保留概率高的 assistant 段）\n",
        "        is_assistant = False\n",
        "        # 明确标识\n",
        "        if any('assistant' in c or 'chatgpt' in c or 'ChatGPT' in c for c in classes) or 'assistant' in node_id or 'ChatGPT' in node_id. or 'chatgpt' in node_id:\n",
        "            is_assistant = True\n",
        "        # 含有代码块、表格或 markdown code fence\n",
        "        elif node.find('pre') or node.find('code') or node.find('table') or '```' in inner_html:\n",
        "            is_assistant = True\n",
        "        # 文本足够长且不是典型问题（以 ? 结束且很短）\n",
        "        elif len(text) >= MIN_ASSISTANT_LENGTH and not (text.strip().endswith('?') and len(text) < 200):\n",
        "            is_assistant = True\n",
        "        # 明确以 \"assistant:\" / \"chatgpt:\" 前缀开始\n",
        "        elif text.strip().lower().startswith('assistant:') or text.strip().lower().startswith('chatgpt:'):\n",
        "            is_assistant = True\n",
        "        # 否则跳过（避免把用户短句 / 标题 /菜单等当作 assistant）\n",
        "        if not is_assistant:\n",
        "            continue\n",
        "        # sanitize fragment (去除 base64 图等，保留 code/table/links)\n",
        "        cleaned_html = sanitize_fragment(inner_html, remove_images=REMOVE_IMAGES)\n",
        "        # after sanitize, skip if nothing left\n",
        "        if not cleaned_html or len(BeautifulSoup(cleaned_html,\"html.parser\").get_text(strip=True)) < 20:\n",
        "            continue\n",
        "        messages.append({\"html\": cleaned_html, \"text\": node.get_text(separator=\"\\n\",strip=True)})\n",
        "    # 合并相邻相同来源（按出现顺序）\n",
        "    merged = []\n",
        "    for m in messages:\n",
        "        if merged and merged[-1].get('html') and merged[-1].get('text') and True:\n",
        "            # 合并文本和 html：插入分隔以保持段落\n",
        "            merged[-1]['html'] += \"\\n\\n\" + m['html']\n",
        "            merged[-1]['text'] += \"\\n\\n\" + m['text']\n",
        "        else:\n",
        "            merged.append(m)\n",
        "    # 去重并保留顺序（按 text）\n",
        "    seen = set()\n",
        "    unique = []\n",
        "    for m in merged:\n",
        "        t = m['text'].strip()\n",
        "        if t in seen:\n",
        "            continue\n",
        "        seen.add(t)\n",
        "        unique.append(m)\n",
        "    return unique\n",
        "\n",
        "# ---------------------------\n",
        "# 生成简洁的输出 HTML（ChatGPT 回复全屏显示）\n",
        "# ---------------------------\n",
        "def generate_output_html(messages, title=\"ChatGPT Replies (clean)\"):\n",
        "    header = f\"\"\"<!DOCTYPE html>\n",
        "<html lang=\"zh\">\n",
        "<head>\n",
        "<meta charset=\"utf-8\">\n",
        "<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\">\n",
        "<title>{html.escape(title)}</title>\n",
        "<style>\n",
        "body {{ font-family: Arial, sans-serif; margin:0; padding:20px; background:#f7f7f7; color:#111; }}\n",
        ".container {{ max-width:1200px; margin:0 auto; }}\n",
        ".header {{ margin-bottom:12px; }}\n",
        ".reply {{ width:100%; background:#fff; border-radius:10px; padding:18px; margin:14px 0; box-shadow:0 2px 8px rgba(0,0,0,0.06); line-height:1.6; word-wrap:break-word; }}\n",
        ".reply pre {{ background:#f4f4f4; padding:10px; border-radius:6px; overflow:auto; }}\n",
        ".reply table {{ border-collapse:collapse; width:100%; }}\n",
        ".reply table, .reply th, .reply td {{ border:1px solid #ddd; padding:6px; }}\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "<div class=\"container\">\n",
        "  <div class=\"header\"><h2>{html.escape(title)}</h2><div style=\"color:#666;font-size:13px\">生成时间: {datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%SZ\")}</div></div>\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    if not messages:\n",
        "        body += '<div style=\"color:#666\">未识别到 ChatGPT 回复（请检查输入文件或调整 MIN_ASSISTANT_LENGTH）。</div>\\n'\n",
        "    else:\n",
        "        for m in messages:\n",
        "            body += '<div class=\"reply\">\\n'\n",
        "            body += m['html']\n",
        "            body += '\\n</div>\\n'\n",
        "    footer = \"\"\"</div></body></html>\"\"\"\n",
        "    return header + body + footer\n",
        "\n",
        "# ---------------------------\n",
        "# 批量处理目录中的 HTML 文件\n",
        "# ---------------------------\n",
        "input_dir = Path(INPUT_FOLDER)\n",
        "output_dir = Path(OUTPUT_FOLDER)\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "files_processed = 0\n",
        "for p in sorted(input_dir.glob(\"*.html\")):\n",
        "    try:\n",
        "        print(\"处理：\", p.name)\n",
        "        txt, enc = read_file_with_encoding_detection(str(p))\n",
        "        msgs = extract_chatgpt_messages(txt)\n",
        "        out_html = generate_output_html(msgs, title=f\"Cleaned - {p.name}\")\n",
        "        out_name = p.stem + \"_clean.html\"\n",
        "        out_path = output_dir / out_name\n",
        "        with open(out_path, \"w\", encoding=\"utf-8\") as fw:\n",
        "            fw.write(out_html)\n",
        "        print(\"已生成：\", out_path, \"| messages:\", len(msgs))\n",
        "        files_processed += 1\n",
        "    except Exception as e:\n",
        "        print(\"处理失败：\", p.name, e)\n",
        "\n",
        "if files_processed == 0:\n",
        "    print(\"没有处理到任何 .html 文件，检查 INPUT_FOLDER 路径是否正确。\")\n",
        "\n",
        "# ---------------------------\n",
        "# 打包并触发下载（zip）\n",
        "# ---------------------------\n",
        "zip_base = \"/content/chatgpt_clean_outputs\"\n",
        "shutil.make_archive(zip_base, 'zip', str(output_dir))\n",
        "zip_path = zip_base + \".zip\"\n",
        "print(\"生成 zip：\", zip_path, \"（开始下载）\")\n",
        "files.download(zip_path)"
      ],
      "metadata": {
        "id": "-jwT5FfZG9EA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "16ce1f99-8cf0-457f-8947-aaa905f4a7ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "处理： AWS_SAA_C03_4.html\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3546487727.py:227: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  <div class=\"header\"><h2>{html.escape(title)}</h2><div style=\"color:#666;font-size:13px\">生成时间: {datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%SZ\")}</div></div>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "已生成： /content/chatgpt_html_clean_outputs/AWS_SAA_C03_4_clean.html | messages: 1\n",
            "处理： 解释下文意思.html\n",
            "已生成： /content/chatgpt_html_clean_outputs/解释下文意思_clean.html | messages: 1\n",
            "生成 zip： /content/chatgpt_clean_outputs.zip （开始下载）\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6e94198b-5232-4d14-abb8-2780f23e5772\", \"chatgpt_clean_outputs.zip\", 1785038)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 精简且稳定的 Colab 脚本：只保留 ChatGPT 回复（全屏），去除「您说：」及垃圾内容\n",
        "# 直接复制到 Google Colab 执行\n",
        "# ---------------------------\n",
        "\n",
        "# 安装依赖（Colab）\n",
        "!pip install -q chardet beautifulsoup4\n",
        "\n",
        "# ---------------------------\n",
        "# 导入\n",
        "# ---------------------------\n",
        "import os, re, shutil\n",
        "import chardet\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "import html\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from google.colab import drive, files\n",
        "\n",
        "# ---------------------------\n",
        "# 配置（请按需修改）\n",
        "# ---------------------------\n",
        "INPUT_FOLDER = \"/content/drive/MyDrive/chatgpt_exports\"   # <- 修改为你 Drive 的输入文件夹\n",
        "OUTPUT_FOLDER = \"/content/chatgpt_html_clean_outputs\"     # 输出到此临时目录（会被打包并下载）\n",
        "REMOVE_IMAGES = True   # 是否删除所有图片（True 可显著减小体积）\n",
        "MIN_ASSISTANT_LENGTH = 20  # 判定为 assistant 的最小纯文本长度阈值\n",
        "\n",
        "# ---------------------------\n",
        "# 挂载 Google Drive\n",
        "# ---------------------------\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "# ---------------------------\n",
        "# 读取并检测编码（并修剪开头垃圾）\n",
        "# ---------------------------\n",
        "def read_file_with_encoding_detection(path):\n",
        "    with open(path, \"rb\") as f:\n",
        "        raw = f.read()\n",
        "    res = chardet.detect(raw)\n",
        "    enc = res.get(\"encoding\") or \"utf-8\"\n",
        "    try:\n",
        "        text = raw.decode(enc, errors=\"ignore\")\n",
        "    except Exception:\n",
        "        text = raw.decode(\"utf-8\", errors=\"ignore\")\n",
        "    # 去掉开头的非 HTML 元数据 / 垃圾\n",
        "    first_lt = text.find('<')\n",
        "    if 0 <= first_lt < 2000:\n",
        "        text = text[first_lt:]\n",
        "    # 去掉末尾可能的控制字符\n",
        "    text = text.strip('\\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\x0b\\x0c\\x0e\\x0f')\n",
        "    return text, enc\n",
        "\n",
        "# ---------------------------\n",
        "# HTML 片段安全清洗\n",
        "# ---------------------------\n",
        "def sanitize_fragment(fragment_html, remove_images=True):\n",
        "    allowed_tags = {\n",
        "        'p':[], 'br':[], 'pre':[], 'code':[], 'blockquote':[],\n",
        "        'ul':[], 'ol':[], 'li':[], 'strong':[], 'b':[], 'em':[], 'i':[],\n",
        "        'a':['href','title','target','rel'], 'table':[], 'thead':[], 'tbody':[],\n",
        "        'tr':[], 'th':[], 'td':[]\n",
        "    }\n",
        "    soup = BeautifulSoup(fragment_html, \"html.parser\")\n",
        "    for s in soup(['script','style']):\n",
        "        s.decompose()\n",
        "    for c in soup.find_all(string=lambda text:isinstance(text, Comment)):\n",
        "        c.extract()\n",
        "    for img in soup.find_all('img'):\n",
        "        src = (img.get('src') or '').strip()\n",
        "        if remove_images or src.lower().startswith('data:'):\n",
        "            img.decompose()\n",
        "        else:\n",
        "            img_attrs = {}\n",
        "            if src:\n",
        "                img_attrs['src'] = src\n",
        "            if img.get('alt'):\n",
        "                img_attrs['alt'] = img.get('alt')\n",
        "            img_attrs['loading'] = 'lazy'\n",
        "            img.attrs = img_attrs\n",
        "    for tag in list(soup.find_all(True)):\n",
        "        if tag.name not in allowed_tags:\n",
        "            tag.unwrap()\n",
        "        else:\n",
        "            keep = allowed_tags.get(tag.name, [])\n",
        "            attrs = dict(tag.attrs)\n",
        "            for k in list(attrs.keys()):\n",
        "                if k not in keep:\n",
        "                    del tag.attrs[k]\n",
        "            if tag.name == 'a' and tag.has_attr('href'):\n",
        "                href = tag['href'].strip()\n",
        "                if href.lower().startswith('javascript:') or href.lower().startswith('data:'):\n",
        "                    del tag.attrs['href']\n",
        "                else:\n",
        "                    tag.attrs['href'] = href\n",
        "                    tag.attrs['target'] = '_blank'\n",
        "                    tag.attrs['rel'] = 'noopener noreferrer'\n",
        "    cleaned = str(soup)\n",
        "    cleaned = re.sub(r'\\n{3,}', '\\n\\n', cleaned)\n",
        "    return cleaned\n",
        "\n",
        "# ---------------------------\n",
        "# 提取 ChatGPT 段落（只保留 \"ChatGPT 说：\" 内容）\n",
        "# ---------------------------\n",
        "def extract_chatgpt_messages(html_text):\n",
        "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
        "    for s in soup(['script','style']):\n",
        "        s.decompose()\n",
        "    for c in soup.find_all(string=lambda text:isinstance(text, Comment)):\n",
        "        c.extract()\n",
        "    messages = []\n",
        "    for node in soup.find_all(True):\n",
        "        text = node.get_text(separator=\"\\n\", strip=True)\n",
        "        if not text:\n",
        "            continue\n",
        "        # 去掉用户输入\n",
        "        if text.startswith(\"您说：\"):\n",
        "            continue\n",
        "        # 只保留 ChatGPT 回复\n",
        "        if text.startswith(\"ChatGPT 说：\"):\n",
        "            text_clean = text.replace(\"ChatGPT 说：\", \"\", 1).strip()\n",
        "            if len(text_clean) < MIN_ASSISTANT_LENGTH:\n",
        "                continue\n",
        "            inner_html = node.decode_contents()\n",
        "            cleaned_html = sanitize_fragment(inner_html, remove_images=REMOVE_IMAGES)\n",
        "            messages.append({\"html\": cleaned_html, \"text\": text_clean})\n",
        "    return messages\n",
        "\n",
        "# ---------------------------\n",
        "# 生成输出 HTML\n",
        "# ---------------------------\n",
        "def generate_output_html(messages, title=\"ChatGPT Replies (clean)\"):\n",
        "    header = f\"\"\"<!DOCTYPE html>\n",
        "<html lang=\"zh\">\n",
        "<head>\n",
        "<meta charset=\"utf-8\">\n",
        "<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\">\n",
        "<title>{html.escape(title)}</title>\n",
        "<style>\n",
        "body {{ font-family: Arial, sans-serif; margin:0; padding:20px; background:#f7f7f7; color:#111; }}\n",
        ".container {{ max-width:1200px; margin:0 auto; }}\n",
        ".header {{ margin-bottom:12px; }}\n",
        ".reply {{ width:100%; background:#fff; border-radius:10px; padding:18px; margin:14px 0; box-shadow:0 2px 8px rgba(0,0,0,0.06); line-height:1.6; word-wrap:break-word; }}\n",
        ".reply pre {{ background:#f4f4f4; padding:10px; border-radius:6px; overflow:auto; }}\n",
        ".reply table {{ border-collapse:collapse; width:100%; }}\n",
        ".reply table, .reply th, .reply td {{ border:1px solid #ddd; padding:6px; }}\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "<div class=\"container\">\n",
        "  <div class=\"header\"><h2>{html.escape(title)}</h2><div style=\"color:#666;font-size:13px\">生成时间: {datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%SZ\")}</div></div>\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    if not messages:\n",
        "        body += '<div style=\"color:#666\">未识别到 ChatGPT 回复。</div>\\n'\n",
        "    else:\n",
        "        for m in messages:\n",
        "            body += '<div class=\"reply\">\\n'\n",
        "            body += m['html']\n",
        "            body += '\\n</div>\\n'\n",
        "    footer = \"</div></body></html>\"\n",
        "    return header + body + footer\n",
        "\n",
        "# ---------------------------\n",
        "# 批量处理目录中的 HTML 文件\n",
        "# ---------------------------\n",
        "input_dir = Path(INPUT_FOLDER)\n",
        "output_dir = Path(OUTPUT_FOLDER)\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "files_processed = 0\n",
        "for p in sorted(input_dir.glob(\"*.html\")):\n",
        "    try:\n",
        "        print(\"处理：\", p.name)\n",
        "        txt, enc = read_file_with_encoding_detection(str(p))\n",
        "        msgs = extract_chatgpt_messages(txt)\n",
        "        out_html = generate_output_html(msgs, title=f\"Cleaned - {p.name}\")\n",
        "        out_name = p.stem + \"_clean.html\"\n",
        "        out_path = output_dir / out_name\n",
        "        with open(out_path, \"w\", encoding=\"utf-8\") as fw:\n",
        "            fw.write(out_html)\n",
        "        print(\"已生成：\", out_path, \"| messages:\", len(msgs))\n",
        "        files_processed += 1\n",
        "    except Exception as e:\n",
        "        print(\"处理失败：\", p.name, e)\n",
        "\n",
        "if files_processed == 0:\n",
        "    print(\"没有处理到任何 .html 文件，检查 INPUT_FOLDER 路径是否正确。\")\n",
        "\n",
        "# ---------------------------\n",
        "# 打包并触发下载（zip）\n",
        "# ---------------------------\n",
        "zip_base = \"/content/chatgpt_clean_outputs\"\n",
        "shutil.make_archive(zip_base, 'zip', str(output_dir))\n",
        "zip_path = zip_base + \".zip\"\n",
        "print(\"生成 zip：\", zip_path, \"（开始下载）\")\n",
        "files.download(zip_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "iIVpCaFvJigd",
        "outputId": "e9aee7fc-d19d-43a2-824a-1b9be60ff944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "处理： AWS_SAA_C03_4.html\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1436145591.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  <div class=\"header\"><h2>{html.escape(title)}</h2><div style=\"color:#666;font-size:13px\">生成时间: {datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%SZ\")}</div></div>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "已生成： /content/chatgpt_html_clean_outputs/AWS_SAA_C03_4_clean.html | messages: 0\n",
            "处理： 解释下文意思.html\n",
            "已生成： /content/chatgpt_html_clean_outputs/解释下文意思_clean.html | messages: 386\n",
            "生成 zip： /content/chatgpt_clean_outputs.zip （开始下载）\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e97b83d7-abc8-44e9-bb1b-d26ddddec5a4\", \"chatgpt_clean_outputs.zip\", 258574)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 精简且稳定的 Colab 脚本：支持中/日/英三语言界面\n",
        "# ---------------------------\n",
        "\n",
        "!pip install -q chardet beautifulsoup4\n",
        "\n",
        "import os, re, shutil\n",
        "import chardet\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "import html\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from google.colab import drive, files\n",
        "\n",
        "# ---------------------------\n",
        "# 配置\n",
        "# ---------------------------\n",
        "INPUT_FOLDER = \"/content/drive/MyDrive/chatgpt_exports\"\n",
        "OUTPUT_FOLDER = \"/content/chatgpt_html_clean_outputs\"\n",
        "REMOVE_IMAGES = True\n",
        "MIN_ASSISTANT_LENGTH = 20\n",
        "\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "# ---------------------------\n",
        "# 读取文件并检测编码\n",
        "# ---------------------------\n",
        "def read_file_with_encoding_detection(path):\n",
        "    with open(path, \"rb\") as f:\n",
        "        raw = f.read()\n",
        "    res = chardet.detect(raw)\n",
        "    enc = res.get(\"encoding\") or \"utf-8\"\n",
        "    try:\n",
        "        text = raw.decode(enc, errors=\"ignore\")\n",
        "    except Exception:\n",
        "        text = raw.decode(\"utf-8\", errors=\"ignore\")\n",
        "    first_lt = text.find('<')\n",
        "    if 0 <= first_lt < 2000:\n",
        "        text = text[first_lt:]\n",
        "    text = text.strip('\\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\x0b\\x0c\\x0e\\x0f')\n",
        "    return text, enc\n",
        "\n",
        "# ---------------------------\n",
        "# HTML 片段安全清洗\n",
        "# ---------------------------\n",
        "def sanitize_fragment(fragment_html, remove_images=True):\n",
        "    allowed_tags = {\n",
        "        'p':[], 'br':[], 'pre':[], 'code':[], 'blockquote':[],\n",
        "        'ul':[], 'ol':[], 'li':[], 'strong':[], 'b':[], 'em':[], 'i':[],\n",
        "        'a':['href','title','target','rel'], 'table':[], 'thead':[], 'tbody':[],\n",
        "        'tr':[], 'th':[], 'td':[]\n",
        "    }\n",
        "    soup = BeautifulSoup(fragment_html, \"html.parser\")\n",
        "    for s in soup(['script','style']):\n",
        "        s.decompose()\n",
        "    for c in soup.find_all(string=lambda text:isinstance(text, Comment)):\n",
        "        c.extract()\n",
        "    for img in soup.find_all('img'):\n",
        "        src = (img.get('src') or '').strip()\n",
        "        if remove_images or src.lower().startswith('data:'):\n",
        "            img.decompose()\n",
        "        else:\n",
        "            img_attrs = {}\n",
        "            if src:\n",
        "                img_attrs['src'] = src\n",
        "            if img.get('alt'):\n",
        "                img_attrs['alt'] = img.get('alt')\n",
        "            img_attrs['loading'] = 'lazy'\n",
        "            img.attrs = img_attrs\n",
        "    for tag in list(soup.find_all(True)):\n",
        "        if tag.name not in allowed_tags:\n",
        "            tag.unwrap()\n",
        "        else:\n",
        "            keep = allowed_tags.get(tag.name, [])\n",
        "            attrs = dict(tag.attrs)\n",
        "            for k in list(attrs.keys()):\n",
        "                if k not in keep:\n",
        "                    del tag.attrs[k]\n",
        "            if tag.name == 'a' and tag.has_attr('href'):\n",
        "                href = tag['href'].strip()\n",
        "                if href.lower().startswith('javascript:') or href.lower().startswith('data:'):\n",
        "                    del tag.attrs['href']\n",
        "                else:\n",
        "                    tag.attrs['href'] = href\n",
        "                    tag.attrs['target'] = '_blank'\n",
        "                    tag.attrs['rel'] = 'noopener noreferrer'\n",
        "    cleaned = str(soup)\n",
        "    cleaned = re.sub(r'\\n{3,}', '\\n\\n', cleaned)\n",
        "    return cleaned\n",
        "\n",
        "# ---------------------------\n",
        "# 提取 ChatGPT 段落（支持中/日/英）\n",
        "# ---------------------------\n",
        "def extract_chatgpt_messages(html_text):\n",
        "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
        "    for s in soup(['script','style']):\n",
        "        s.decompose()\n",
        "    for c in soup.find_all(string=lambda text:isinstance(text, Comment)):\n",
        "        c.extract()\n",
        "\n",
        "    messages = []\n",
        "    # 多语言匹配前缀\n",
        "    USER_PREFIXES = [\"您说：\", \"あなた:\", \"You:\"]\n",
        "    GPT_PREFIXES = [\"ChatGPT 说：\", \"ChatGPT:\", \"ChatGPT :\"]\n",
        "\n",
        "    for node in soup.find_all(True):\n",
        "        text = node.get_text(separator=\"\\n\", strip=True)\n",
        "        if not text:\n",
        "            continue\n",
        "        # 跳过用户输入（任一语言）\n",
        "        if any(text.startswith(prefix) for prefix in USER_PREFIXES):\n",
        "            continue\n",
        "        # 仅保留 ChatGPT 回复（任一语言）\n",
        "        for prefix in GPT_PREFIXES:\n",
        "            if text.startswith(prefix):\n",
        "                text_clean = text.replace(prefix, \"\", 1).strip()\n",
        "                if len(text_clean) < MIN_ASSISTANT_LENGTH:\n",
        "                    continue\n",
        "                inner_html = node.decode_contents()\n",
        "                cleaned_html = sanitize_fragment(inner_html, remove_images=REMOVE_IMAGES)\n",
        "                messages.append({\"html\": cleaned_html, \"text\": text_clean})\n",
        "                break  # 找到匹配即退出循环\n",
        "    return messages\n",
        "\n",
        "# ---------------------------\n",
        "# 生成输出 HTML\n",
        "# ---------------------------\n",
        "def generate_output_html(messages, title=\"ChatGPT Replies (clean)\"):\n",
        "    header = f\"\"\"<!DOCTYPE html>\n",
        "<html lang=\"zh\">\n",
        "<head>\n",
        "<meta charset=\"utf-8\">\n",
        "<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\">\n",
        "<title>{html.escape(title)}</title>\n",
        "<style>\n",
        "body {{ font-family: Arial, sans-serif; margin:0; padding:20px; background:#f7f7f7; color:#111; }}\n",
        ".container {{ max-width:1200px; margin:0 auto; }}\n",
        ".header {{ margin-bottom:12px; }}\n",
        ".reply {{ width:100%; background:#fff; border-radius:10px; padding:18px; margin:14px 0; box-shadow:0 2px 8px rgba(0,0,0,0.06); line-height:1.6; word-wrap:break-word; }}\n",
        ".reply pre {{ background:#f4f4f4; padding:10px; border-radius:6px; overflow:auto; }}\n",
        ".reply table {{ border-collapse:collapse; width:100%; }}\n",
        ".reply table, .reply th, .reply td {{ border:1px solid #ddd; padding:6px; }}\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "<div class=\"container\">\n",
        "  <div class=\"header\"><h2>{html.escape(title)}</h2><div style=\"color:#666;font-size:13px\">生成时间: {datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%SZ\")}</div></div>\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    if not messages:\n",
        "        body += '<div style=\"color:#666\">未识别到 ChatGPT 回复。</div>\\n'\n",
        "    else:\n",
        "        for m in messages:\n",
        "            body += '<div class=\"reply\">\\n'\n",
        "            body += m['html']\n",
        "            body += '\\n</div>\\n'\n",
        "    footer = \"</div></body></html>\"\n",
        "    return header + body + footer\n",
        "\n",
        "# ---------------------------\n",
        "# 批量处理目录中的 HTML 文件\n",
        "# ---------------------------\n",
        "input_dir = Path(INPUT_FOLDER)\n",
        "output_dir = Path(OUTPUT_FOLDER)\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "files_processed = 0\n",
        "for p in sorted(input_dir.glob(\"*.html\")):\n",
        "    try:\n",
        "        print(\"处理：\", p.name)\n",
        "        txt, enc = read_file_with_encoding_detection(str(p))\n",
        "        msgs = extract_chatgpt_messages(txt)\n",
        "        out_html = generate_output_html(msgs, title=f\"Cleaned - {p.name}\")\n",
        "        out_name = p.stem + \"_clean.html\"\n",
        "        out_path = output_dir / out_name\n",
        "        with open(out_path, \"w\", encoding=\"utf-8\") as fw:\n",
        "            fw.write(out_html)\n",
        "        print(\"已生成：\", out_path, \"| messages:\", len(msgs))\n",
        "        files_processed += 1\n",
        "    except Exception as e:\n",
        "        print(\"处理失败：\", p.name, e)\n",
        "\n",
        "if files_processed == 0:\n",
        "    print(\"没有处理到任何 .html 文件，检查 INPUT_FOLDER 路径是否正确。\")\n",
        "\n",
        "# ---------------------------\n",
        "# 打包并下载 zip\n",
        "# ---------------------------\n",
        "zip_base = \"/content/chatgpt_clean_outputs\"\n",
        "shutil.make_archive(zip_base, 'zip', str(output_dir))\n",
        "zip_path = zip_base + \".zip\"\n",
        "print(\"生成 zip：\", zip_path, \"（开始下载）\")\n",
        "files.download(zip_path)"
      ],
      "metadata": {
        "id": "1dncTQQyQtxF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "19af5daf-7d95-4bcc-e40b-c4544c4d9ed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "处理： AWS_SAA_C03_4.html\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-758364400.py:148: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  <div class=\"header\"><h2>{html.escape(title)}</h2><div style=\"color:#666;font-size:13px\">生成时间: {datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%SZ\")}</div></div>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "已生成： /content/chatgpt_html_clean_outputs/AWS_SAA_C03_4_clean.html | messages: 78\n",
            "处理： 解释下文意思.html\n",
            "已生成： /content/chatgpt_html_clean_outputs/解释下文意思_clean.html | messages: 386\n",
            "生成 zip： /content/chatgpt_clean_outputs.zip （开始下载）\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c57e67c0-64ca-4fa5-a9c2-7a094565ae79\", \"chatgpt_clean_outputs.zip\", 412212)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}
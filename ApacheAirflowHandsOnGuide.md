### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/01_Udemy - Apache Airflow The Hands-On Guide p01 1. Important Prerequisites.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


你好，欢迎来到Apache Airflow
实践指南 我叫Marmi
我非常兴奋见到你们
为什么，因为你们正开始一段创造可靠
强大和惊人的数据管道的奇妙旅程
确实，在本课程中，你将发现所有必要的功能
以便最终从您的数据中获得见解
并使用Airflow将所有工具编排在一起
但是事不宜迟
我需要给你一些重要的前情提要
首先，你需要有一些编程经验
Python编程经验
Airflow的所有内容都是用Python编写的
如果你不知道Python
你将无法跟随这门课程
所以请确保你了解Python
然后你必须将Docker安装在你的计算机上
为什么，因为我需要了解你的环境
而我显然不知道你的环境安装了什么
所以我们将使用Docker
我们将有一个虚拟机在你的计算机上运行
我可以确保我们共享相同的环境
所以我可以确保你在开始这门课程之前
将Docker安装并运行在你的计算机上
这非常重要
就是这样
这就是所有先决条件 让我们继续 前进
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/02_Udemy - Apache Airflow The Hands-On Guide p02 2. The Roadmap.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在开始课程之前
让我先向你展示你的学习路径图
你将经历的所有步骤来掌握 airflow
显然，第一步是掌握基础知识
你将发现 airflow 的主要组件
airflow 的主要概念
什么是 airflow
什么是不是 airflow
还有很多，这非常重要
因为这一步之后，你将创建你的第一个数据管道
在这个数据平面中，你将与不同的工具进行交互
你将看到如何使用Airflow创建一个可靠的数据管道
相信我 这将有点具有挑战性
但你会非常喜欢这个过程
在子步骤中，你将掌握Airflow的高级功能
例如数据集
动态任务映射
如果你想动态创建任务
如何将你的任务分组
使用任务组 如何更好地组织你的数据计划
还有很多更多，你将会发现那些特性是什么
如何混合它们
更重要的是，何时使用这些特性
然后你将看到如何扩展Airflow
如何使用不同的执行者在不同的机器上分配工作
如果你不知道我在说什么
那很正常 你将在课程中发现
最后 但并非不重要 如何运行Airin生产
如何监控
例如 关于安全性以及所有事情，确保您有一个准备就绪的生产环境
最后，我想告诉你，在这个课程的每一步
你将始终有一个介绍
然后是如何运作
然后是实践，最后是测验
我想确保你知道这个部分为什么在这里
学习目标是什么
然后是理论 在将概念应用于现实世界之前，了解概念是很重要的
最后，你可以通过测验评估你的技能
所以希望你们会感到兴奋 在下个视频中再见
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/03_Udemy - Apache Airflow The Hands-On Guide p03 3. Who I am.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


快速介绍我自己
我叫mori 我住在一个充满问题的地方
一个位于无何有国的小岛上
我也是法国人，你可以从我的口音中听出来
我也有数据工程的背景
我每天都在创建管道
我是宇航员客户教育的负责人
如果你不知道那是什么 那是在云中运行airflow的最佳场所
尤其是在你想要在生产环境中运行airflow时
并且你不想处理细节时
你知道如何设置enflow等
Astronomer会帮助你
然后最后 但并非最不重要
我是udemy上airflow的最佳销售讲师
拥有超过六万名学生
我很高兴看到你是其中之一
我希望你会享受学习体验
如果你真地想享受这个学习体验，
与我联系， 你可以在领英上联系我，
给我发邀请， 我很乐意与你联系，我在那里每天发布关于气流的小贴士，
如果你想观看关于气流和其他主题的视频，
你可以查看我的YouTube频道，
最后， 但不最不重要的是， 我有一个发布教程的网站，
所以暂停视频
给我发个邀请
我很乐意与你联系 下次视频见
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/04_Udemy - Apache Airflow The Hands-On Guide p04 4. Development Environment.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 你将发现开发环境，我们将使用这个环境来实验
Airflow并构建我们的第一个数据管道
所以不再多说
让我们开始吧 第一步是在您的计算机上安装Docker
如果您不知道Docker是什么
可以将其视为一种在容器中运行应用程序的方式
容器就像一个微小的虚拟机
它有助于运行应用程序
无论你已经在你的操作系统上安装了什么
在库，依赖项等方面
为了避免任何冲突
所以去docker com
然后点击下载你的操作系统
Mac windows linux
然后一旦你下载了docker桌面
打开它
你应该能够访问这个docker桌面窗口 然后你应该能够访问这个docker桌面窗口
基本上，如果你使用的是mac os
你应该能看到docker桌面或者寻找它
然后你会来到左边的这个窗口
你有容器
镜像，卷等等
如果你不知道它们是什么
现在还不用担心这些术语
好的 现在你有了桌面
下一步是去以下网站
两位天文学家
更具体地说，您正在寻找Astro CLI
Astro CLI是一个开源的命令行界面
你不必是天文学家才能使用它
但我们将使用CLI在本地设置和运行Airflow
有很多方法可以在本地运行Airflow
但说实话，Astro CLI是最简单和最快的方法
并提供所有最佳实践
您需正确设置您的工作环境
这就是为什么请访问我们的网站
然后点击 在左边安装cli
然后选择你的操作系统
取决于你使用的操作系统
这将会更容易或不容易
如果你是mac
这相当直接
你需要输入brew
安装一个真实的
就是这样 如果你使用的是Windows系统
你有两种不同的方法
如果你使用的是Linux系统
你可以按照这里的指示操作
安装命令行界面
然后，为了验证你已经成功安装了命令行界面
你需要输入 astro version
因此，打开终端并输入 astro version
如果你看到 astro ti version 后面跟着一些内容
这意味着你已经成功安装了 AO I
这就是开发环境的设置
稍后你将看到如何使用命令行界面设置并运行 Airflow 本地
正如你所看到的 这真的很棒
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/05_Udemy - Apache Airflow The Hands-On Guide p05 1. Why data orchestration.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


为什么在引入Apache Airflow之前你的数据需要编排
让我们从定义数据编排开始
以及为什么你的数据像Airflow一样对你很重要
数据编排的定义是协调和自动化数据在不同系统和流程中的运动、转换和集成过程
以确保高效的可靠的数据工作流
好的
这里有很多需要解析的内容 所以让我们从一个数据工作流的例子开始来说明这个定义
这里有四个任务可能是Python脚本来提取
提取、转换、加载和可视化数据
清洁 无需交谈即可加载和清理数据
你可能首先想到的问题是
我如何定义这些任务之间的依赖关系
我如何确保提取任务首先运行
然后进行清理 接着进行转换等
使用案例您可以做到这一点
您可以创建和管理任务之间的依赖关系
以确保执行顺序得到持续遵守
现在 你可以按照顺序执行你的任务
想象一下，清理任务
因为你设置了依赖关系
其余的任务将不会运行
这就是拥有一个数据调度器的美丽之处
这将自动为你处理
但让我们回到没有数据的清理任务
你将不得不自己检查不同的任务来确定哪一个失败了
一旦你确定了任务
你需要调试它
检查日志并手动再次运行
这不方便
尤其是如果失败发生在周末
或者如果你有数千个任务需要管理
而使用数据协调器，你会得到通知，任务会自动重试
好的 到目前为止，我们已经使用了一个简单的示例在现实生活中
你将不得不管理数千个任务和数百个数据管道
如果你试图在一个单机上手动运行和管理一切
这不仅将是一个噩梦
而且它无法扩展
你将需要计算资源
而且这并不可行
相反 使用像airflow这样的数据调度器
你可以轻松地使用多台机器在规模上管理数千个任务和数据工作流
除了这些原因之外，拥有一个数据调度器还可以帮助你避免精神崩溃
但请记住，它允许你编写 管理和监控数据工作流
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/06_Udemy - Apache Airflow The Hands-On Guide p06 2. Why Airflow.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 你将发现为什么airflow是数据操作者，我将给你五个原因
为什么airflow是数据操作者，你所需要的一切
所以让我们回到一个简单的例子 你有四个任务提取
清洁
转换和加载 首先你要做的就是实际上创建一些依赖关系
连接那些任务之间的点
为了做到这一点
正如你所知，你需要一个数据调度器
而这正是你需要一个数据调度器的真正第一个原因，甚至不是airflow
但只是一个数据操作者
现在，每个任务后面
你可能与一个外部工具或服务交互
例如airbyte用于提取数据
Dbt用于清洁和转换数据
然后snowflake作为一个数据仓库来存储你的数据
没有像airflow这样的数据调度器
你将不得不检查每个服务或工具
如何与其他一个集成
如何集成airbyte与dbt
然后dbt与snowflake等等
这将是一个噩梦来管理
记住，这是一个简单的例子，现实中
你可能有数十个服务和工具来管理
说到数十个服务和工具来管理
你可能要问下一个问题是它是否可扩展
如果我有一个任务或数千个任务要运行
我是否能够使用airflow
这是一个很好的问题
而且，猜猜看，airflow被许多大公司使用
例如airbnb Slack甚至沃尔玛
你可以想象他们每天运行多少任务和数据计划来管理他们的业务运营
airflow可扩展的一个原因是它可以在流行的框架上运行
例如salary，kubernetes或dask以在多个机器上并行运行你的任务
最后，但不是最不重要的，airflow有一个组件
调度器，它负责监控和触发任务，没有调度器
你不能使用airflow运行任务，任务越多，工作量越大
但你可以有多个调度器来分配工作量
这可以增加你可以执行的任务数量
以及你的空气流实例的可靠性
例如，如果其中一个调度器失败，这将是一个很大的问题
这就是为什么airflow通常部署在多个节点上
每个节点运行一个调度器，这样可以提高系统的可靠性
这就是为什么airflow通常被称为数据操作者
因为它可以处理大量的任务和数据 这就是为什么airflow在数据操作领域非常流行
因为它可以处理大量的任务和数据 但你仍然有其他任务需要运行
这在生产环境中非常重要
总结来说 Airflow的第一个原因是你可以可靠地编排你的数据工作流
你的数据管道可扩展
接下来第二个原因
让我给你展示这个疯狂的景观
我必须说名字非常适合它
因为它令人震惊
这很吓人
但这是你职业生涯中可能会接触到的工具和服务的数量
这真是太疯狂了
但最重要的一点是
你需要确保你的数据能够与这些工具进行交互
可能不是所有工具
但肯定是大部分
嗯 Airflow有很多集成
让我来向你展示一个我非常喜欢的网站，这个网站是注册表
你不必是天文学的客户才能使用它
但是，我之所以要展示注册表，是因为你可以看到Airflow集成了多少
集成
例如，你可以与Byte High、HDFS等交互
亚马逊等
这样你就不必重新发明轮子了
以便与那些工具和服务进行交互
但现在你可能会想，嘿
如果我想要与列表中没有的工具或服务进行交互怎么办
你可以创建自己的集成
你自己的集成
您自己的插件
就是这样 这就是Airflow的魅力所在
您永远不会感到被限制
您可以创建自己的自定义集成
将其放入您的空气中
例如 并立即从您的数据管道中与您的工具或服务进行交互
实际上 Airflow的模块化特性是一个巨大的优势
当你安装airflow时
你也安装了car包
这是一个python包
然后如果你想要与其他工具或服务进行交互
例如databricks
soda 亚马逊或snowflake
你只需要安装相应的提供商
提供商是一个可以在你的空气流安装之上添加的python包
就这样你就能获得更多的功能
所以空气流动的第二个原因是
集成的数量和您需要与服务或工具交互的定制化水平
您需要从您的数据管道中
说到数据管道
我们还没有触及它们
所以首先要了解的是，一切都是用Airflow编写的Python
Python是一种广泛使用且强大的编程语言
只要你知道Python
你就可以创建Airflow中的数据管道
让我来向你展示如何从零开始创建一条管道
首先，你会导入所需的内容
以构建你的管道
然后，你会定义你的DAG
你的数据管道
稍后你在课程中会了解更多关于DAG的含义
但此处DAG定义有一些关于您的数据线的设置
例如，它应该在何时开始
它应该多久运行一次
以及更多
之后，你会定义任务
这里我们有三个任务 提取、转换和加载
最后，你可以定义这些任务之间的依赖关系
就是这样，没什么难的
对吧 我的意思是 这是每次你创建Airflow中的数据管道时都会遵循的一般模板
另一个酷的地方是，既然我们用Python创建这些数据管道，嗯
Python足够强大和灵活
所以你可以动态创建任务
例如 假设你从一个S3桶或本地目录获取文件
你想要为每个文件创建一个任务
但你不知道提前
你将获得多少文件，嗯
这在Airflow中是可以做到的
即使你不知道输入，
你还是可以在你的数据管道执行时创建任务
但你会在课程中看到那部分
这是一个非常强大的功能
所以Airflow的第三个原因是它的Python可访问性和动态性
这展示了你的数据管道可以有多么强大和灵活
好的 创建数据管道很好
但监控它们并在出错时反应更好
而且任务在周末或晚上失败是很常见的
所以你需要确保您的数据调度器可以正确处理这一点
除了报警和重试机制之外
这里有一个我想强调的功能在现实世界中
您的简单数据管道可能具有不那么简单的依赖性
实际上，你可能有多个数据管道，它们相互依赖
由不同的团队拥有，这些输出将在您的数据管道中使用
问题是，你可以有一个失败的任务
实际上，这是由于一个非常上游的任务存在错误引起的
但是看看你的管道
你不知道这是为什么
这就是你需要数据血缘关系的地方
数据血缘关系是一种追踪您数据集之间复杂关系的方式
以及您的数据生态系统
这样如果出了什么问题
我们的数据集已被更新
你就会知道为什么
以及如何何时
这就像您的工具和数据集的地图
Airflow提供了一个内置的开放年龄集成
您可以使用Marif工具获取此地图
如果您不知道开放年龄或Mar
去看看
非常有用
第四个原因是监控和数据血缘能力在生产中非常有用
最后，Airflow有一个庞大的社区，正如你所看到的
它在Slack上有超过三万名用户
超过两千名贡献者正在努力工作
实际上 如果您看一下仓库
您会看到许多提交
许多拉取请求
问题和等等
这绝对是一个活跃的项目
一个非常活跃的项目
它正在快速增长
几乎每个月都有一个新的发布
并且您知道每当您使用数据时
您想确保在不到一年的时间内
它不会过时
您还希望确保每当您有问题或疑问时
嗯 您至少有一些支持来帮助您
这就是Airflow的美丽之处
Airflow有一个庞大的社区和支持
这就是这个视频的全部内容 现在 您知道为什么Airflow是您最好的调度器
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/07_Udemy - Apache Airflow The Hands-On Guide p07 3. The Core Components.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 你将发现 airflow 的核心组件
让我们开始吧，当你第一次启动 airflow 时
你会到达用户界面
而这个用户界面实际上是由 Web 服务器驱动的
所以 Web 服务器是 airflow 的第一个核心组件
然后你在用户界面上看到的数据
例如 不同的数据管道是我们所说的元数据
而元数据存储在元数据库中
这个数据库可以是mysql
Postgres 等等
在右边的用户界面上放大
已经成功执行的任务或正在运行的任务
这是由调度程序管理的
因为它负责管理和监控任务集
因此，您的数据管道
所以，照顾好它是非常重要的
调度器的一部分
你有执行者
你不会把它当作一个标准的加载部件
就像其他组件一样
但它存在
它定义了如何以及在哪个系统上执行你的任务
它将你的任务推送到一个队列中
如果你想在你的任务上运行kubernetes ities
你有Kubernetes执行者
如果你想在你的服务器上运行你的任务
你有服务器执行者
如果你想在你的单台机器上运行你的任务
你有本地执行者等等
但你稍后会在本课程中看到
最后还有Walker
Walker可以是子进程或专用机器
负责从队列中拉出任务并执行这些任务
总之
你负责管理Web服务器，用于无服务器提供用户界面，如果没有Web服务器，
你就无法访问用户界面，
你有调度器，调度任务，
在检查依赖项是否满足的同时，
它监控和管理你的任务，
然后你有一个元数据库，包含Airflow元数据，
例如， 你的任务实例或数据管道的状态，
执行者定义如何以及在哪个系统上执行你的任务，
但它不会执行你的任务
我们没有讨论的触发器
但它用于特殊类型的任务，你将在课程后期看到
最后一个是步行者
步行者负责执行你的任务
但如果你在单台机器上运行 airflow，你通常不会看到它
但如果你将 airflow 分布到多台机器上，你将会看到它
关于这一点，课程后期会详细介绍 这就是你需要了解的所有 airflow 组件
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/08_Udemy - Apache Airflow The Hands-On Guide p08 4. The Core Concepts.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


是时候发现空气流动的核心概念了
这样你就知道如何开始
第一个概念是任务
任务是Airflow中的基本执行单元
要创建一个任务
你将实际使用操作员
你可以将其视为预定义的任务模板
例如 你想在Postgres上运行SQL请求
你将使用Postgres操作员
你想执行bash命令
你将使用bash操作员
你想执行Python函数
你将使用Python操作员，等等
Airflow为你提供了许多操作员
这样你就不必重新发明轮子
每次你想在你的数据管道中做某事时
通常你会创建多个任务
一旦你有两个任务
大多数时候你想创建一个依赖关系
你想确保第一个任务被很好地
首先执行，然后第二个
最后，如果你结合了任务的概念
操作员和依赖关系
你得到一个任务
有向无环图
在Airflow的背景下，这只是一个数据管道
有向意味着你可以根据依赖关系看到执行顺序
无环意味着你的无向图中没有循环
如图中的红色箭头所示
这在Airflow中是不可能的
否则它就不再是无环的了
并且图形很好
这就是你看到的
图形由节点和边组成
边是依赖关系
节点是任务
如果你想知道DAG在Airflow中的样子
这是一个例子
你看到的这是一个数据管道
在Airflow的用户界面中
有三个任务提取
加载和转换
在右边，这就是如何构建的
你将在本课程中构建一个数据管道
这就是Airflow的概念
记住 任务是Airflow中的基本执行单元
由操作员构建
操作员是预定义的任务模板
你想与python进行交互
你将使用python运算符
并且一个dag由具有依赖性的任务组成 并且一个dag对应于一个数据管道或工作流
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/09_Udemy - Apache Airflow The Hands-On Guide p09 5. How does Airflow work.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 你将发现当你创建一个dag并在airflow中运行它的时候会发生什么
首先你可以看到四个核心组件，就是我们之前讨论过的web服务器
调度器 元数据库
和执行者
即使执行者不在调度器之外
在这个例子中 记住它是调度器的一部分
这只是为了简单起见
我们有dag目录，用于添加dag文件
好的 假设你刚刚创建了一个新的数据管道
我的dag.py
这是一个python文件
你想要安排它
所以第一步是将其添加到dags目录中
接下来 airflow调度程序每五分钟扫描dag目录以查找新文件
默认情况下
调度程序检测到新的dac文件并将其序列化到元数据库中
这样web服务器及其他airflow组件可以访问它
如果你不知道civilization是什么意思
只需记住这基本上是你的dag的代码
存储在元数据库中
airflow就是这样做的
因为它比从本地目录读取更快更容易
然后 调度程序检查你的dag是否准备好运行
基于你将会看到的特定设置
如果是，它创建一个dag实例
这是你dag的一个特定时间的实例它有不同的状态，这里是运行状态
然后调度程序检查是否有任务需要运行
如果是，它为每个任务创建一个任务实例
就像在图表中一样
一个任务实例有许多不同的状态，这里是预定状态
然后调度程序将任务实例发送给执行者
记住，执行者负责定义如何
以及在哪个系统上执行你的任务
它不会执行你的任务，而是为你的任务准备执行
执行者将你的任务实例推入队列
这个队列你看不到
但它存在，允许按顺序执行你的任务
你将会看到，这个队列可以是一个单独的组件
这取决于你的空气流实例运行的架构
现在，你的任务实例的状态是排队状态
执行者也已经在元数据库中更新了任务实例的状态
一个walker进程，可能是内部的也可能是外部的
这取决于airflow的架构
你选择的进程从队列中获取任务实例并执行它
现在任务实例的状态是运行中
走流程序更新任务实例状态
一旦任务完成
走流程序将任务实例的数据库状态更新为最新状态
以最新状态
在这个例子中是成功
调度程序检查是否有其他任务需要运行
如果没有根据任务最新状态查看流程图是否标记为已完成
你的成功或失败就在这里
成功 与此同时
你可以通过用户界面监控你的数据计划中正在发生的事情
与你的任务 就是这样
这就是Airflow如何处理你的数据管道的执行
这比那更复杂 但现在这就是你需要知道的
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/10_Udemy - Apache Airflow The Hands-On Guide p10 6. Airflow limitations.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


Airflow有一些限制，你将在本视频中发现这些限制
首先，流处理，当你需要对数据进行实时处理时，Airflow无法做到这一点
没有延迟
Airflow是批处理导向的
这意味着你不能在实时中进行你的数据处理
你可以每分钟运行你的数据处理任务
但可能会出现延迟，这取决于你的任务和资源
你也可以使用像kafka这样的流处理解决方案与Airflow结合
但你的数据处理仍然不会在实时中进行
下一个限制是数据处理
每当你在Airflow任务中处理数据时，你都需要小心
因为Airflow是一个调度器
你可能没有你的数据处理框架（如Spark）那么多的资源
如果你直接在Airflow中处理大量数据
你可能会遇到内存溢出错误
总是确保你有足够的资源来处理你想要处理的数据
否则
使用Airflow作为触发任务或脚本的方式 该任务或脚本在外部工具或服务（如Spark）中处理数据
这就是关于Airflow限制的所有内容
流处理不能在Airflow中进行，数据处理可以，但你需要考虑你的资源是否足够处理你的数据
以及它们是否足够处理你的数据 这就是关于Airflow限制的所有内容
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/11_Udemy - Apache Airflow The Hands-On Guide p11 7. Practice Installing Airflow.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


安装Airflow有很多不同的方法
这个视频将只展示其中的几种
但我强烈建议你查看文档
如果你想要得到一个详细的安装方法列表
以及它们的优缺点
那么我们从第一种开始
Python包索引或PEP
这是最基本和手动安装Airflow的方式
如果你不知道PEP是什么
PEP是一个Python包管理工具，用于轻松安装Python包
让我来展示你安装Airflow的不同步骤
请不要使用这种方法或下一个方法来安装Airflow
请等待视频结束
我会向你展示我们将如何安装Airflow
以遵循课程中的最佳实践
首先
如果你是Windows用户 你需要做一些准备工作
第一步是确保你的计算机上安装了Python3.7或更高版本
以及Python包管理工具PEP
然后确保你已经启用了Windows子系统为Linux
WSL与Ubuntu最新版本
但最后
你需要启动一个Linux终端
以便执行我将要显示给你的命令 而对于那些不是Windows用户的人来说
那将更容易
因为你不需要启用WSL
好的，猜猜看，这将会更简单
因为你不需要启用WSL
好的，猜猜看，这将会更简单
第一步是创建一个Python虚拟环境
使用下面的命令创建一个名为venv的Python虚拟环境
总是最好创建一个Python虚拟环境
以避免与操作系统上已安装的Python包发生冲突
然后激活虚拟环境
你需要执行以下命令以激活虚拟环境
之后，你需要指定Airflow将存储其配置文件和元数据的位置
例如日志和SQLite数据库文件
使用airflow home
为了确保Airflow能够正确安装
建议你下载并使用一个约束文件
包含所有Python依赖项及其相应的版本
由于Airflow依赖于许多Python依赖项
使用约束文件有助于确保安装过程顺利进行
你不想浪费时间处理依赖项冲突
现在是时候使用pip安装Airflow了
在这里安装你想要的Airflow版本，例如1.10.12
以及约束文件，这将花费一些时间来完成
一旦气流被安装
下一步是初始化元数据数据库
默认情况下是Sqlite
请记住气流的核心组件
元数据数据库存储所有元数据
例如任务实例
用户等
要访问气流用户界面
您需要使用以下命令创建管理员用户
最后，您需要在80端口上运行Web服务器
80和sker谁
这太多了
只是为了让气流安装并运行
您需要初始化数据库
创建管理员用户，并且还有其他一些您在这里看不到的东西
您将无法同时运行多个任务
如果您想这样做
那么您需要对您的数据库进行配置，如postgres
更新气流配置文件以进行一些特定设置等等
所以，只是为了得到一个完全功能的气流实例，需要做很多工作
这就是我为什么不推荐使用这种方法来安装气流
而是推荐使用Docker方法
如果您不知道Docker是什么
可以将Docker视为一种在称为容器的微小虚拟机中运行应用程序的方式
这样您的应用程序将始终运行
无论您的操作系统和现有依赖项如何
它工作的方式非常简单
您创建一个Dockerfile
这是一个包含构建您应用程序所有必要指令的文本文件
例如 假设您想要运行一个Python脚本应用程序
在Dockerfile的顶部写Py
我们指示脚本将在一个包含Python3的容器中运行
然后我们安装一些额外的Python包
例如Flask或您想要的任何东西
我们复制Python脚本应用程序
点py它在标准输出上打印一个世界
并在Dockerfile的末尾运行它
所以现在我们有了安装和运行我们的脚本的不同步骤
下一步是构建Dockerfile以获取Docker镜像
Docker镜像是Dockerfile的编译版本
最后一步是运行此Docker镜像
以获得在隔离状态下运行我们的Python脚本的Docker容器
使用这个Docker镜像
您可以与任何人分享
无论他们的操作系统或依赖项如何
您的应用程序将始终运行
这就是Docker的美妙之处
它使运行和共享软件应用程序变得容易
这门课程不是关于Docker
如果你不熟悉docker
我强烈建议你多了解一下它
了解这个工具对你来说非常重要
好的 回到airflow
好消息是
你不必创建docker文件
它已经为你准备好了
你需要做的就是下载文件
使用以下命令运行它
你已经在你的电脑上运行了一个完全功能且配置好的空气环境
这仅仅需要两个命令
就是这样 我想我们都同意这比之前的方法要容易得多且更快
使用Python包索引
这就是为什么在现实世界中，你总是使用Docker的原因
在公司里，你将永远不会手动安装Airflow
好的 虽然这种方法很好
但你仍然需要做几件事来创建一个Airflow环境
这遵循了最佳实践
这就是为什么有更好的方法
方法是使用astro cli
astro cli是一个开源的命令行界面，用于创建和管理本地airflow环境
轻松遵循最佳实践
让我来向你展示，在这一点上，你应该已经安装了astro s i
如果没有，请访问以下网站
然后安装cli
选择你的操作系统并按照说明进行操作，一旦你安装了astro ci
打开你最喜欢的代码编辑器
对我而言，这是视觉工作室代码，也包括终端在内。
如果你在终端中输入astro version，
你应该能看到你安装的ao cli版本，可能跟我的不一样，
但你应该能看到一个版本号。
好的。
这意味着你已经成功安装了astro chi。 现在是时候用它来设置并运行airflow了。
也确保你已经运行了docker desktop，
否则在终端中无法运行。
创建一个新文件夹
Udemy airflow
然后进入这个文件夹并在其中打开你的文本编辑器
如你所见，在左边
我现在在udemy airflow目录下
现在，下一步是在其中输入astro dev
以便初始化你的空气流环境
然后你按 回车
如你所见，搞定
我有很多文件夹和文件是生成给我的
为了得到一个完全功能的airflow环境
遵循最佳实践
使用docker方法你不会得到那个
这就是为什么我认为astro cli更好
但是话说回来 让我给您快速描述一下我们这里的每个文件和文件夹
所以第一个是文件夹dags
所以文件夹dags正如你所知
是你放你的dag文件的地方
对应于你的数据管道的文件
你有一个是例子
然后你有include目录
这个include目录是
你将放你所有你将在你的数据管道中使用的文件
例如python脚本或函数
sql请求
bash脚本
任何它不是数据管道
但你将在你的数据管道中使用的东西
然后你有plugins文件夹
如果你想自定义你的空
例如你可以这样做
你只需要在这里添加你的插件
然后你有tests文件夹非常有用以确保你的任务和你的数据管道工作
我们将在后面的课程中看到
然后你有一个.dotignore文件
这在这种情况下不是很重要
然后你有.dotml文件来导出环境变量
你将在你的f中可以使用的变量
那可能对你配置你的空有用
或者创建你将在你数据管道中使用的连接和变量
正如你将在后面的课程中看到的
然后你有ignore文件再次
这不很重要 airflow设置非常有用来持久化你在你的空实例中创建的数据
并且你想要确保你不会失去它
每次你重新创建你的环境
所以你会使用这个文件节省很多时间
当我说数据时
我的意思是连接
变量和池
你将看到那些概念在后面的课程中
然后你有docker文件
并且因为我们使用astro cli
我们使用astro运行时图像
astro运行时是airflow的包装器
如果你想知道哪个as运行时版本对应哪个afl版本
你只需要去以下网站
你可以看到最新的ao运行时版本与对应的airflow版本
所以每当有新的 airflow 发布
您只需要更改这个数字
好的 下一个文件是 packages
Xt 这个文件在你想安装操作系统包时很有用
例如获取 git 等等，这不是 python 包应用程序
然后你有 readme
显然我推荐你阅读
然后是 requirements.txt 文件
这个文件用于安装 python 包
例如，你可能想从你的数据流中向 API 发送请求
为此，您需要请求的 python 包
您可以仅使用版本请求使其从数据管道中可用
这就是关于文件和文件夹
这是 astral cli 为您生成的
如果您仍然有一些问题
别担心 我们将在课程中广泛使用它们
为了运行 airflow
在下一个终端中运行 astro dev
启动
等待一会儿
如您所见，Docker 正在下载运行 airflow 所需的所有内容
几分钟后，您应该能看到 airflow 正在启动
项目正在运行所有组件现在可用
您可以在此 URL 访问 airflow 用户界面
验证一切正常运行的一种方法是转到 Docker 桌面
然后在容器中，您应该看到 udemy airflow 在这里有某些内容
展开它 您将看到所有与 airflow 实例相对应的容器
元数据库 postgres
调度程序 触发器和 Web 服务器
如果您有任何问题
请告诉我在 houdini 的问答部分
也请确保端口 8080 可用
否则您将无法运行 Web 服务器
好的 看起来 airflow 通过 astro cli 正在运行
让我们确保可以访问用户界面
在新标签中打开您的 Web 浏览器并输入 local hosts 8080
如您所见，您应该到达这个惊人的页面
然后您输入 admin
管理员
登录 您已成功安装并使用 astro cli 设置 airflow
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/12_Udemy - Apache Airflow The Hands-On Guide p12 9. Practice Quick tour of Airflow UI.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 你将发现我认为Airflow用户界面最重要的视图
让我们开始吧，当你访问Airflow用户界面时
默认情况下你将进入DAG视图
DAG视图列出
你Airflow环境中活跃和非活跃的DAG
并允许你快速查看给定数据平面中多少个任务成功
失败 或正在上运行
你可以通过点击这些按钮来过滤显示活跃的DAG
你也可以根据正在运行或已经完成的dag进行筛选
这在你有很多数据管道的情况下非常有用
你想专注于那些有问题的管道
如你所见
我们有一个标签示例
让我告诉你每个列的含义，从左到右
第一个带有开关的列是暂停或启动你的数据管道
决定是否调度dag
然后你有dag名称，对应dag id
dag的唯一标识符
或者从气流到九
你的工作流显示名称
稍后会看到 但大多数时候，你可以在这里看到你的工作流的唯一标识符
在DAG名称下方，你有标签在这里
只有一个标签示例
标签非常有用，用于按团队或功能对工作流进行分类
你可以按标签过滤工作流
正如你所看到的，非常有用
如果你有不同的团队
你想知道哪些dags属于哪个团队
那么这里所有者为真
你可以修改这个
但请记住，所有者实际上在用户界面中并没有被使用
我的意思是你在审计日志中看不到它，除了在审计日志中
你也无法根据它进行筛选
所以老实说
我真的不知道为什么它还在这里
然后你有所有预览的状态，目前正在运行
Dagrants 排队
成功运行和失败的调度定义了你的数据管道运行频率，这里设置为每天
这意味着每天午夜
最后一次运行对应了你DAG最后一次执行的数据区间
而下一次运行则对应了下一个数据区间的开始
在这个区间里，你的数据管道将会运行
如果你悬浮在这个小图标
你可以确切地知道管道何时运行
以及下一次运行后的下一个数据区间
这里有最近的任务
这些是所有活跃图表任务的状态
或者如果不是当前活跃的，则是最近一次运行的状态
你会看到许多策略
比如已知的调度
排队运行
成功 重绘
等等 在整个过程中
你可以通过点击这里手动触发你的数据管道
你也可以通过点击这里删除你的数据计划
但是 请记住，它不会删除dag文件
只会删除与之相关的beta数据
所以你在用户界面上不会再看到它
但你仍然会保留dag文件
最后，你有一些链接可以访问我们将在后续看到的不同视图
好的 让我们启动数据管道，以便一些任务可以运行
这样更容易说明其他视图
我们将会看到
好的，完美 点击dag
默认情况下你将进入网格视图
网格视图是一个条形图和网格的dag表示，跨越时间
顶部行是dag运行时间的图表
所以条形越高
完成dag所需的时间就越长
下面对应于网格的task实例
每个网格都有一个颜色
表示task实例的状态
如果管道延迟
你可以快速看到不同步骤在哪里，并识别出阻塞的步骤在右边
你有dag总结
特定dag
你知道显示多少运行
有多少是成功的
最大运行时间
平均运行时间
这非常有用来指定你的时间
我为你的dag等等
如果你点击一个特定的dag
详细信息会有所不同
如你所见，首先
你可以指定一个dag节点
我强烈建议你使用
如果你想跟踪这个特定dag发生的任何事情，并点击保存备注
然后你可以看到dag的状态以及运行类型
运行时间
数据区间等等
如果你再次选择一个任务实例
细节会有所不同
你有我们所谓的数据集
你将在后续课程中看到的事件
以及研究
任务ID 这个DAG中你任务的唯一标识符
尝试次数等等
基本上使用网格视图
每当你想查看图表的历史和你对应的任务
与他们的状态一起
您拥有图表视图 du
帮助您可视化您的任务依赖关系及其当前状态，适用于特定运行
例如，您可以看到两个任务和一个数据集
您知道什么操作员
每个任务都使用一个任务
这是执行Python函数的Python操作员
他们的状态与颜色
成功，在这种情况下
您知道print astronaut craft依赖于get astronauts
所以，在你想要验证任务依赖性时，使用图表视图
这对于这一点非常有用
在图表视图旁边
你有甘特视图
或者甘特图
甘特图特别适用于分析任务持续时间和重叠
你可以看到，获取宇航员任务完成的时间比其他任务长
打印宇航员
如矩形所示
所以矩形越长
完成这项任务所需的时间越长
你可以看到两种颜色
所以灰色区域是提示的持续时间
另一种颜色可以是绿色或红色
取决于任务实例的状态是运行时间
所以这种视图非常适合快速识别瓶颈
并了解特定图表中大部分时间花费在哪里
所以你知道你应该优化哪个任务，或者你喜欢的另一个是代码视图
你不仅能够看到你的DAG代码
你还能知道
当你的dag最后一次被airflow调度器通过是什么时候
记住，airflow调度器每五分钟为新的dag文件通过，每三十秒为现有的文件通过
所以，而不是猜测
你可以简单地检查日期，看看你的数据管道最后一次被通过是什么时候
这样，你可以确保你制作的任何更新已经被airflow正确处理
这样 你可以确保你制作的任何更新已经被airflow正确处理
但是，最后 你有任务持续时间视图，显示过去和运行的任务持续时间
这对于找到异常值和快速理解你的bag中的时间花费是完美的
多次运行后
实际上 如果我们再次运行dg，点击这个按钮并稍等片刻
然后选择一个任务
你可以看到在两次dg中，宇航员的持续时间如下
所以你知道在第一个图中完成任务的时间比在第二个图中要长
现在，你对使用哪个视图有了更好的了解
以及为什么 但在用户界面中还有许多其他视图
例如，你有变量和连接
数据集 聚类活动和等等
但稍后你会更容易看到这些视图
在封面特征的背景下，而不是我现在展示它们
所以稍微有点耐心 我看到你下一段视频
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/13_Udemy - Apache Airflow The Hands-On Guide p13 10. Practice Quick tour of Airflow CLI.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


除了用户界面之外
你还有命令行界面
这对于许多方面非常有用
正如你将发现的那样
让我们开始吧 第一个问题是为什么使用Airflow的命令行界面
为什么不直接使用UI而使用CLI呢
嗯，第一个原因是可能你无法访问UI
因此你必须使用CLI
此外，一些有价值的操作仅通过CLI可用
正如你在这个视频的后面会看到的
然后是最后 但并非最不重要的是，命令行界面比用户界面更快
或者通过与你的数据管道或任务进行交互的REST API
我在这个幻灯片上要添加的另一件事是
你可以完美地在CI/CD流水线中使用命令行界面
如果你不知道什么是CI/CD流水线
你可以把它想象为一系列你将要执行的操作
以便将你的数据管道部署到生产环境中
并且在这个CI/CD流水线中
你可以肯定地使用Airflow的命令行界面
以便执行一些命令来测试你的任务等
好的 现在你知道为什么CLI了，让我们探索如何访问它
如果你直接在机器上运行Airflow
那么你什么也不做
你只需在终端中直接输入Airflow和要执行的命令
但是如果你使用Astro Ali或Docker，正如我们现在所做的那样
那么在Docker中只有一个额外的步骤
你可以使用docker ps
然后复制容器
调度器或Web服务器的ID
执行以下命令
到这个点你已经在Web服务器的Docker容器中了
并且你有使用astro cli访问airflow cli的权限
你只需要执行 astro dev bash，这会容易得多
现在您可以访问 airflow cli 并执行命令
让我来告诉你，我认为你应当了解的最重要的事情
让我们从与数据库相关的评论开始
第一个是气流数据库
检查
元数据数据库是否可达
然后 正如你在课程后面会看到的
Airflow存储了大量的元数据，你需要定期清理这些数据
你可以执行命令
Airflow db clean，这将清除所有数据，将它们从初始表移动到归档表中
如果你想将数据库中的归档数据导出为CSV文件
你可以使用airflow db export archived
如果你想知道如何从数据库中删除归档数据
肯定你可以使用airflow db drop archived last
但最后你不得不执行的命令
如果你手动安装airflow
是airflow db以便初始化数据库
但如果你使用docker或astro cli
那么你就不必执行那个命令
它会自动为你完成，接下来是下一组命令
让我们看看如何与你的dags交互
第一个命令是airflow dags backfill
也许你不知道什么是回溯
但你会在课程中看到它，目前
记住，这个命令允许你在指定的日期范围内运行或重跑dag
例如 这里我们重跑或运行dag
在一月一日2024年和一月十日2024年之间
你可以通过airflow ui完成这项任务
但在cli中做会更简单
记住，airflow调度程序会将你的数据管道序列化到元数据库中
这个进程每5分钟对新dag运行一次
对现有dag每30秒运行一次
如果你不想等待
或者出于某种原因你需要重新同步你的数据管道
运行airflow dags
序列化
最后，airflow ag's list对于验证非常有用
airflow知道一个特定dag的存在
如果在列表中看不到它
它将不会在ui中显示
好的 最后一组命令与任务有关
老实说，我想给你看一个命令
airflow tasks test作为最佳实践
我推荐你每当向dag添加一个新任务时
你都测试它，airflow tasks test
通过运行任务而不检查依赖项或存储元数据来验证你的任务是否工作
当然，还有其他与你的任务相关的命令
例如列出你的任务或运行一个特定任务
但我觉得这一个真的是最重要的
这就是关于airflow命令行界面的所有内容
不要犹豫去尝试它
另外 你可以通过运行airflow cheat sheet获取所有命令的列表
并且随意查看文档 总是有用的
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/14_Udemy - Apache Airflow The Hands-On Guide p14 11. Practice The Rest API.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


与Airflow交互有多种方式
你有四个UI
你看到了你有CLI
你也看到了
还有Airflow REST API
Airflow REST API非常有用
如果你想处理你的数据
从另一个工具中获取管道或任务
想象你有一个网站
并且有一个表单
你感觉到这种形式 然后你点击按钮
你可以想象这个按钮会触发你空中的计划
例如 这是通过使用dpi来实现的
所以看看dpi的流程
我建议你去官方airflow文档
然后点击它并转到底部左侧点击稳定rest api
从那里 这是air flow dpi的官方文档
你有所有的根
你可以用它们来与您系统的不同组件进行交互
例如 您可以在左侧找到那些组件
如您所见 您有连接
甲板 匕首
水池
供应商等等
想象一下，你想查看你的dags
你可以点击dag 然后你会看到所有可用的路径列表
以便与你的数据管道进行交互
附带http方法
例如，列出你的数据管道
你点击这里 然后你复制这个链接
你粘贴到这里并确保不要使用https而不是
本地主机
八十八十
你应该能看到这个json，它列出了你的数据列表
Piexample
这是基本的和先进的例子
还有大量的信息
你可以配置身份验证过程
查看身份验证和API授权
你会看到你的选择
你可以做哪些事情来验证你的空气流
REST API，话虽如此
我不会走你所能用的所有路线
因为很多，所以请随意看看气流
Dpi真的很有用
尤其是当你想再次触发时
来自另一个工具的工作
像zapier
像spark
或者像网络应用 这将非常有用
所以看看文档
了解更多关于cpi的信息
你将看到所有可以解决的问题 谢谢它
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/15_Udemy - Apache Airflow The Hands-On Guide p15 1. Introduction.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎来到课程第三节
我很高兴在这里见到你
为什么，因为这时候我们要开始构建项目
基本上，在这一节结束时
你将能够创建你的数据
从A到Z的应用
你也将能够定义不同类型的任务
例如等待一个事件
执行Python函数
或者与数据库交互
你也将能够监控和管理你的数据管道
你将了解如何重启任务
如何调试你的数据管道和任务等等
然后最后 但并非最不重要的是如何与你的数据平面其他工具进行交互
当然，还有很多其他内容
但我想让你保持惊喜
如果你感到兴奋 让我们继续前进
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/16_Udemy - Apache Airflow The Hands-On Guide p16 2. The Project! What you will build.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我来向你展示这个项目结束时你将拥有的东西
这个仪表板展示了一年内苹果股票市场的价格
正如你可以从蓝色线条看到的那样，这就是价格
而在蓝色线条下方，这就是成交量
一天内交易了多少股
此外 你也可以看到一年内的平均收盘价以及成交量的平均值
但最重要的一点是，这个仪表板是基于
你将要构建的数据管道构建的
让我来向你展示这个数据管道
这是股市数据流
首先，我们要确认为什么金融是否可用
这个API允许我们获取股市中苹果的价格
一旦API可用
为此我们使用传感器
我们想要获取苹果的股价
为此我们执行一个Python函数
一旦我们获取了价格
我们想把这些价格存储到minao中，minao就像一个本地的三方或谷歌云存储
一旦我们在minao的json文件中存储了价格
我们准备好格式化那些价格
因为使用那些价格
我们需要改变格式并做一些小的调整
而为了做到这一点，我们执行任务格式化价格，这触发了一个spark作业
如果你不知道什么是spark
把它想象成一个数据处理框架
我们将使用它来处理价格
一旦我们做完这些
我们将得到一个csv文件
然后我们想把文件给你
这就是我们处理这个任务的方式
一旦我们有了csv文件
我们就可以将它加载到我们的数据仓库中
数据仓库是postgres，数据存储在postgres中
我们可以使用数据库last在它上面构建仪表板
但同样重要的是，当我们的DAG成功时
当您的数据成功加载时
我们将使用airflow中的新机制
即通知机制，以便在slack上自动接收通知
正如您将在本节后面发现的那样
这个项目使用了许多不同的空气流机制
例如，如何使用答案等待一个事件
或者如何使用Python操作器执行一个Python函数
如何使用Docker镜像触发一个应用程序
在这个例子中，它是Spark，如何让您的任务之间共享数据
如何将文件系统上的文件加载到数据库中
还有很多更多 如果您准备好了 让我们开始吧
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/17_Udemy - Apache Airflow The Hands-On Guide p17 3. Project materials.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 我将向你展示一些关于docker的知识
以确保你可以运行这个项目
然后你会在udemy下下载这个视频的文件
对应于你需要运行这个项目的一些文件
例如docker compose文件
我们将查看那个文件
这样你就知道哪些是不同的服务
不同的组件以及在这个项目的上下文中它们有什么用处
那么事不宜迟
让我们开始
好的 第一步是确保你的电脑上安装了docker desktop
如果你没有docker
请访问docker com，然后按照说明安装docker
我们使用docker的原因是我不知道你的操作系统
我不知道你安装了什么依赖
如果我不用docker，我无法帮助你
docker帮助我们创建一个相似的环境来运行项目
这就是为什么使用Docker非常重要的原因
在运行项目之前，我想让你验证几件事情
首先，检查磁盘空间
确保你有足够的可用空间，就像这里看到的那样
如果你没有足够的磁盘空间，你将无法运行项目
你可以做的就是转到图像
例如 然后选择所有它们，你可以点击
删除以释放一些空间
如果这不够
你可以去容器那里
然后选择所有的容器
删除它们 你可以对卷做同样的事情
你选择所有的卷并删除它们
你也可以对构建这样做
所以你可以看到有很多地方你可以释放一些空间
但是再次 确保你有足够的空间来运行项目
去设置
所以在右上角
你有设置
你点击这里
然后你看看资源，因为我们将要使用spark
如果你不知道什么是spark
把它想象成一个数据处理框架，用于大规模处理数据
你需要至少有8GB的内存
否则你可能会遇到一些奇怪的spark问题
因为你没有足够的内存
所以再次 确保你有至少8GB的内存给CPU
这并不重要
但如果你有至少4个CPU
这将会更好
那么，现在你已经正确定义了所有设置
你可以继续项目并下载项目材料
你将在Udemy上找到材料
在这个视频下 你有资源点击
然后下载Airflow项目
压缩文件 这是一个压缩文件
所以你需要先解压文件
解压后
你将得到以下文件和文件夹
我需要你做的是复制这些文件和文件夹
并将它们移动到你的astro项目中
在这里，你将你的文件和文件夹粘贴到你的项目中
如你所见
我有一个新的文件夹
包括不应该存在的副本
所以点击助手并将该文件夹移动到包含文件夹中
然后您可以像这样删除包含副本文件夹
也要确保您有显示在这里的Spark文件夹
Spark主笔记本等
这个文件夹不在任何文件夹中
如您所见，它位于项目根目录中
然后您应该拥有docker compose点override
如所示，位于docker文件旁边
好的 让我给您解释一下这些文件和文件夹
从docker compose文件开始
如果你记得docker compose
你可以在同一环境中同时运行多个应用程序
例如 在我们的项目中我们需要airflow
我们需要minau
我们需要spark等等
所有这些服务应用程序都在docker compose文件中定义
因此，一旦我们运行docker compose文件
我们同时运行所有这些应用程序，它们可以相互通信
所以如果你查看文件并往下滚动一点
你会看到我们有一个服务minao
然后我们有spark master和spark walker spock
需要主和步行者才能工作
然后mebase用于稍后在项目中构建仪表板和docker代理
这样服务可以内部连接到docker
但是不要忘了我们有一个网络，我们用它来运行所有服务
以便它们可以相互通信
这就是为什么我们也重新定义了airflow服务的网络
Web服务器 调度器和测试者
我们想要确保Airflow能够与Minao Spark等通信
说到Spark
如果你打开Spark文件夹
我们有这些文件夹
主 笔记本和步行者
对应我们将构建的基本上的Docker映像，主对应Docker映像
Airflow / Spark主
你可以在这里看到文件夹，然后对于步行者
它对应Airflow / Spark步行者，正如你所看到的这里，再次
你有文件夹
斜杠Spark斜杠步行者
还有笔记本是另一个我们将构建的Docker映像
它包含我们将作为Spark作业运行的脚本，在我们的项目中处理数据
但你将在视频中看到
最后，在include文件夹中，你有开发者文件夹和一个文件
Minayour.py
此文件包含以下函数以获取Minayour客户端
我不想让你重新创建它
这就是为什么你有这里的函数
它基本上会帮助你从你的空气流任务连接到Minao
现在你有了所有关于项目材料的解释
最后一步是打开dags文件夹并删除这里的示例dag 所以右键 点击并删除并移动到回收站
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/18_Udemy - Apache Airflow The Hands-On Guide p18 4. Running the new environment.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


项目材料在你的环境中
现在是时候验证一切按照你的代码预期工作
或者打开一个新终端
然后确保你在你的astro项目的文件夹中
如你所见，我有airflow，并且我的项目是airflow
然后在终端运行以下命令为true
dev stop以停止可能正在运行的任何容器
所以运行命令如你所见
我没有任何容器
这就是为什么我得到了这个你完美的结果
所以现在转到spark文件夹
然后从这里设置master
你需要运行以下命令
Docker build -t
然后如果你打开docker
Compose文件
复制这个并粘贴到这里
这将是spark master对应的docker镜像名称
然后你添加一个点，因为docker文件在master文件夹中
然后按回车以构建spark master对应的docker镜像
如果你收到了一些警告
一旦Docker镜像构建完成
这是可以接受的
这只是警告
所以目前 不要担心
然后对Spark Walker做同样的事情
所以你回到Spark文件夹
然后你进入Walker
然后你再次运行相同的命令Docker
构建dash t
但这次它是由Docker Compose文件定义的火花步行者
在这里再次
您使用点作为Docker文件位于步行者文件夹中
运行命令
再次您会收到相同的警告
这是可以的 因此，在这一点上，您已经创建了两个新的Docker镜像，它们对应于Spark主和Spark步行者
实际上，您可以通过访问Docker桌面来验证您已经构建了这些镜像
然后查看图像，你应该能看到两个docker图像
Airflow slash spark master和airflow slash spark walker
如果你有那些警告
这是没问题的 它应该能正常工作
但如果它不能正常工作，请告诉我在udemy的问答部分
所以现在让我们回到代码
现在是时候运行项目了
回到项目根目录
这是airflow
那么我们回到那里，然后以真的方式运行命令
开发启动
进入并稍等片刻
几分钟后
你应该能看到那些指示如何找到Airflow用户界面的行
以及PostgreSQL数据库
如果你在你的Web浏览器中跟随以下链接
你将到达该页面
在这里你输入admin
现在你可以访问Airflow用户界面
如果你无法运行Airflow
你需要检查几件事
首先确保你的容器以预期方式运行
你前往Docker桌面
你点击容器
在这里你应该能看到类似这样的东西
你可以展开以查看与Airflow项目对应的所有容器
如你所见，有minao
Spark walker PostgreSQL bays等等
确保所有容器都在运行
如果一个容器没有运行
比如说Spark walker
你可以点击一个给定的容器
然后查看日志并尝试仅重启容器
在这里你有重启
你点击它 这将仅重启此容器
再等一会儿，看看是否起作用
还确保你没有使用项目所需的一些部分
例如minao需要端口9000
然后你有Spark walker
8081
PostgreSQL5432
如果你已经有一个PostgreSQL实例正在运行
那么这不会起作用，同样对于Airflow
它运行在端口8080上
如果你已经在端口8080上有东西正在运行
你将无法运行Web服务器，所以再次
确保所有这些端口都可用在你的计算机上
最后但同样重要的是，如果你在运行命令as true destart后遇到一些奇怪的网络问题
或者在一个受限的网络中，这可能是由于防火墙或一些安全政策
在这种情况下，尝试使用不同的网络
好的 现在你在本地有一个运行的Airflow实例 你可以开始项目
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/19_Udemy - Apache Airflow The Hands-On Guide p19 7. Create the DAG with the dag decorator.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 你将实现股票市场数据管道的第一项任务
这是一个容易获取的API
这项任务的目标是验证雅虎财经API是否可用
顺便说一下，这个API是由雅虎提供的
它允许你获取特定公司的股票市场价格
例如 如果你访问以下链接
你将看到苹果公司一年内每天的价格
从这个JSON 我们要提取的是时间戳
这些时间戳对应于日期
我们要提取一些关于价格的信息
例如，一天中价格达到的最高点
特定日期交易的股票数量
一天中价格达到的最低点
这一天的开盘价是多少
以及这一天的收盘价是多少
这就是我们要从这个json中提取的
为此我们将使用股票市场数据计划
但是首先，我们需要创建DAG
让我们回到质量在跟随DAG
创建一个新的Python文件
股市
然后导入du so从Airflow装饰器
导入deck装饰器
并导入daytime对象
因为我们需要为其定义起始日期
我们希望开始安排数据流的日期
您可以使用deck装饰器与一些参数
第一个是起始日期
假设是2023年1月1日
然后我们想要定义我们想要触发数据管道的频率
在这个例子中是每日
所以每天午夜
我们不想运行任何未触发的过去任务
所以ketchup等于false
例如我们是2023年2月1日
而起始日期是2023年1月1日
我们不想运行所有未触发的过去任务
在2023年2月1日和2023年1月1日之间
基本上，使用此参数
我们总是触发最后一个DAGRANT
我们定义了一些标签
标签非常有用，可以分类和过滤您在UI中的数据管道
在这种情况下，我们只想使用一个标签
这是股票市场
您可以根据团队正在处理的项目指定标签
或者根据项目等等
只需利用标签以更好地组织您在Airflow UI中的数据管道
一旦我们设置了那些参数，这就是全部
我们可以在以下创建函数
这是股票市场
这是您的数据管道的dag id
您的数据管道的唯一标识符
别忘了在文件的底部调用此函数
像这样，如果您保存文件并进入dia for you
您应该能够看到您的数据类型和股票市场与标签
股票市场以及代码中定义的每日日程
这就是如何在流体中创建数据管道，非常简单
您可能会想知道这是什么标记
这是任务三的API
任务三的API是Airflow中创建任务和DAG的一种新方法 我将在下一个视频中向您详细介绍
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/20_Udemy - Apache Airflow The Hands-On Guide p20 8. The new way of authoring DAGs with Taskflow.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


我认为展示vpi测试的最佳方式是通过一个具体的例子向你展示
顺便说一句，你会在这个视频的后面找到例子
大多数时候我都建议你做所有的作业
你会找到所有你需要的东西
尤其是代码和其他你在视频中发现的东西
在这个数据管道中，你有两个任务
任务a和任务b
两者都使用python操作符
任务a执行以下python函数
然后打印在太阳能输出任务a并返回值四二
所以，这个值将被存储在Airflow的数据库中
然后，tasb将执行以下Python函数
该函数将在标准输出上打印
测试B并从Airflow的主数据库中拉取xcom so four two
以便也将其打印到标准输出
这就是我们使用ti dot xcom pool的原因
别担心 你将在本课程中学习更多关于x comes的信息
但现在 请记住，这种xcom机制允许您在任务之间共享数据，话虽如此
如果你在 airflow ui 上查看任务流 dag，然后查看任务 b 和日志，
你可以看到有四个 two 进入 b 正如预期，
所以任务 free pi 的目的是真正去除所有这些冗余代码，
来做同样的事情，
因为想想看，
那两个任务执行 python 函数，
仅此而已，
但你仍然需要创建一个 python operator， 然后定义一个任务，
任务
然后我调用Python函数
同时使用X Com池来共享你的任务之间的数据
这需要大量的代码
只是为了做非常简单的事情
而这正是TaskFor API非常有用的地方
让我来告诉你，你可以将TaskVI视为一组装饰器
这些装饰器相当于一些操作符，也包括DAG对象
例如，我们可以删除from airflow import dag
取而代之的是使用装饰器
导入流程图和任务流程图相当于DAG对象
这里和任务相当于Python操作符
所以我们也可以删除这个导入
然后我们可以通过使用DAG装饰器来更改DAG定义
在DAG中我们保留相同的参数
我们删除我们定义在DAG装饰器下面的标签，只保留任务标签
这里是DAG的ID
你的数据管道的唯一标识符
你可以删除这个参数
然后你需要在文件的末尾调用任务流函数
在这个点上 你已经通过使用带有dark装饰器的taskapi重新定义了暗对象
所以你指定了dark装饰器
你列出了参数
你创建了一个对应于你的DAG的Python函数
然后你在这里调用这个Python函数
这比之前多了一些代码
让我来展示它如何与任务一起工作
所以与任务一起，你可以删除所有这些
相反，复制这两个Python函数
把它们放在这里
确保缩进正确
然后使用测试装饰器在上面那些python函数
去掉下划线
现在 python函数的名称
对应你在UI上看到的任务名称
你可以删除它
相反 我们想说嘿
这个Python函数
这个任务期望一个值
那么我们设置一个值
我们希望在标准输出中打印这个值
然后最后 但并非最不重要，因为任务b需要任务a返回的值
这是四二
我们可以像这样设置依赖关系，任务b需要一个参数
任务a，就是这样
你得到了和之前一样的dag
但是使用了更少的样板代码
这就是test three pi对于某些操作员的力量
并非所有操作员 例如kubernetes操作员
docker操作员 分支python操作员
等等 你可以使用装饰器得到等价的效果
这些装饰器更容易更快速地编写和阅读
也基于任务之间共享的数据
任务三的API能够自动定义依赖关系
这就是我们在这里做的事情
如果你看一下f四的UI并转到图形视图
你可以看到任务B依赖于任务A，因为它们之间共享数据
你可能想知道 我可以混合这些装饰器与经典操作符吗
是的 我可以让你看一个例子
所以再次 让我们重用Python操作符
因此从airflow操作符中导入Python操作符
然后我们使用 让我们假设
任务a
我们创建任务a等于Python操作符与任务a任务a
Python调用下划线任务a
然后这里粘贴Python函数
让我们删除那个
然后下划线任务a像以前一样
但现在而不是调用任务a像这样
你可以直接使用任务a点输出
然后保存文件并返回UI
然后刷新页面
如你所见，没有错误
如果你看图表
您仍然拥有依赖于任务a的任务b
您在这里看到的点输出相当于这样做xcom
任务ID等于任务a
如果任务a和b不共享任何值怎么办
在这种情况下，您仍然可以定义依赖关系
使用旧方法，使用右和左shift操作符，如任务a
然后任务b，不要忘记在这里删除参数
这就是关于test three pi的所有内容
请记住 写您的DAG可能会更快更容易
但那被说，正如test three pi并非所有操作符都可用
有时混合经典操作符与装饰器可能会很棘手
所以我建议您使用test three pi或经典操作符 并尽量避免混合使用它们
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/21_Udemy - Apache Airflow The Hands-On Guide p21 10. Checking API availability with the Sensor decorator.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


你现在已成功定义了正确的DAG参数
下一步是创建DAG中的第一个任务
第一个任务是验证Y Finance API是否可用的API可用性
让我们验证API是否可用
我们将使用传感器
传感器等待条件为真以成功
在这种情况下，传感器将等待Y Finance API可用以成功
否则，传感器将在某一点失败
为了创建该任务以创建该传感器
我们将使用新装饰器
任务点传感器装饰器
所以导入任务对象在这里，然后删除pass
现在您可以使用装饰器task点传感器
然后您可以定义几个参数，第一个是book_interval
以便定义传感器应如何频繁检查API是否可用
让我们说pointerval等于30
因此每30秒传感器将检查API是否可用
然后您应该定义超时
传感器应在何时超时
让我们说在5分钟内这是秒
在这里您应使用模式
这是默认模式
别担心 您稍后将在本课程中学习更多关于模式的信息
现在您已经定义了装饰器和参数
下一步是创建实际对应于您的任务的Python函数
最终创建传感器和每30秒检查的条件
直到传感器在5分钟后超时
所以让我们说任务应称为is_api_available
并且该任务应返回特殊对象book_return_value
因此您必须导入poreturn_value
在装饰器类型之后从airflow.dot.sensors.base导入poreturn_value
然后您想要定义条件以验证API是否可用
但是您不想在代码中硬编码API
因此我们将创建一个连接
然后转到UI并转到管理员
添加一个新记录并为连接ID输入stock_api
然后连接类型是HTTP
像这样并且主机是API的这一部分
因此您复制所有内容，包括结尾的斜杠
非常重要
好的 确保结尾有斜杠
并且这里没有拼写错误
然后对于额外参数
您想要具有以下JSON值
这非常重要，您不能在这里出错
对于端点
确保结尾有斜杠
确保端点结尾有斜杠
然后对于标头
确保您的内容类型
用户代理和接受完全按照这里写的
也确保您所有的逗号都如这里所示
好的 这非常重要
如果您在这里有任何错误
您将无法连接到API
然后您可以点击保存
所以现在您有连接
您可以回到代码中，首先您刚刚创建的连接
您可以使用基本钩子对象
所以在文件的顶部
导入airflow.hooks.base_hook
Bayes导入bayes_hook
然后在任务中
所以在Python函数中
您可以输入api等于bayes_hook
获取连接
然后传递连接
Id stock api如定义在di中
您可能会想知道什么是钩子
嗯 将钩子视为一个特殊的类
它封装了与给定工具交互的逻辑
给定服务
例如，您想与Postgres交互
您有Postgres钩子
您想与
您有egress s钩子等等
避免重复发明轮子
每当您想与给服务或工具交互时
从airflow，airflow有许多钩子
如果您对此感兴趣
我强烈建议您查看钩子文档
所以，既然说了
在获取连接后
我们可以创建我们要验证的URL
为此我们可以使用连接
所以这里api主机
然后api. extra json
记得这是连接的额外参数的json值
所以这就是我们的json
然后，我们希望使用这里的端点
像这样，也要打印URL，以确保URL正确
您将能够在任务的日志中看到它
现在您有了URL
下一步是向该URL发送请求
到该API
为此您可以使用requests
所以将requests导入到任务中
因为我们使用requests
仅在此处
所以输入导入requests
然后在这里你可以输入response
等于requests.get（derurl）
并传递您在连接中创建的标头
点额外的dion再次
如果您查看API
然后删除所有后的trot
像这样
您可以看到，我们得到finance，然后result等于null
所以这意味着如果我们能够获取finance和result等于null
那么API可用
这就是我们在传感器中的条件
所以让我们回到代码编辑器
然后条件等于response.json（）
因为我们从pi获取json值
我们查找finance和result是none
所以如果它是none，这意味着API可用
好的，最后
但并非最后，我们需要返回poke
返回值已完成条件
最后xcom值
所以如果我们想从这个任务中分享一个值
如果我们想从这个任务中导出一个值
在这种情况下，我们可以导出URL，这样下一个任务就可以使用它
最后，您只需调用Python函数is_api_available
如果您不这样做
您将无法运行任务
现在您只需设置文件
您已成功创建了第一个股票市场数据管道任务
所以现在问题是
我如何验证
任务是否工作
嗯 猜猜看，您有一个评论用于这一点
在终端中，您输入astro dev
运行任务
然后测试dag_id的任务您要测试是，所以这种情况下股票市场
然后是您要测试的任务id
这是is_api_available，最后
但并非最后，一个日期
让我们说 一月一号2025年
输入并等待一会儿
如您所见，任务已成功执行
这意味着API可用
您也可以验证URL是否正确
所以请确保与我相同
如果你这里只有一个斜杠
那没问题
但确保最后有一个斜杠
再次 恭喜你
你现在能够实现一个传感器并等待条件为真
在继续下一个任务之前
顺便说一下 传感器任务只是众多传感器之一
气流带来的
如果你去以下链接 regiread
那个天文学家 Io 然后搜索传感器
你会看到气流有小型传感器
例如
假设你正在等待来自 databricks 的分区
你可以使用 databricks 分区传感器
如果你正在等待
例如 emme 或者 job
你可以使用 emr 基础传感器等等
再次 这只是一个传感器
Airflow 有许多传感器，你不需要实现任何逻辑 逻辑已经预先为你构建
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/22_Udemy - Apache Airflow The Hands-On Guide p22 11. Fetching stock prices with the PythonOperator.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们已成功实现了API可用性
下一个任务是创建抓取股票价格的任务
正如你可以猜到的
这个任务将抓取特定公司（如英伟达）的股票价格
从财经API获取
让我们回到数据管道
在传感器之后
让我们使用Python操作符创建一个新任务
Python操作符帮助你运行一个Python函数
首先在文件的顶部
让我们导入Python操作符，从Airflow操作符中导入Python
导入Python操作符
然后在传感器之后，在这里创建一个新的任务
获取股票价格等于Python操作符
然后任务获取股票价格和Python可调用函数是下划线
获取股票价格
你可能想知道我们为什么使用传统操作符
而不是Python操作符，因为
因为我们稍后在项目中将使用Docker操作符来运行Spark作业
问题是将装饰器与Docker操作符混合使用有点奇怪
尤其是在依赖项方面
这就是为什么在测试视频期间
我建议你使用装饰器或传统操作符
但避免混合使用两者
因为再次强调这可能会很棘手
尤其是当你需要做出区别时
任务之间的依赖关系
这就是为什么我们更喜欢使用Python操作符
而不是使用装饰版本的那点，我们需要创建下划线
获取股票价格的Python函数
但我们不会黑箱式地创建这个Python函数
为什么 因为那样可能会变得非常大
尤其是如果你把所有的Python函数、SQL请求等都放在里面
所以作为最佳实践
利用Astro Chi提供的功能会更好
包括文件夹
你将在那里放你的Python函数
SQL请求
等等
然后你会在你的dag文件中调用它
这样你就可以让你的dag尽可能的干净
所以在include文件夹中
打开它并创建一个新的文件夹
股市
和一个新的python文件tasks
让我们创建一个python函数_
获取股票价格
然后我们需要创建一个新的变量对应于url
找到特定公司股票价格的地方
所以，如果你看一下财务的原因
你可以看到，我们有链接
URL和公司，公司的符号
最后，为了获取股票价格，我们需要一切
让我们这样做
回到您的公司
然后创建一个新的变量url
这是财务的URL
然后是对应公司的符号
最后，为了获取股票价格，我们需要一切
但现在你可能会想知道这些参数来自哪里
URL来自之前的任务
记住，这是API可用
所以这
然后符号实际上是一个变量，我们需要在deck文件中创建，对应于公司
所以让我们回到dag文件，文件的顶部
我们可以创建一个新的变量symbol等于，让我们说nvidia
然后为了将值传递给_
获取股票价格
我们需要在python操作符中使用一个新的参数
op key works
这个参数接受一个字典，该字典与python函数中的参数相匹配
您从python coable参数调用
在这种情况下，因为我们有两个参数URL和符号
那么我们需要一个包含URL和符号的字典
我们在这里做什么
嗯，首先在这个字典中
我们有一个键 URL映射到python函数_get stock prices中的参数URL
并且值有点奇怪
这就是在airflow中称为模板的东西
所以你可以看到这里有两个对括号
这意味着您在括号之间的内容将在运行时被评估
所以只有在任务运行时
并且这意味着我们要从任务中拉取xcom
id是API可用
所以我们想要获取由API可用在这里推送的xcom
所以URL
以便在任务中使用
获取股票价格
这将在运行时由URL替换，当任务运行时
最后，我们有一个键
这是符号，其值是我们在这里定义的符号
模板是airflow中一个非常强大的概念
我强烈建议您查看文档
但是别担心 我们将在后续课程中使用模板
说到
让我们回到python函数
在这里，我们希望获取连接栈
我们之前定义的API
所以API等于基本钩子
无法获取连接
然后重新安装股票API
我们需要导入基础钩子
因此，从 airflow 点 hook
点湾
导入基础钩子
然后我们可以创建一个新的变量response
等同于请求
点 获取 URL
就像我们之前所做的那样，我们传递头信息
头信息等于 api. extra dj
像这样的头信息
好的
显然，你需要在这里导入 requests
最后，我们可以从 json 值中返回股票价格
从响应的 json dumps 中
响应的 json 并且我们希望获取股票价格
所以，我们需要访问图表结果，并且数组的第一个结果
让我们在这里做
Trot
然后结果
最后，数组的第一个索引，不要忘记导入j ok
所以现在函数已经准备好了
我们在这里做的事情
这很简单，对吧
我们只是从API获取股票价格
然后获取值
所以回到dag，你需要导入下划线
获取股票价格
为此你输入from include stock market tasks
导入下划线
获取股票价格，就像那样你成功地创建了第二个任务
获取股票价格，从中获取特定公司的股票价格从wii finance api
现在验证这个任务工作
你不能使用与is api available相同的命令
为什么，因为这个任务依赖于is api available
如你所见
我们需要来自is api的数据
所以在这种情况下，你需要在is api和获取股票价格之间建立依赖关系
那么我们来做这个
你使用正确的p shift运算符is api available
然后获取股票价格，表示你想要先运行is api available
然后再获取股票价格
记住，使用传统运算符时
你不必显式地使用括号调用
而每当你使用装饰器时
你必须使用括号，就像这里显示的那样
好的 那么，话虽如此
保存文件 然后在终端运行另一个
这是astro dev
Dags测试
然后dagger id
这是带有过去日期的股票市场
例如2025年1月1日，按回车
让我们看看dagger是否按预期工作
正如你所见，所有任务都已成功执行
你可以看到dagger运行成功标记
如果你仔细看看日志
你可以看到任务
获取一些惊喜，这已成功执行
更重要的是
如果你往上滚动一点，你可以看到
所有这些数据都与从wi finance api推送的股票价格相对应
所以你能够从airflow metadatabase中获取特定公司的股票价格 所以，你能够从airflow metadatabase中获取特定公司的股票价格
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/23_Udemy - Apache Airflow The Hands-On Guide p23 12. Storing stock prices in MinIO (AWS S3 like).ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


您已成功实现获取股票价格
现在是时候转向下一个任务了
那就是股票价格
在那里你将学习如何在minao中存储数据
这相当于aws s3存储桶
让我们开始回到我们的数据管道，获取股票价格之后
您可以创建一个新任务存储价格
所以停止价格等于python运算符
再次接受任务id
这是您的任务存储价格的唯一标识符
然后一个python电缆函数
这是_存储价格
让我们创建相应的python函数
为此您前往包含，然后股票市场
然后在任务文件中您创建一个新的python函数之后_
获取股票价格 这是存储价格
所以现在因为我们想在minao中存储数据
我们需要首先连接到minao
你知道如何做
您可以重复在这里所做的，并将其粘贴在那里
并重命名api为minao，您想要获取minao连接
所以我们需要创建该连接
让我们去dfo ui
然后前往管理连接
添加一个新记录
然后对于连接id
它是minao
对于连接类型
查找亚马逊网络服务
然后访问密钥id是minao
然后秘密访问密钥是minao123
这就是您可以在docker compose文件中找到的内容
如果您查看minao服务在这里
您可以看到根用户是mina you
并且密码是mina you123
这正是我们在这里使用的
对于额外参数
您需要指示以下端点
连接到minao的位置
这是此端点url
然后传递值http mino 90 0
好的 确保您在这里没有输入任何错误
然后点击保存
您已成功创建minao连接
如果您想知道这是从哪里来的，嗯
这指的是您在docker compose文件中的服务minao在这里
所以请记住，您docker compose文件中的每个服务
可以在同一环境中在同一网络中相互通信并运行
为了验证您可以访问终端，
然后输入docker网络
您可以看到不同的网络
您将得到一个名为_nds_net的网络
所以复制这个网络的ID，然后输入docker网络inspect
然后输入网络的ID
现在如果您滚动查看
您应该能够看到不同的服务
正如您所看到的，car proxy
然后我们有minao
然后我们有spark master
然后我们有spark walker等等
所以再次确认所有服务都在同一个网络中
如果您无法看到某些服务，例如minao
在这种情况下，您需要执行其他操作，即运行docker网络命令
然后连接网络ID和容器ID
您要连接到该网络，并且要找到容器ID
您可以去docker桌面
如果您展开
您将看到不同容器ID在这里 好的
所以再次 确保所有容器都在同一个网络中运行 否则将无法工作
回到获取minao连接的Python函数
下一步是创建minao客户端
在这里您输入client等于minao
然后它需要几个参数
端点URL
mino extra
在这里您查找从创建的连接中查找端点URL
像这样，并且我们只需要主机
然后访问密钥，即在登录中
然后是秘密密钥，即mina密码
我们可以将secure设置为false
因为它不是生产环境中运行的项目
显然我们需要导入minao，所以文件的顶部输入
从minao中导入ayo
我们需要安装该包
minao Python包
为此您可以去requirements
在这里输入minao等于等于版本7.2.14
然后保存文件
然后在终端中重启环境astro dev
以安装minao包
让我们等几分钟
现在项目已经重新启动
完美，确认容器正在按预期运行
为此您去docker桌面
您应该看到所有容器正在运行，如此处所示，回到Python函数
下一步是创建一个桶
如果你去 minao 并且访问本地主机 90 或 1
然后输入用户名minao和密码minao一二三
正如你所见，你还没有桶
正确 就像如果你点击桶的话
你没有任何桶
因此我们需要先创建一个桶，以便存储与股票价格相对应的数据
创建一个新的变量桶名称
等于股票市场
好的 确保你不做任何类型的这里
然后
如果不客户
那个桶存在
与桶名
在这种情况下 我们想要创建桶
所以客户制作桶
和桶的名称
现在我们可以确定桶将存在
下一步是将数据加载到该桶中
但是数据在哪里
我们需要从之前的任务中获取数据
获取股票价格
所以第一步是向股票价格添加新的参数
即股票
然后像之前任务一样
获取股票价格 正如你看到的这里
我们将使用此参数 op key walks 与其几乎相同的值
但这里而不是 url
我们想要股票
因为这里是我们作为参数的 _prices
然后我们要从 get stock prices 拉取数据而不是 is api available
我们可以删除这一部分
好的 所以再次 记住 get stock prices 返回或创建 x come with the stock prices
我们将要存储在minao桶中的数据
现在我们有数据
我们可以将其转换为一个字典，其中股票等于json.loads股票
然后我们提取公司的股票符号stock['mea_symbol']
因为我们将使用该变量在桶中创建一个文件夹
使用公司的股票符号
然后我们将部分数据转换回字符串
使它们等于json.dumps
股票
然后确保使用ascii强制使用uf进行编码
现在数据已经准备好
我们可以创建一个实际的文件来存储数据
所以我们用新变量来做
og w等于客户端
我们调用put object函数
我们要存储数据的桶名称
对象名称是
符号，这将是文件夹
然后是文件prices.json
prices.json包含股票价格
这个文件将存放在相应的公司符号文件夹中
然后there等于bytes
I O
和数据
最后数据的大小是length等于len data
最后导入bytes
I O 顶部
所以from io导入bytes
I You
像这样最后
但并非最后 我们可以返回存储在桶中的文件路径
然后obg w点bucket name
斜杠
符号，就是这样
到这一步你已经成功地创建了python函数
下划线stock prices连接到mino
验证stock market桶是否存在
如果不存在 则创建桶
然后加载之前的任务中的数据
获取股票价格
最后创建一个新文件
在包含公司符号的文件夹中存储股票价格数据
并返回该文件的路径
现在我们已经完成了，你可以回到数据管道
确保在score store prices中导入python函数
在文件顶部，get stock prices下方
导入store prices
现在我们可以添加
我们知道他们的依赖
在get stock prices之后
执行store prices
好的 让我们验证任务是否正常工作，因为我们做了很多工作
并且为了这个第一个
如果你去minao
确保你没有任何桶
好的 正如你所看到的，没有桶，完美
然后回到你的终端
运行以下命令
这是adevon dags
测试股市
好的 正如你所做的那样
带有过去日期
eat under并稍等片刻，看看任务是否工作
正如你所见，dig已成功执行
事实上，如果你往上滚动一点
你可以看到任务存储价格返回值股市nvidia
让我们在minao验证一下
回到me 现在刷新页面
你可以看到一个新的桶
股市点击它
你可以访问一些详细信息
但如果你去对象浏览器然后点击股市
你有一个新的文件夹
Nvidia 这是股票符号
点击它 你有文件价格，正如预期的json
祝贺你 你已成功创建了一个新的任务来存储数据
这意味着你喜欢一个地址
S三桶 使用python操作符
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/24_Udemy - Apache Airflow The Hands-On Guide p24 13. Formatting stock prices with Spark and the DockerOperator.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


到这个点你已经成功实现了股票价格任务
现在是时候转向下一个了
那就是格式化价格
那一个会很有趣
你将学习如何使用Docker操作员运行一个Spark作业
如果你不知道Spark是什么
把它想象成一个数据处理框架
你可以处理TB或GB的数据
打开文件夹 Spark
然后笔记本和股票转换
正如你所见，你打开了几个文件，股票转换
Py 这是我们为了格式化股票价格而运行的应用程序
我们将基本读取json文件
价格json并将其转换为具有一些特定列的csv文件
让我给您一些快速的解释
首先，我们使用许多参数创建了一个Spark会话，正如您在这里看到的
更重要的是
我们连接到minao
这就是你在那些线条上看到的
下一个 我们用spark.read读取json文件，读取prices.json
如你所见 路径由一个特殊的可变提供
这是一个环境变量
并且这个路径将是由前一个任务存储价格产生的xcom值
记住，前一个任务存储价格产生了这个xcom值，值为stock market
这是我们在mineo/nvidia中的桶
对应于价格那个json在minau的符号
好的 再次
这将在运行时由这个值替换
这个值来自之前的任务
接下来 我们在这里做很多事情
这不太重要
但我们正在读取json数据
然后选择一些列
最后 但是，最后我们将数据写入这个位置的csv文件
再次 与上次相同的位置
所以股市/英伟达
但这次文件将格式化为价格
这将是一个带有标题的csv文件
现在 你可能说 好的
让我们把这个放在一个python函数中
然后从我们的dyke中运行它
但那不会起作用
原因是Spark未在我们的Airflow环境中安装
所以如果你尝试这样做
你会得到与那些依赖项相对应的错误
那些不存在的Spark依赖项
这就是为什么我们不必混合Spark依赖项
顺便说一下
Spark需要许多依赖项来运行
所以，我们不必混合Spark依赖项和Airflow依赖项，这可能会导致冲突
我们更喜欢使用docker并将脚本库存转换为docker镜像
在docker镜像中使用py
最终使用spark依赖项构建docker镜像
得到一个可以运行stock transform的docker容器
我们将从airflow任务运行这个docker容器
再稍微澄清一下
假设我们有这些数据管道
任务a，然后另一个任务
任务b是docker操作员
所以这一项运行一个docker容器
好的 所以这是docker操作员
然后我们有一个docker镜像
包含spark依赖项
好的 所以你需要 你有spark依赖项
你还有脚本
也
所以我们想要运行的Python脚本，我们将要做的是
我们将构建那个Docker镜像以获得一个Docker容器
那个容器就像一个应用程序，我们将从Docker操作员那里运行它
从我们的任务这里
好的 所以这个任务将触发容器以使用Python脚本格式化股票价格
好的 这就是它工作的方式
基本上，当我们运行一个容器时
所有依赖项都封装在那个容器中
这不会影响到我们的空气流环境
好的 首先，我们需要根据这个Docker文件构建Docker镜像
为此，我们需要打开终端
然后进入spark文件夹
然后进入notebooks文件夹，然后在stock_transform文件夹中
执行docker build -t命令
然后进入airflow文件夹
然后进入stock文件夹
好的 所以请确保在这里不要出现任何打字错误
并且句尾的句号
它会进入并稍等片刻直到图像构建完成
好的，完美
您可以验证图像已成功构建在docker中
所以如果您打开docker桌面然后转到图像
您应该能够看到airflow斜杠stock up
正好如这里所示
现在打开dag并之后存储价格
我们将创建一个新的任务
它是格式价格
而这一个使用docker operator
因为我们想要运行一个docker容器
对应于格式股票价格的spark脚本
首先任务id
这是任务的唯一标识符
再次它是格式价格
然后图像ok
我们要运行的docker图像
这正是我们构建的airflow斜杠stock dash up
然后容器名称
好的 所以容器的名称应该是什么
在这我们想要称之为格式价格
然后我们有api版本到auto
然后auto remove
因为我们想要删除docker容器一旦任务完成
所以让我们说true
然后docker url以连接到docker
我们使用tcp docker dash proxy与以下部分
二十三
七十五
而这个docker proxy是我们在docker compose文件中的服务
所以如果您向下滚动一点
您可以在这里看到 docker proxy
好的
然后在那之后
我们指定网络模式
我们希望使用现有docker容器中的同一网络
所以让我们使用这个spark master容器
好的 所以再次 我们将使用与spark master容器所使用的同一网络
然后t y等于truex
com all等于false因为我们不想产生任何x comes
然后and tp
D等于false因为我们不想挂载临时目录
最后的参数是环境
这是一个字典
并且这个字典有键spark应用 o
值是
这是来自前一个任务的xcom
所以ti点xcom pull
任务是
存储价格
好的 所以我们想要的这里
我们想要的值
存储价格的xcom值
这个值是价格存储在json的路径
记住spark应用是不可用的
那个python脚本用来找价格
点json在miao bucket
好的而且事实上 如果你打开docker文件
你会看到默认是这个值
但是我们会传递这个
好的 所以现在你有了corporator和任务准备好了
下一步是导入docker operator
所以去文件的顶部
然后输入from airflow dot providers
Dot docker operators
Docker导入docker operator
如你所见这里
我们导入了docker operator从providers
所以我们需要安装相应的provider
记住一个provider有额外的operator
所以你可以与一个给定的服务或工具进行交互
在这个情况下你想要与docker进行交互
所以你有docker provider
如果你要与snowflake进行交互
你有snowflake provider等等
所以我们安装这个provider
你去requirements
然后你输入apache dash airflow
Dash providers
Dash docker 然后你使用这个版本为o o像这样
然后你保存文件
好的 到这个点你已经成功创建了任务格式prices它会运行一个docker容器
它会运行在spark中的这个python脚本
为了格式化价格并在最后得到一个csv文件
别忘了在依赖项中添加format prices在stock prices之后
保存文件 现在让我们在终端中验证它是否工作
回到项目
这是空气流动文件夹
然后使用astro dev重启astro环境
重启，因为我们想安装docker提供商，好的现在
Airflow又运行起来
让我们用以下命令触发其他数据管道，设置为true
开发运行
然后运行dags测试
股票市场和过去的日期
进入
正如你所见
我犯了个错误
这是网络节点
应该是网络模式
所以在代码中修复那部分，同样的对于自动删除选项
这不应该是真
但这应该成功像这样
好的 所以保存文件
现在让我们再运行一次
好的 看起来有些事情正在发生
让我们打开看看顶部，看一下容器
你可以看到容器格式
价格对应于我们的应用程序Airflow股票应用
所以我们等它完成回到终端
你可以看到DAG已成功执行
如果你看看docker桌面
docker容器不见了
所以让我们看看minao
所以让我们看看对象浏览器
然后股票市场然后nvidia
正如你所见，我们为新价格文件夹
我们在里面有csv文件
祝贺你
因为你知道如何在隔离环境中运行Spark作业
使用docker容器 最终你知道如何使用docker运营商运行docker容器
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/25_Udemy - Apache Airflow The Hands-On Guide p25 14. Fetching formatted prices from MinIO (AWS S3 like).ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们在minao中有格式化的价格
我们用spark完成了这一点
下一步是创建任务
获取格式化的csv文件，它将拉取csv文件
将minau中格式化的股票价格引入我们的数据管道
让我们开始吧 好的
回到数据管道
在任务格式化价格下方滚动
创建一个我们称之为的任务
获取格式化CSV
这与Python运算符相同，因为我们想要再次运行Python函数
然后，这个Python运算符的任务是获取格式化CSV
并且Python可调用函数将是_获取格式化cg
好的 所以现在我们有打开任务文件，里面有所有任务
然后_股票价格
创建一个新的Python函数
_获取格式化CSV
并且那个python函数需要一个路径作为参数
那个将是格式化价格的文件夹的路径，里面有csv文件
所以让我们设置参数来传递路径值
所以返回到数据管道
在这里 就像你之前所做的那样，我们将再次使用键行走
这是一个字典
键是path，值是ti点x come pull
所以我们想要拉取x come从任务中
存储
价格
可以 现在我们将拉取xcom，值为以下内容
记住，这是股价回报的值
这将是文件夹格式化的价格路径
最终是csv文件
如果你去minao
你将能够看到股票市场/nvidia格式化的价格
然后csv文件就在那里
让我们回到任务，从minao拉取数据
我们需要再次连接到 minao
你知道怎么做
你需要创建一个客户
但是而不是再做那件事
我们可以将这段代码导出到一个我们将其称为“下划线”的功能中
获取你的客户
然后我们在这里粘贴代码并返回客户
好的 所以我们在这里可以直接使用这个函数
客户等于让你成为我的客户，并在这里做同样的事情
别忘了在函数中添加mina
就像那样在你的基础钩中使用它
好的 而不是硬编码相同的代码两次
我们可以使用这个函数并按需在任何地方调用它来获取mina你的客户端
现在我们在你的客户端中有它们
下一步是创建前缀
这基本上是在mina中找到csv文件的路径
为此 让我们创建一个新的变量prefix_name等于path
然后分割/格式化价格
现在我们可以使用minayour客户端的特定功能
以便列出对象
以便列出我们在这个位置的文件
所以这里输入objects
等于客户端列出对象
然后我们需要传递桶
但是再次 因为我们已经得到了桶
让我们在顶部创建一个全局变量
所以这里bucket name等于stock market
然后这里使用bucket name
像这样 然后我们可以在这里使用bucket name
然后我们传递prefix
一个prefix名称
recursive设置为true，因为我们想列出所有文件
如果在formatted prices文件夹中有文件夹
我们也想列出它们
然后我们只需要检查到目前为止的对象
如果 og.dot object name 以 that csv 结尾
如果文件 如果当前对象是我们正在寻找的 csv 文件
那么我们想要返回
odt object name
最后如果我们无法找到 csv 文件，原因不明
我们希望返回一个异常
这是 airflow 未找到异常，带消息
csv 文件不存在
显然，我们必须导入空气流不找到异常
所以在文件的顶部，您输入从空气流通异常
导入空气流不找到异常
就像那样，您已成功创建了任务
从CSV获取以从minao拉取SV文件
以便稍后在数据管道中使用
所以让我们回到数据管道
在这里，我们还需要导入下划线获取格式cz
所以在文件的顶部，在下划线价格之后
导入下划线获取格式它cv
然后你想要运行获取格式化csv，并在格式化价格之后
然后你就可以验证了
如果数据管道工作正常
这相当直接
你需要去终端，然后运行以下命令，就像运行dag一样
测试 股市
并输入一个过去的日期
让我们等一会儿
确保你已经保存了文件
再次 spark作业将运行
所以它需要几分钟才能完成
如你所见 dag已成功执行
实际上 如果你稍微往上滚动
你将能够看到获取格式化的csv返回了以下值
这与你在miao中的csv文件相对应
所以恭喜你
现在你知道如何从minao桶中拉取数据 现在 你知道如何从minao桶中拉取数据
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/26_Udemy - Apache Airflow The Hands-On Guide p26 15. The best way to load files into data warehouses with Postgres and Astro SDK.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 是时候实施项目数据管道的最后任务了
那就是加载到dw
为了将数据从CSV文件加载到我们的数据仓库中
这是我们的数据仓库
让我们开始吧
好的 因为我们想把数据加载到我们的数据仓库中，这是我们的postgre
我们需要创建相应的连接
为此，您转到FLI，然后转到管理员和连接
然后你添加一个新的记录，连接id
它是postgre
然后连接类型是postgre
主机是postgre
登录是postgre
密码是post gray
并且端口是54
32 然后你点击保存，就像那样
你可以连接到你的postgres数据库
为了将CSV文件中格式化的股票价格数据加载进来
我们将使用特殊库中的一个特殊操作符
那就是astro sdk
SDK允许使用Python和由Airflow驱动的SQL进行快速干净的ETL/ELT工作流开发
并且这是完全开源和免费的
所以任何人都可以使用这个库
基本上，astro sdk带来了一些操作符，这将使你的生活更轻松
正如你所看到的这里
你有一堆操作符
你又有了一堆操作符
我强烈推荐你查看文档
但现在我们将专注于一个
那就是加载文件操作符
如果我们看一下
我们有几个用例
我们的用例如下：将文件加载到数据库表中
正如你所看到的，它非常简单
您只需使用加载文件操作符创建一个任务
然后传递输入文件，其中包含您想要在数据库中加载的数据
在我们的情况下，是minao的csv文件
格式化后的股票价格
然后你定义你想要将数据加载到你的数据库中的哪个表
就是这么简单
这正是我们现在将要使用的
回到项目中使用sdk
你需要首先安装它
但由于我们使用cli
它已经安装好了
这里不需要做任何事情
我们需要做一些导入
因此，文件顶部
在这里，来自真实的docker操作员类型
导入 SQL 作为 QL
然后从 astro 点 files 导入 file
然后从一个真实的dot sql表格导入表格和元数据
好的 所以现在你有了那些导入
在文件的底部进行操作
然后得到格式为tcsv的文件
创建一个新任务，并将其命名为“加载到dw”
等于ql加载文件
运算符
然后任务我做了任务的唯一标识符加载到dw
然后你需要指定输入文件
你想要加载到你的数据仓库的文件
到你的数据库
所以输入文件等于文件对象
在这里有两个参数
第一个是路径
这是minao中csv文件的路径
所以这里我们放一个三
然后桶的名称
所以确保你导入了桶的名称
这里
然后斜杠和这里
确保你有正确的括号数量
所以1 2 3和4
好的 所以你必须有四个括号像这样
然后使用任务ID调用xcom拉取
等于获取formati csv
好的 记住获取format是csv会返回csv文件的路径
这正是你现在拥有的
第二个参数是ID
对应着minao的连接
否则加载文件操作员无法到达minao
无法从你的桶获取文件
所以把mina你放在这里
好的 现在我们有了输入文件
下一步是定义加载数据的位置
为此，您可以使用输出表参数，该参数接受一个表对象
这个表对象有几个参数
第一个是表的名称
所以我们想在Postgre上创建一个名为stock on the market的表
然后是连接到Postgre数据库的数据库ID
所以这里是postgre
然后是元数据
在这里我们将使用元数据对象，并仅传递 schema public
好的 我们希望在哪个 schema 中创建该表
我们希望在 public schema 中创建表
好的，就是这样
好的 一旦你有了那个
下一步是定义加载选项
所以加载选项等于以下字典
所以 aws access key id 等于 base hook 点 get connection
然后你登录
然后你必须定义aws秘密访问密钥，再次使用基础钩子
但这次你输入密码
最后，终端点URL
这仍然是基础钩子
但这次是主机
所以现在你已经准备好验证任务是否正常工作
别忘了在获取格式化CSV后编辑
所以加载到dw
保存文件并转到您的终端
然后按照以下命令运行以验证DAG是否正常
让我们稍等片刻
因为所有任务都将被执行
好的 到目前为止，一切顺利
不 你罗
如你所见 该应用程序在Spark中运行
如果你仔细查看日志
你可以看到 DAG 已经成功执行
这意味着你已经成功将 CSV 文件中的数据加载到 Postgres 数据库中
我们可以通过打开 Docker Desktop 来验证这一点
然后打开你的项目，展开它并点击 Postgrecontainer
然后转到 exec
在这里输入 slash bean
Slash bash
现在输入 psql -U postgre
然后输入
Select * from stock_market 你可以看到预期的数据
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/27_Udemy - Apache Airflow The Hands-On Guide p27 16. Creating the dashboard to track Apple stock with Metabase.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 你将构建一个仪表板来监控你的数据仓库中存储的给定公司的股票价格
让我们开始吧
首先 前往本地主机
30.0
然后点击
让我们从这里开始
你选择语言英语
然后你输入你的名字
这不重要
然后对于电子邮件地址，再次不重要
所以
让我们说gmail.com 实际上
我将输入我的 然后你创建一个密码
公司名称
让我们说by name
在这里让我们创建这个密码
点击下一步
然后你不在乎那个
所以让我们说不确定点击下一步
然后你选择数据库put gray
然后显示名称
让我们说dw
我们的房子
主机是host.docker 点互联网
然后部分是54
32
数据库名是postgrethe
用户名是postgreand
密码是postgreand
对于模式
你可以保持所有 然后连接到数据库看看它是否工作
你可以禁用那个点击完成并带我到meta base
现在你准备好构建你的仪表板了
meta base的工作方式是，你需要创建问题
你对数据想知道什么
要创建一个问题
你到右上角
你点击新
然后问题 然后选择表dw
然后选择底部的股票市场
好的 现在
让我们说你想知道过去一年中给定股票的平均收盘价
为此 你可以选择一个函数或指标
然后计算平均值和收盘价
因为你想知道过去一年的平均收盘价
然后点击可视化
你可以看到nvidia过去一年的平均收盘价
然后点击保存并命名为平均收盘价
然后点击保存
让我们将其添加到仪表板中
所以我们要创建一个新的仪表板
在这里创建一个新的仪表板
我们称之为talk仪表板
创建并选择股票仪表板
现在我们有了预期的平均收盘价
这是一个指标
但让我们再添加一个
点击保存以保存仪表板
添加一个新的问题
然后选择股票市场
现在我们想要了解交易量
交易股票的数量和收盘价
为此点击可视化
这里有数据
在左下角点击可视化并选择组合
然后设置
在这里可以删除时间戳
最高价 最低价和开盘价
现在你可以看到每天的交易量
你可以在这里看到
以及收盘价
你可以看到蓝色的线条
然后点击完成
然后保存并命名为交易量与收盘价
点击保存并将其添加到现有仪表板股票仪表板
让我们使可视化更大一些
现在关于
如果我们想要了解过去一年某个股票的平均交易量
所以在这种情况下点击保存以保存仪表板
点击新建 然后点击问题
点击股票市场
然后选择功能指标
让我们选择平均值
这次我们选择交易量可视化
现在你可以看到nvidia过去一年的平均交易量
在我们这个案例中然后点击保存
我们称之为平均交易量
然后保存
将其添加到仪表板
股票仪表盘
现在我们这里有平均成交量
就像这样 您可以探索股票价格
您在元数据仓库中的数据使用元数据
不要犹豫尝试不同的指标 我期待下一期视频
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/28_Udemy - Apache Airflow The Hands-On Guide p28 17. The pipeline in action!.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


恭喜你
如果你正在观看那个视频
这意味着你已经成功完成了项目的所有任务
现在是时候看看它在行动中的表现了
为此，你去到fl
然后点击dag股票市场
如果你去图表
你应该能够看到如此处示的数据管道
漂亮，对吧 不是吗
你可以看到我们有容易
I可用 然后获取股票价格
然后获取股票价格
格式化价格
从atcsv获取并加载到dw
这将创建一个数据集
可以将其视为你将能够稍后使用的一组逻辑数据
如果你想根据这个触发你的数据管道
所以现在让我们从ui运行它，看看它是否起作用
你只需要在这里启用开关并刷新页面
你可以看到dag正在运行
让我们等一会儿，完美
格式化价格正在运行
这将花费一些时间
因为Spark在后台运行
记住，这是一个Spark作业
好的 从atcsv获取
最后加载到dw
恭喜你 dag你完成了
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/29_Udemy - Apache Airflow The Hands-On Guide p29 18. Getting alerts on Slack with the new Notifiers.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在生产中，你想知道你的狗是否成功
但更重要的是，你想知道它是否失败
这就是你必须定义通知的地方
Airflow有许多回调函数，你可以使用它们来接收通知
如果一个任务成功或失败
如果一个DAG成功
或失败 如果一个任务被重试，等等
如果你想使用Slack接收通知
例如，你必须使用Slack Web Book 操作符
然后定义一些参数并在回调中调用那个操作者，那真是太棒了
但如果您需要在其他地方使用此逻辑
或者如果您想更改通知方式
那么您必须编写大量代码
或者至少有点乏味
这就是为什么在2007年有一个新概念
通知器是这样想的
通知器是一个封装了通过特定系统获取通知的逻辑的对象
例如，您想使用slack接收通知
然后你将拥有松弛通知器
你想通过Discord接收通知
你将获得Discord通知器
通知器的一个好处是你可以创建自己的通知器
只需通过扩展提供基本通知发送结构的基础通知器类
使用各种回调
成功回调
失败回调等等
这视频有点超出范围
但你可以看到你可以很容易地创建自己的通知器
如果你想使用通知器
就像你看到的那样，你有一个名为我的通知器
你可以在不同的回调中使用它
成功回调
失败回调等等
如果你想改变你使用的通知系统
例如，你想从slack切换到teams
你可以通过替换通知器来做到这一点
就是这样 这非常容易
它就像一个即插即用的通知系统
现在你有了预构建的通知器供你使用
如果你查看文档
你会看到这些准备好的通知器列表，可以在你的任务图中使用
例如，如果你想使用亚马逊的sns或sqs
你有sns通知器和sqs通知器
你想通过teams接收通知
你可以使用uprise
或者如果你想使用slack
那么你有slack通知器，这就是我们将在我们的deck中使用的
如果你想要跟随我在这个视频中展示的内容
你需要创建一个slack账户
如果你无法创建slack账户
我将无法观看视频
因为它几乎与所有其他通知器相同
一旦你有了slack账户
下一步是安装slack提供商
记住，airflow有许多提供商
你需要安装一个提供商
如果你想与特定工具交互
正如我们希望与slack交互
我们需要安装相应的提供商以获取slack通知器
以及操作员等
找到这个提供商非常容易 你只需点击这里，然后复制这个
你前往你的命令行在requirements中
添加带有版本号的行
你可以使用八一零像这样
你保存文件
然后你重启f
例如，现在安装提供商
f正在运行 你已经安装了slack提供商
下一步是配置你的slack
以便你可以在以下工作区 airflow课程中接收通知，在这个特定频道
让我们这样做
前往你的网页 进入api slack
点com slash apps这里
你需要从零开始创建一个新的应用
我们称之为通知并选择工作区
我有一个工作区 airflow课程
但显然选择你的工作区并创建
然后在添加功能和功能下
你需要给予权限
在权限下
添加一个新权限并查找聊天权利，同时也查找聊天公共权利
就这样
slack通知器将能够与你的slack频道通信
你必须在你的工作区中安装应用程序
安装工作区allo权限
这将给你提供一个你将使用的token
在我们即将创建的slack连接中
在you i中，前往管理与连接
然后添加一个新记录
称之为slack
然后寻找连接类型slack api
你看到你有slack api token可以提供
回到你的应用
复制token
并把它粘贴到这里
然后点击保存下一步
返回您的代码编辑器并在您的dag文件顶部
导入slack通知器from airflow providers slack
Slack on score notifier
导入slack通知器
然后你可以使用这个对象为您的不成功回调
所以使用slack通知器
指定slack连接id
这是slack在我们连接中定义的
要发送的通知
让我们说dag股票市场已成功
频道是通用的，同样适用于不熟悉的回调
您可以只需复制并粘贴此通知器
但是改为失败而不是成功
好的 所以让我们验证它是否起作用
第一步是检查在for ui中您没有错误
您看到没有错误
是时候找出dag
并验证我们在slack上收到通知
选择dag并且如果这里切换是关闭的
您只需将切换打开
否则您可以通过点击这里来触发它
所以让我 case中打开切换并刷新页面
您看到 我有另一个dag
如果不是您的 case 只需点击触发dag并稍等片刻
然后我们有spark job格式价格
好的 图表已完成
让我们验证我们是否收到了通知
正如您所看到的，在workspace airflow course在频道general下
有新的通知
dag股票市场已成功
这就是您可以获得通知的方式
如果某事出错或不
与您的dag和任务甚至您的任务一起使用其他fiers，感到自由去玩 我会在下一个视频中见到你
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/30_Udemy - Apache Airflow The Hands-On Guide p30 1. Set up the new Airflow environment.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中，我们将对环境进行一些清理，然后我们将创建一个新的数据管道，我们将在课程的剩余部分使用它。让我们开始吧。
之前我们创建了一个名为股票市场的项目，它稍微复杂一些，因为它使用了许多不同的工具，如minao、Spark等。
所以，在接下来的课程中，我希望你更多地关注功能，而不是不同的工具等。
这就是为什么我们不再使用该项目的原因。
之前我们创建了一个名为股票市场的项目，它稍微复杂一些，因为它使用了许多不同的工具，如minao、Spark等。
所以，在接下来的课程中，我希望你更多地关注功能，而不是不同的工具等。
这就是为什么我们不再使用该项目的原因。
之前我们创建了一个名为股票市场的项目，它稍微复杂一些，因为它使用了许多不同的工具，如minao、Spark等。
所以，在接下来的课程中，我希望你更多地关注功能，而不是不同的工具等。
这就是为什么我们不再使用该项目的原因。
而你的第一步是去你的命令行窗口
并确保运行ao dev stop
以便停止与项目环境对应的Docker容器
一旦你做了这些，打开下一个步骤
并确保所有与项目对应的本地容器都已停止，正如你在这里看到的
它们没有运行
一旦你做了这些
你可以回到你的命令行窗口并让我们创建一个新的文件夹
所以，在我这个案例中，我在airflow文件夹中
所以我要创建一个新的文件夹，让我们称它为course
然后进入那个课程文件夹并打开从这个新文件夹中你的长廊
在我这种情况下我正在使用游标
所以我要使用游标打开我的
然后 如你所见这个文件夹是空的
让我们打开一个新的终端并在终端中运行命令 ao dev 在它里面
为了创建一个新的项目
为了在本地运行 airflow
并且如你所见 我得到了一些自动生成的文件和文件夹 现在我们可以在下一集中创建一个新的数据管道
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/31_Udemy - Apache Airflow The Hands-On Guide p31 2. The best way to create your DAGs.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们探索在代码编辑器中创建您的空气流程的不同方式
打开左侧的AGS文件夹
再次删除示例DAG
然后创建一个新的Python文件
我们将其命名为ecom.py
现在您已经准备好定义您的数据管道
在Airflow中，有三种不同的方式
我将告诉您我认为最好的一种
但我们从最经典的方式开始
首先，您需要导入airflow中的
Dag
好的 没有它，您无法创建DAG
定义DAG的一种方式
这是非常古老的
老实说，是Dag = Dag和某事
但这种表示法真的很差
原因让我们说，现在您想要创建一个新的任务
再次导入它
Python操作员
来自airflow.operators.python
然后Python操作员
好的
在下面创建一个新的任务
T = PythonOperator（任务id，某些参数）
但由于您使用此表示法，以便将此任务附加到DAG
您需要在操作员中使用参数DAG
Dag = Dag
您必须为每个创建的任务都这样做
假设您有另一个任务tb
再次，您将使用Dag = Dag
如果您有另一个任务tc，再次
您将使用Dag = Dag
正如您所看到的，这不方便
我的意思是每次您添加一个新任务
您都必须使用Dag = Dag
使用其他方式定义DAG
您将不再需要这样做
因此，您可以删除这种代码
这就是为什么 每次您看到这种表示法
您可以假设您正在查看的博客帖子非常古老
现在让我们探索第二种方式
这是我认为更好的方式
您可以删除这一部分并输入with
然后在Dag对象后添加一个列
现在，由于您这样做了
您实际上需要对任务进行缩进
所以你可以看到这里 我得到了一个错误
但如果我这样做 错误消失了
然后我需要为tb和tc这样做
因为我们在这个上下文中
我不再需要定义它了
所以我可以像这样删除那一部分
所以这三项任务都属于我的dag对象
因为它们在我的dag上下文中
正如你所见，这比之前的方法要好得多
现在 定义Airflow dags的第三种和最后一种方式
是使用dag装饰器
所以不是导入dag from airflow
你导入dag装饰器from airflow.decorators
你不再使用with dag anymore
相反，你将创建一个函数
让我们说
我的dag
然后冒号，在上方的Python函数中
你将使用dag装饰器，最后
但并非最后，你需要在末尾调用该函数
我的dag
否则Airflow将无法触发该dag
你将在dag装饰器这里添加dag参数
好的 所以现在你可能会想知道你应该使用哪种方式
你应该使用类型装饰器
嗯 老实说，这取决于你
但我建议你使用dag装饰器版本
因为最终Airflow将
确实，不仅你用dag装饰器定义dag
你还有其他装饰器定义任务
例如，而不是使用Python operator
你可以使用任务装饰器
这相当于Python operator
这意味着，而不是使用Python operator那样
你只使用任务装饰器
然后在任务装饰器下
定义你的任务
让我们说t，在这里你做任何你想做的事情
你可以删除那一部分
所以再次
我推荐你使用dag装饰器版本而不是上下文管理器with
但两种方式都很好
只要你不使用很第一种方式with doug = Tag Dag
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/32_Udemy - Apache Airflow The Hands-On Guide p32 3. The parameters your DAGs need.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 我将向你展示我认为你应该始终定义的参数
当你在airflow中创建你的dags时
让我们开始吧 需要定义的第一个参数是显然你的dags的唯一标识符
你的数据点管道
这就是dags id
因此，正如我们所用dags装饰器
我们不必显式定义dags id
因为dags id将是对应于我们的dags的python函数的名称
但是假设您使用联系经理
像这样使用dag
那么这种情况下，dag的ID将是第一个参数
因此，作为最佳实践
dag的ID应该是dag文件的名称
因为我们使用了deck装饰器
我们可以删除这条线并更改我的dag为ecom，同样在这里
好的 记住，这是我们的deck of my dag并且我们遵循最佳实践
因此，deck的ID等于dag文件的名称
第二个参数是开始日期
你的数据流将从这个日期开始被调度
你可以定义开始日期等于一个日期对象
为此你需要导入pendulum库中的日期对象
所以从put on
导入日期
然后你可以使用这个日期对象在这里设置你想要的日期
所以 让我们假设 2025年1月1日
像这样，那意味着dag将从2025年1月1日开始被安排
通常在起始日期之后
你总是需要定义的参数是调度参数
你的dag会如何运行
让我们假设schedule等于at daily，每天午夜运行
从2025年1月1日开始
别担心 我会在课程后面覆盖调度
第三个参数与起始日期有点相关
与调度参数相关的是ketchup
通常你应该将catch up设置为false
为什么，因为默认它是设置为true
这意味着Airflow会自动运行DAG，直到它没有运行
在起始日期和今天的所有日期之间
或者从它上次运行到今天的所有日期之间
如果你不想你的DAG有很多正在运行的实例
你应该总是将catch up设置为false
我们将在后面的课程中看到
这些参数非常普遍
正确 我的意思是 这就是你现在总是要定义的那种参数
现在是进入最佳实践世界的时候了
第一个参数是描述
所以你应该总是为你的狗定义一个描述
你不想让你的团队成员不得不进入代码
来理解你的dag做了什么
而这就是描述的好处
因为你可以在airflow的UI中看到它
所以添加一个参数描述
假设这个工作流处理的是电子商务数据
好的 此外
我建议你定义标签
标签在您想要过滤用户界面上的数据时非常有用
特别是在您有许多不同团队之间的许多工作流时
例如，对于这个我们可以说团队A
然后我们可以说项目电子商务
甚至可以添加像π这样的内容
如果你有一些敏感数据被这个数据管道处理
或者你可以指明这些数据管道从哪些来源提取数据
所以老实说 选择标签完全取决于你
但请记住，你可以在UI上对这些进行过滤
这就是为什么它们非常有用
接下来 我们有默认参数，这些参数接受一个字典
它是一个字典，包含默认参数，并将应用于你的任务
例如重试次数
如果你希望将免费尝试的次数应用到你的工作流中的所有任务上，你应该使用默认操作。
所以，你不需要逐个设置每个任务的尝试次数。
例如，你不需要分别设置每个任务的尝试次数。
让我们假设 尝试次数为1，然后你可以设置所有任务的尝试次数都为1。
你可以在这里设置尝试次数。
作为键。
然后，值为1，所有任务的尝试次数都将自动设置为1。
这样，你不需要逐个设置每个任务的尝试次数。
所以，你可以删除这些设置。
好的 这就是为什么默认参数非常有用
但是再次 记住，这些参数适用于你的任务
我喜欢定义的另一个参数是dagon和timeout
它接受一个时间差对象
让我们说minutes等于20
这个时间差对象
你也可以从pendulum中导入它
这意味着什么
好吧 这意味着只要你的dag运行
这个dag运行不能超过20分钟
否则它会自动失败
好的 所以你只是在这里说你的dag应该少于20分钟完成
否则它会失败
最后 我推荐你总是定义的最后一个参数
这与之有点关系
这个是最大连续失败的dag运行数
假设是2
所以用这个参数
你是说你有两个连续的dag运行
一个接一个失败
那么airflow应该停止调度dag
这真是太棒了
因为这避免了有很多dag运行
尤其是如果你的dag在周末出错
或者因为某些原因你没有运行dag
甚至如果你的dag经常运行
这是很酷的避免有很多dag失败
这样你有时间调试你的dag
但不是最后，不要在airflow UI运行这个dag
它不工作
这里有些东西缺失
这是可以预见的，我们会在下个视频中修复
关于这些参数，我推荐你总是定义
这就是全部了
当然，你会在后面的课程中看到其他参数 但是再次强调 这是我认为你应该总是定义的
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/33_Udemy - Apache Airflow The Hands-On Guide p33 4. DAG scheduling the basics.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 你将发现数据调度的基本知识
让我们开始吧
首先想象一下，你想每天午夜运行一个数据管道
一个数据管道期望的第一个参数是起始日期
起始日期定义了何时开始调度一个数据管道
例如 如果你想从2024年1月1日开始调度你的数据管道
那么你会定义这个日期为起始日期
然后定义这个日期为起始日期
第二个最重要的参数是调度调度
定义了数据管道运行的频率
让我通过这两个参数给你展示一个代码示例
这里 你可以看到，数据管道的起始日期为2024年1月1日
将从该日期开始默认运行
然后每天午夜运行
因为我们使用了每日调度
这里的每日是一个慢性表达式
慢性表达式是一个字符串，由六个或七个子表达式组成
描述日程个人所得细节的字段
如果你想定义不同的cron表达式
那我强烈建议你查看chrome tab guru
这个网站帮助你找到chrome表达式
你需要 根据你想要运行你的数据的频率
管道 请随意尝试这些信息
Airflow提供了一些预设
例如你看到的这里
所以你不需要显式使用cron表达式
这使得任何人都更容易理解
当你的管道运行时
好的 你知道为了DAG运行
你需要一个起始日期和一个时间表
现在 问题是当你调度你的管道时发生了什么
假设起始日期是2024年1月1日
所以到那天之前什么也没发生
自从预定的参数是每日
每天午夜
Airflow等待一天，正如在日程中定义的
然后在2024年1月2日
Airflow运行管道
你DAG的第一实例
因此 这个DAG实例覆盖了数据区间
从2024年1月1日到2024年1月2日
然后再次 Airflow等待一天
直到2024年1月3日
你的数据间隔从2024年1月2日到1月3日
你的第二个DAG实例运行
并且会继续这样
现在，你的DAG的每个实例我们都称之为DAG运行
并且DAG运行总是有一个数据间隔
在第一个2024年1月1日
状态这里成功
但它可以失败或排队
并且有一个数据间隔结束
这是2024年1月2日
你可以在Airflow用户界面中找到这些信息，如图所示。好的，现在
你可能会想知道，如果第二个DAG运行需要开始
但第一个还没有完成。嗯
默认情况下 Airflow会运行第二个DAG运行
即使第一个还在运行
通常不会有问题
但可能会 因此，你可以使用max_active_runs参数来控制这一点
max_active_runs控制特定DAG的DAG运行数量
如果你设置它为1
那么只有一个DAG运行可以同时运行
因此，第二个DAG运行将等待第一个完成
有一个等效的参数max_active_runs_per_dag
适用于你的空气流实例
如果你想说所有的DAG都可以同时运行一个DAG运行
这就是基础
让我们稍微玩一下调度参数 这就是基础，让我们稍微玩一下调度参数
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/34_Udemy - Apache Airflow The Hands-On Guide p34 5. Backfill and Catchup.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在你对dag调度有了更好的理解
让我们探索另一个概念，叫做backfilling和ketchup
让我们想象你有一个数据管道
有一个起始日期为2024年1月2日，每日调度
这意味着它在每天午夜运行
假设今天是2024年1月5日
如果你在用户界面中解冻dag以默认运行
你将得到从2024年1月2日到今日的三个dag运行
如起始日期所定义，2024年1月5日
这是番茄酱机制
Airflow赶上未触发的dagg
在开始日期或最后一次运行之间运行
以及您暂停数据管道的那一天
这是默认行为
您可以通过在dagg中使用catch a参数来控制它
对象禁用它
您只需将catch up设置为false
现在您将只获得最新的未触发的图表作为最佳实践
我推荐您禁用ketchup
很容易在你Airflow实例上运行许多图表。
大多数时候，那不是你想要的
因此，最好是你控制这种行为，而不是让空气流动为你做这件事。
好的 现在让我们想象一下，你已经成功地执行了你的数据管道三次
因此，你有三个成功的图表
然而 你意识到你不想从1月2日开始运行你的数据管道
二零二四年
但是从2024年1月1日起
因为你缺少这个图表，嗯
这在图表中引入了一个错误
从1月2日到1月3日运行
因为你想在开始日期之前运行一个图表
并重新运行一个图表
你将使用另一个机制称为回填
不是Airflow自动执行的机制，你必须手动使用命令行界面
如这里所示的用户界面
再次
这在任何时候对你重新运行你的数据都是有用的，以便填充你的数据
或者当你想要在开始日期之前进行运行授权时
这就是关于番茄酱和填充的全部内容
你将在airflow中经常使用这些机制 所以不要犹豫重新观看视频
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/35_Udemy - Apache Airflow The Hands-On Guide p35 6. The most important rule to follow when creating tasks.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


正如你所知 回填是针对特定日期范围运行DAG（Direct Acyclic Graph，定向无环图）的过程
这在您向DAG添加新任务时非常有用
并且希望为其之前的DAG运行回填
或者当您有数据问题时，需要回填它
然而，要回填您的DAG
您需要确保它们有效
如果您不知道我在说什么
请继续关注我 因为这个概念至关重要
如果你希望你的数据管道重新运行
每当你犯错误需要补全一个数据管道
你必须确保你的任务是幂等的
这意味着它们可以安全地多次重新运行
而不会导致意外后果或复制数据
此外，如果你的任务执行写操作或修改外部系统
请检查补全不会导致数据重复或不一致
稍后我们将看一些方法来做到这一点
总之，如果你的数据管道不是幂等的
你不能补全它们
好的 让我给你举一个不健壮的数据管道的例子
假设我们有一个完整的销售表，用于存储每天销售的数量
例如 在1月1日我们进行了5笔销售
在1月2日我们进行了3笔销售
依此类推 然后我们有一个以下的数据管道，用于提取每天销售的数量
通过运行这个滚动请求
好的 让我们假设我们处于2024年1月5日
我们希望在2024年1月1日和1月5日之间回填DAG
所以今天
你认为哪一天 第一个图表将从表中获取数据
嗯
它将获取2024年1月5日的销售数量 那就是它，不是吗
那关于第二个图表呢
怎么样
好吧，再次 它检索了完全相同的记录
一月五日2024年的销售数量
所以看起来
无论数据收集器在回溯期间运行的日期
它都从数据库中获取相同的记录
一月五日2024年
对应于当前日期
这是一个完美的例子
效力问题
运行任务 多次运行任务会在你的数据库或下游管道中产生重复数据
因此你不能在不引起数据问题的情况下回填你的数据管道
这是由我们现在使用的函数引起的
在评分请求中，现在总是返回当前日期，该日期是DAG运行的日期
这在你回填DAG时不起作用
因为你以过去日期运行它
解决方法很简单
将函数now替换为Airflow变量data_interval
Interval
这个变量返回当前DAG运行执行的数据间隔的结束日期
让我们再次回填DAG
第一个DAG运行正在运行
记住，DAG运行总是有一个数据间隔在这里
数据间隔从2024年1月1日开始
数据间隔到2024年1月2日结束
因此，变量data_interval
Interval
在SQL查询中返回2024年1月2日
减去一天间隔
这给出2024年1月1日
因此，第一个DAG运行如预期地检索了2024年1月1日的销售额
然后第二个DAG运行检索2024年1月2日的销售额
依此类推 祝贺你
你的任务现在已完全运行
你可以多次运行它而不会引起意外后果或产生重复数据
总是确保你创建的任何任务都是完整的
这非常重要
另一个例子，你在写入操作中可能会遇到完整性问题，作为最佳实践
每当你写入数据库时
总是使用merge或insert overwrite
也称为absurd
永远不要使用open data
例如使用insert
否则你会产生重复数据
关于完整性就是这样 现在你就像专业人士一样可以回填你的DAG
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/36_Udemy - Apache Airflow The Hands-On Guide p36 7. Play by scheduling your DAGs.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中，你将会与暗调度参数进行一些小的互动。
例如更新日程和补课
让我们先走
让我们确保你保存了文件
然后使用py并运行环境
所以在终端
运行命令 astro dev
重新开始 你必须确保没有其他空气流动环境正在运行
正如你所看到的，项目的环境已经停止
我没有对应于Airflow的本地容器，所以可以在这里运行我的环境
所以，我可以在这里运行我的环境
所以，它进入并等待一会儿
好的 你可以看到Airflow正在运行
你可以在这个链接访问Airflow的UI
你可以通过访问Docker桌面来验证你的环境
如你所见
我有所有对应于环境的Docker容器都在运行
所以，让我们去UI
然后输入 admin
我的意思是
正如你所看到的，我们在获取一些进口物品
正如所料，再次如此
我们还没有真正完成我们的工作
所以让我们回到代码，首先需要改变的是时间 delta
所以我们不会使用时间差
因为平行时空中不存在时间差
我们将使用持续时间
等同于哪一个
在这里，而不是使用时间差
您也可以添加持续时间
如您所见
我们有一堆任务
但我们没有定义任何Python函数来运行
Python操作符期望一个Python函数
这就是为什么我们得到这个
您在这里，为了修复这个问题，我们只是删除所有那些任务
取而代之，我们创建一个任务t等于mt操作符
mt操作符绝对什么也不做
所以你可以将其用作下订单的地方
并且可以导入空的操作员
你只需要到文件的顶部
在这里使用空的，然后使用空的操作员
保存文件 然后回到UI并刷新页面
正如你所见，DAG在这里
好的，现在 让我们对DAG调度参数进行一些操作
有一个问题问你
你认为我们会得到多少dagg跑
我一旦解开匕首
记住我们有一个dagg跑
一个dag的实例
你认为我们会得到多少图表
如果我们解冻dag
考虑到以下参数
起始日期是2025年1月1日
日程是每日并且补课是false
并且今天是2025年2月4日
好吧，我们只会得到一个图表
所以让我们暂停dag并刷新页面
你可以看到只有一个图表
如果你想知道为什么，让我给你一些解释
所以记住因为我们将catch up设置为false
这意味着我们不想在起始日期2025年1月1日和今天2025年2月4日之间运行任何缺失的dagruns
然而，最后一个未触发的dagon
那缺少的部分将被触发
即使赶上是等于false
这就是我们最终得到一个如图所示的图表的原因
现在另一个问题问你
如果你将赶上从false更改为true，你认为会发生什么
你认为在这里你会得到多少个图表
如果你刷新页面
你仍然得到一个图表
实际上，如果你看看dag
然后你可以看到代码确实赶上被设置为true
那么你为什么在今天和开始日期之间没有获得所缺的挖洞运行
那是因为记住补齐
查找开始日期
如果dag从未被触发过
或者如果它已经被触发过补齐
查看dag最后一次被触发的时间
并且dag最后一次被触发的时间是今天
这就是你为什么没有获得任何其他dag的原因
那么你怎么修复它
你怎么以某种方式改变它
你想要在起始日期和今天之间运行缺失的图表
好的 你需要改变一件事
让我们回到dag
然后暂停它，去浏览dagon并删除记录
好的 所以你删除那个图表
现在如果你回到dags
它就像如果我们从未执行过dag一样
对吧 因为没触发器
因为那样以及我们赶上了真实时间
如果我们重新启动触发器并刷新页面
这次我们会得到越来越多的触发器
因为现在流体正在追赶从起始日期到今天的时间
而这正是你可以看到的
如果你点击触发器并查看任务运行
你看
我们有一月份的不同图表
直到它到达今天
而现在所有缺失的图表都已经成功执行
我想强调几点
首先 如果你点击图表
所以点击这个非常长的条形图并查看右边的数据
你可以看到数据区间开始和截止日期区间
对于特定的图表
记住一个图表总是有一个数据区间开始
这里是2025年2月3日，数据区间结束
这里是2025年2月4日
所以你知道这次运行是在这个时间段内执行的
你也可以看到图表是否被
比如说来自流体的API调用触发
这并不是情况 这里是调度的
实际上你可以通过查看运行类型来看到这一点
你可以看到调度到手动触发一个图表
通过点击这个按钮在
然后你会看到手动而不是调度，最后
但并非最不重要的，你可以看到那个特定的图表花了多长时间
此外 如果你回到图表并查看不同的列
这里你可以通过悬停在这个图标上
你会看到确切的图表运行时间
在这里午夜
所以每天午夜
然后在右边你有最后一次运行
所以截止日期区间
最后一次图表运行开始时间
然后你有截止日期表
下一个图表开始时间
所以还没有触发的那个
我强烈建议你悬停在这个小图标上
你会得到更多信息
这里你知道下一个将在16小时后发生
在2025年2月4日和2025年2月5日之间的数据区间
人们经常想知道的问题是，如果而不是每天
我想改变我的图表调度并运行它
让我们说每周在每周的基础上
嗯 让我们看看会发生什么
为此我们需要设置挖掘
然后转到运行并删除一些图表
所以12345
6和7
然后删除那些图表
回到代码并更改daily为weekly并保存文件
然后回到UI
转到dags
点击dag
验证更改已被采纳
在这里可以看到每周
让我们再次暂停 dag 并刷新页面
让我们看看是否得到 dag
看起来不像，所以我们可能需要删除额外的 ddiagrams
所以让我们这样做
转到浏览
然后再次 让我们删除一些额外的图表
然后点击删除
去d
让我们参考页面
现在我们得到了一个新的图表
所以我们等一会儿
好的 点击dag
正如你所见
看起来什么都没有改变
但这里有一个陷阱
如果你点击
让我们说这张图表
或者让我们说这张图表
你可以看到这张图表的数据区间是2025年1月19日
而数据区间是2025年1月26日
所以这张图表覆盖了一周而不是一天，像之前那样
然而 在UI上
除非你查看数据区间，否则你无法看到这一点
最后一张图表的情况也一样，它覆盖了一周
但如果你点击一些之前的运行
你可以看到这一个覆盖了一天
所以这里对你来说的关键点是什么
如果你必须改变一个现有dag的日程
那么我认为你应该创建一个dag的副本
然后更改dag的id为
例如v2
然后暂停预览的dag
这与v1的日常日程相对应
以及新的每周日程的dag
例如
并且保持它在这个新时间表上运行
如果你这样做 匕首在流UI上将保持一致
这不会像这里一样你不知道
哪个挖运行在每日时间表上
以及哪个挖运行在每周时间表上
并且如果你需要回填你的挖
那么你仍然能够在两个时间表上做
每周时间和每日时间 因为你有两个挖
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/37_Udemy - Apache Airflow The Hands-On Guide p37 8. Dealing with timezones in Airflow.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 你将了解时区如何应用于 airflow 以及如何使你的 dags 与时区相关
让我们从时区的定义开始
根据维基百科
时区是地球上一个区域，遵循统一的标准时间
大多数时区都与协调世界时（utc）相差若干小时
这通常被称为
例如 如果我住在巴黎
我的时区比 utc 提前两个小时
反之 如果我住在纽约
我会比uc少4个小时
协调世界时或uc非常有用
因为它使我们能够参考同一时间
无论我们处于哪个时区
让我们快速了解一下世界是如何划分时区的
你可以看看这个地图
地图在下方被分成多个切片
你可以在每个切片的偏移量中找到uc的小时表示
除了与给定时区对应的颜色
例如 如果我们以法国巴黎为例
最接近的绿色切片是偏移量加两小时的那一个
这基本上意味着我们需要从uc中添加两小时
以获得巴黎的本地时间
你可以为其他国家做同样的事情，以便在python中不处理时区
我们有naive和aware的日期时间对象
这两种对象之间的区别微妙但非常重要
例如 如果你创建一个日期时间对象而没有指定时区，这将创建一个无时区日期时间对象
反之亦然 当你创建一个带有时区定义的日期时间对象时，这将创建一个有时区日期时间对象
棘手的部分是当你使用无时区日期时间对象时
Python会使用解释器提供的本地时区
然而它可以改变并导致问题
你应该永远不会让Python假设你没有明确定义的东西
尤其是与时区有关
记住，Python中的日期时间并不默认创建有时区对象
一个没有时区的日期时间可能并不位于UTC
好的 关于空气流通，如何避免与时区相关的错误
所有内容默认存储和显示为UC
作为最佳实践 你应该总是将数据存储为UC并在本地时区显示
你可以通过修改配置文件中的_default_time_zone参数来更改Airflow的默认时区
例如，你可以将其设置为europe_amsterdam
例如 你可以将其设置为europe_amsterdam
所有创建的日期都将使用阿姆斯特丹时间
当你再次创建天真的日期时间对象时
即使你改变了时区
Airflow 仍然会在 uc 内部转换日期
这里有一个使用摆锤库的例子
Airflow目前使用的处理日期对象和时间区的Python包
让我们检查调度dag之间的差异
使用Chrome表达式和时间Delta对象
当你定义了一个DAG的调度参数时
你可以选择使用一个chrome表达式或一个时间差对象来设置它
有其他方法
但让我们专注于与kron一起做这个
Airflow将始终在特定时间运行您的dags，这意味着
定期表达式
尊重 和夏令时
例如 如果您设置一个dag在5点pm utc加1运行
它将始终在5点pm运行
utc 加1 即使在DST期间
我们谈论的是夏令时
但你可能不知道它是什么
这里有一个简单的解释在一些国家
比如欧洲 我们有所谓的夏令时
这是将时钟在夏季月份向前调整一小时的标准时间
并在秋季再次调整回去以便更好地利用自然日光
好的 让我们解释一下当你使用时间Delta对象时会发生什么
而不是定期安排你的任务
在这种情况下 Airflow 总是会在特定时间间隔而不是特定时间运行你的任务
就像使用定期表达式一样
例如每两小时
三小时等等
假设你有一个每两小时运行的任务
如果我们从秋季切换到夏季
而你的任务通常在 UTC+1 的 5 点运行
它将在 6 点运行
Uc plus two
随着我们将时钟向前调一小时
保持相同的间隔
但并不与以前相同的时间开始
理解慢性表达与时间delta对象之间的区别一种方法是
通过使用catch up sets参数来强制在你的dag中
根据这个参数的值
当dst发生时
你将不会得到相同的行为
这张表格详细显示了它们是如何工作的，日期日期
无论是否有意外情况，catch up的值都被设置为false或true
请注意，在2024年3月1日dd发生时，正在发生的事情
如果你使用chronic表达式，并将catch up设置为false
调度器将跳过那一天
因为我们比调度间隔2 a.m.晚了一个小时
这是错误的 这是因为在另一边时区发生了本地时间区的偏移
时间delta将执行dag
即使catch up被设置为false
因为它总是保持相同的时间间隔
然后如果你看一下第二天的慢性表达
我们一直在当地时间凌晨2点执行
而时间差
我们现在在凌晨3点执行dag
你可能会问 好的
酷，所以哪一个我用呢
这真的取决于你选择哪一个最适合
最好的 你的用例
如果执行时间晚一小时
你的本地时间调度不会给你带来任何麻烦
我会保持我的 dag 配置在 uc 并相应地设置慢性表达式
请注意，实际上
这确实是一个问题
Airflow 给你提供时区时间
模型和 dag 中的对象
并且最常情况下，新的日期时间对象是从现有对象通过时间 delta 生成的
算术 总之
你不应该有任何麻烦
处理时区是工程领域的一个非常热门的话题 但我希望你在这里有一个伟大的介绍
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/38_Udemy - Apache Airflow The Hands-On Guide p38 9. Scheduling DAGs based on data with Datasets.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


到目前为止，你唯一知道的安排你dags的方式是基于时间
比如每天午夜
每周一次等等
但猜猜看，使用airflow
你可以根据数据安排你的tags
让我来向你展示
好的 让我们从一个用例开始
想象你是数据工程团队的一部分
你有一个数据管道产生一个csv文件orders带有日期
然后有一组数据分析团队
他们有一个数据管道等待这个文件
现在问题是如何创建这两个数据管道之间的依赖关系
因为使用传感器的一种方式
例如 文件传感器
那么你将有文件传感器作为dag中数据分析团队的第一个任务
等待csv文件落在一个特定位置
但你会浪费一些玫瑰
因为传感器会持续检查这个文件
并且会有一点延迟
我们知道我们的方式是使用外部任务传感器
作为数据分析团队数据管道中的第一个任务
但这次传感器增加了一些复杂性
假设第一个数据管道在早上9点运行
然后第二个管道应该在上午10点运行
它们不在同一时间安排
并且你需要调整外部任务传感器
所以再次不方便
不容易
所以这就是为什么有另一种方式，要好得多
那就是使用数据集
在airflow中，数据集是数据的逻辑分组
数据集可以是你想要的任何东西
例如文件
表格
桶 你想要的任何东西
只记住
如你所见稍后
数据集只是一个指向资源的指针
airflow不知道这个数据集背后的确切情况
回到例子
我们有对应csv文件的数据集
一旦数据管道产生这个数据集
这将触发数据分析团队的数据管道
因为它是基于这个数据集的
这就是你可以轻松根据数据创建多个dag之间的依赖关系的方式
使用数据集
这个功能非常容易使用
所以例如 这是我们的数据管道
首先我们导入数据集类
然后我们定义一个数据集，路径如下
斜杠 tp 数据 txt
然后我们有一个任务，使用数据集打开数据点 txt
正如你在打开和写入数据中看到的那样
所以这个任务更新了数据集
为了表明 airflow 知道这个任务更新了数据集
你必须添加一个新参数，即输出
并将数据集放入输出参数数组中
如果该任务更新多个数据集
你必须将这些数据集放入数组中
这是产生数据集的 dag
让我们看看消费这个数据集的 dag
所以调度参数不同
而不是调度在时间
你将调度在数据集上
并且这个数据集必须在消费这个数据集的 dag 和产生这个数据集的 dag 之间是相同的
正如你所看到的，这里你有相同的数据集
路径是相同的，所以这样做
一旦任务 a 在产生者 dag 中成功
将触发消费者 dag
因为消费者 dag 正在等待这个数据集被更新以运行
但是现在这里有一个陷阱
如果任务 a 实际上什么也不做
正如你所看到的，这个任务 a 什么也不做
但我们仍然告诉 airflow 这个任务更新了数据集文件
这将仍然触发消费者 dag
即使实际上我们没有修改任何东西
这就是我想要你记住的
airflow 关心的是
如果更新给定数据集的任务成功与否
如果任务成功，将触发等待这个数据集的 dag
如果任务不成功，将不触发等待这个数据集的 dag
但是实际上数据集背后的数据发生了什么，airflow 不在乎
好的
这就是关于数据集的所有内容 正如你所看到的，非常
非常强大的功能 现在您可以轻松地基于数据而不是时间创建不同的 dag 依赖关系
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/39_Udemy - Apache Airflow The Hands-On Guide p39 10. Conditional Dataset scheduling.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


使用数据集的调度非常有力
但你可以使用条件数据集做更多事情
让我们从一个用例开始
想象你有一个更新CSV文件的数据管道
而这个CSV文件是一个数据集
但现在想象你创建了另一个任务
这个任务到你的数据管道创建或更新另一个数据集项带有日期
所以现在你有两个数据集
问题是如何等待所有数据集
想象你有一个喷嘴DAG在你的某个地方
例如 它需要等待这两个数据集运行
你如何等待它们全部
或者你如何等待其中一个
好的，一旦订单或项目准备好
这就是你可以使用条件数据集调度的地方
你有两个操作员
并且或你可以与你的数据集混合来表达那些条件调度
让我给你看几个例子
最基本的一个实际上没有使用任何条件数据集调度操作符
是这个
所以这里DAG一旦a和b被更新就会运行
所以这里你是等待所有数据集a和b
但现在想象你想一旦a或b被更新运行DAG
你需要使用或操作符你看到在调度参数中
你正在使用或a或b
但有一件事要注意这里是使用条件
你需要在调度参数中使用圆括号
没有条件你需要使用方括号
如果你尝试混合条件与你的数据集这非常重要
但你使用方括号
你将在四UI上得到一个错误
所以记住如果你使用条件数据集调度
你需要在调度参数中使用圆括号
除此之外这是一个更复杂的例子
所以我们有四个数据集a b
C和d
如果你仔细看调度参数
我们正在使用所有条件数据集调度操作符或和
所以这里我们说我们希望运行这个DAG
一旦a或b在第一组中
以及c或d在第二组中被更新
你可以想象数据集和最后调度操作符的可能性
但最后
你可能想知道
我可以根据数据集和时间调度我的DAG吗
好吧，你可以这样做
这稍微复杂一点
因为你必须使用一个叫做时间表的东西
我们不会深入细节
这真的很先进
但现在只需记住你可以在这里这样做
你需要导入数据集或时间表
然后使用该函数作为调度参数的值
然后你有两个参数需要填写
第一个是时间表，带有cron触发器时间表
它期望一个cron表达式
所以我们在这里说我们希望每天午夜运行dag
第二个参数是数据集
数据集将用于调度dag
所以再次
总结 这个dag将每天午夜运行，并且当a或b更新时 所以现在你知道如何像专家一样调度你的dag
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/40_Udemy - Apache Airflow The Hands-On Guide p40 11. Datasets in action!.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


是时候看看定时调度的运行了
让我们开始 好的
假设我们有一个API
这个API返回按照顺序制作特定鸡尾酒的指令
例如 这里有一款鸡尾酒，我们有制作它的指令
这个API返回JSON数据
在这种情况下，我们将创建一个DAG
我们称之为提取器
并且这个任务会从API中提取JSON数据
然后将数据写入一个JSON文件
一旦完成，会触发我们的DAG
这里的JSON文件就是我们的数据集
使用数据集
我们会在提取器和ecom之间创建一个依赖关系
这就是我们现在要做的
在你的代码编辑器中 创建一个新的Python文件
命名为extractor.py
然后导入airflow.dot.decorators中的dudecorator
导入dag
然后你需要导入pendulum中的start date对象为daytime
导入
datetime
然后你使用deck decorator
开始日期是2025年1月1日
调度是每日
所以让我们假设我们想要在午夜运行这个dag
并且将catch up设置为false作为最佳实践
然后我们创建与DAG提取器对应的Python函数
并在最后调用该函数
所以现在我们需要创建一个任务
为此我们将使用Python操作符来运行一个Python函数
所以在顶部
导入来自airflow的Python操作符
Python导入Python操作符创建一个新的任务在这里
获取鸡尾酒
等于Python操作符与任务ID获取鸡尾酒
并且你需要传递一个Python函数
你想要运行的Python函数
所以这里下划线获取鸡尾酒
你需要创建该函数，所以在类型定义之前
获取鸡尾酒
我们准备好从API中提取数据了，为此我们需要导入requests库
因为我们需要向API发送HTTP请求
然后创建一个名为api的变量，链接到API
然后创建一个名为response的变量
等于requests.get(api)，以向API发送GET请求
现在我们获得了响应
我们可以使用open mp创建文件
鸡尾酒
点json作为f
然后写入该文件这些数据
此时我们能够向API发起请求
然后将响应写入到以下文件
然而该文件应该是一个数据集
而目前并非如此
它只是一个路径
所以让我们在顶部导入数据集类
来自airflow点data sets
导入数据集
然后我们创建数据集
所以让我们称之为鸡尾酒
等于数据集
您可以在此处提供路径/tp/鸡尾酒
点json像这样
而不是在这里硬编码路径，您将调用数据集与点u或i
这对应于此路径
请记住，这是此数据集的唯一标识符
现在仍然有一件事缺失
确实，我们没有告诉airflow该任务获取鸡尾酒
我们将更新此数据集
要这样做 你知道怎么做
您需要设置参数outlets
所以在python coable之后
让我们定义参数outlets，它接受一个数据集数组
在此情况下，数据集是鸡尾酒
实际上 让我们将鸡尾酒重命名为数据集
鸡尾酒 这样更直观
然后重命名鸡尾酒在这里，并使用数据集在这里
现在airflow知道一旦该任务完成
这意味着数据集已更新
现在最后一步是更新ecom
这样一旦任务获取鸡尾酒成功
这将触发ecom dag
所以我将花几秒钟时间来让您完成
您应该知道如何做
如果不知道，请查看之前的视频
然后我将向您展示解决方案
好的 解决方案非常简单
您只需要使用与提取或dag相同的数据集
所以您可以复制这条线并放在这里
然后导入数据集类
来自airflow点data sets导入data set
然后而不是按周安排
你想在这个数据集上安排这个dag，非常简单
您只需使用方括号
因为我们没有使用任何条件数据集
在这里安排操作员
然后你就把数据集放在那里
现在我们已经成功地在提取器和ecom之间创建了依赖关系。
使用一个数据集
实际上 如果你看一下e的ui
带着太阳 你现在可以看到，计划是基于一个数据集
这是接下来的一个
然后我们有一个额外的标签，每天运行
你也可以去数据集查看
在这里你可以在依赖图中明确看到
我们有一个基于这个数据集的依赖，提取器和econ之间
提取器产生这个数据集，触发ecom在训练dags之前
我们需要进行一些修改
首先，在你添加一个参数之后
wb为正确的字节
然后这里，而不是使用json
你想要使用内容
然后保存文件并返回到UI
如你所见 Eom有二十个图表
让我们启用教程提取器并刷新页面
我们有一个图表
现在你可以看到我们有21个Ecom图表
这意味着我们已经成功触发了Ecom DAG
基于提取器DAG中的数据集
我强烈推荐你多与数据集玩耍
记住，airflow并不在意底层数据
所以你完全可以创建另一个数据集
比如数据集
下划线a等于数据集
然后这里a，现在你有两个数据集
尝试创建一个另外的任务来更新数据集a
你只需要设置输出参数
在ecom dag中
尝试使用条件数据集调度操作符，看看他们是如何工作的
学习最好的方式是通过实践 尽量去做 下次视频见
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/41_Udemy - Apache Airflow The Hands-On Guide p41 12. Sharing data between task with XComs.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 你将发现如何将数据分享到你的任务之间
让我们深入探讨在airflow中分享数据之间的任务实际上是相当简单的
所以想象你有两个任务
让我们说任务a和任务b
你想要在a和b之间共享a
在airflow中实现这一点的机制叫做xcom
它工作方式是这样的
所以在a中，你将使用方法xcom_push来推送xcom
它接受你想要推送的值
当你推那个值
你想要分享的数据将被存储在airflow mea数据库中
然后从任务b中获取它
你将使用xcom
你将指示任务id，任务是推xcom
在这个情况下a，这样做
你将能够从任务b中获取错误
这就是它工作的方式
就是这么简单
你将在airflow中推送你想要分享的数据
然后你从元数据库中拉取你想要获取的数据
在你想要的任务中
你需要记住一件事
确实你应该对数据库小心谨慎
因为我们在airflow和数据库中存储值
取决于你使用的数据库
你可能无法为给定的xcom存储相同的数据量
如果你使用sqlite
这将是每个xcom的2GB数据
如果你使用postgre
那么这将是针对给定的xcom的1GB数据
如果你使用MySQL
那么这将是针对给定的x的64MB数据
好的 所以这里要强调的是
如果你有两个任务需要大量数据共享
那么你要小心，因为根据你使用的数据库
你可能无法存储与给定的xcom相同的数据量
如果你想存储超过
比如1GB的数据
你正在使用Postgres数据库
然后有另一种机制可以让你这样做
那就是通过定义x com
并且默认的x come backend是Airflow元数据库
但你完全可以定义另一个backend
例如
S三
在这种情况下你可以共享你想要的任何数据
但是不要忘记，如果你在S三中存储TB级别的数据
然后你在这里拉取TB级别的数据
你可能会遇到一些麻烦，
因为 airflow 可能没有足够的资源来处理那个，
最终，
这取决于你的基础设施和你在后端提取器 dag 中的资源，
我们要做的就是添加一个新的任务来验证响应，
那个内容有数据，
所以我们要做的第一件事就是在获取鸡尾酒之后创建那个任务，
我们叫它检查大小，
等于 python 操作员与任务，
ID 检查大小，
而 python 函数将是在分数上检查大小，
然后我们需要创建那个 python 函数，
所以在获取鸡尾酒之后，
让我们定义一个新的 python 函数，
现在我们需要获取响应. content 的大小，
然而，
响应存在于获取鸡尾酒中，而不存在于检查大小中， 所以这就是我们需要使用 x comes 的地方，
第一步是将获取鸡尾酒任务的响应. content 的大小推送到 python 函数中，
为此向 python 函数添加参数 ti 为任务实例，
并且这是一个 airflow 保留的参数，
你不能将其设置为 none，
然后这里类型 ti. x can push，
一个键，
让我们说请求大小， 然后值将是 en response. content，
所以这里我们推送了一个 xcom 带有请求 _ 大小的键，
键基本上是 xcom 的名称，
并且值对应于响应. content 的大小，
并且这将存储在 airflow 数据库中，
所以现在我们在 airflow 数据库中存储了那个，
我们可以从检查大小任务中获取它，
为此我们还需要使用 ti 等于 none 的任务实例，
然后这里让我们创建一个新变量 size 并且等于 ti. xcom pull，
这次键请求大小并且任务 id，
等于获取鸡尾酒，
因为创建 xcom 的任务是获取鸡尾酒，正如这里所示，
好的，
所以只是通过这样做， 我们能够推送 x ，
Come 带有获取鸡尾酒的响应. content 的大小，
并且从检查大小中获取它，
并且现在我们可以在任务中打印大小，
仅仅看看它是否工作，
说到那个，
为了验证我们做的走，
我们需要在获取鸡尾酒和检查大小之间创建依赖关系，
确实， 我们希望首先运行获取鸡尾酒，
因为它做了 x can push
然后我们想要运行检查大小
因为它可以做到x可以池
所以如果你向下滚动一点，在两个任务之后
输入获取鸡尾酒
然后使用右移位运算符
然后检查大小
在这里，你说你想要首先运行获取鸡尾酒
然后检查大小
保存文件
然后在你的终端输入astro dev run
测试然后id
这是extr和一个过去的日期
以便运行dag
进入并等待一会儿
如你所见，X中有一个错误
在Com push方法中 所以让我们回去，在这里不是val而是value
然后保存文件并再次运行命令
好的 如你所见，它成功了
如果你仔细看一下日志
你可以在这里看到响应点内容的大小
这意味着我们在两个任务之间成功共享了数据
实际上 如果你去Airflow UI然后管理员和X来
你可以看到我们从任务推送的对应Xcom
获取鸡尾酒
这就是关于任务之间共享数据的内容
你可以做更多 你会在课程中看到的
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/42_Udemy - Apache Airflow The Hands-On Guide p42 13. Organize your DAGs folder and clean your DAGs.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个简短的视频中 你将发现如何更好地组织你的dags
最终，ddas文件夹
让我们开始吧 你会很快发现，关于airflow的一件事
是你的dags会很快变得更大
而且，dax文件夹将包含的不仅仅是dag文件
所以它可能会变得杂乱无章
这就是你想要避免的
在这里，我将给你一些指南来帮助你
所以当你看提取数据管道以及电商数据管道时
你可以看到他们在数据集中都有
所以 我们可以立即做的事情之一是在dax文件夹中创建一个新文件
让我们称其为py数据集
然后我们可以复制这条线并粘贴到这里
不要忘记从airflow导入数据集
所以，空气流量数据集导入数据集
保存文件
然后从这里移除提取DAG的行，并对电子商务DAG做同样的操作。
然后现在您可以删除数据集导入
在这里相反
您想要导入您的数据集数据集鸡尾酒来自datasets文件
所以从datasets导入数据集鸡尾酒
您对ecom做同样的事情
好的 所以，正如您所看到的，您的数据集现在都在一个地方
您可以想象，您会有越来越多的数据集
所以这样做要好得多
而不是在不同的地方写相同的数据集
每次你需要创建一个新的数据集时
你把它放在这里，并在你想要的任何dags中使用它
好的 这是第1个改进
现在，数据集不是一个dags
但是，Airflow调度程序会尝试解析它
我的意思是，至少它会尝试识别该文件是否是dags或否
对于单个文件来说，这是可以的
但如果你有很多不是dags文件的文件在你的dags文件夹中
在某个时候，这会对调度程序的性能产生负面影响
你想要避免这一点，有一个非常容易避免的方法
那是通过使用你可以在这里看到的点Airflow忽略文件来实现的
你已经有了它
因为你正在使用 astro ci
但是没有astro cli，你就没有它
你需要创建那个文件
并且这个文件完全像.git 文件一样行走
忽略文件 因此
如果你想忽略数据集文件
你需要将数据集
dot py文件放入dags文件夹中，否则airflow调度器将尝试跳过此文件
这将节省资源
如果你有一些文件夹
例如helpers
你需要将该文件夹命名为helpers
airflow调度器将忽略该文件夹
请记住这一点
在你的文件中，airflow非常好
如果你在dags文件夹中有非dag文件
让我们暂时移除helpers
尽管这是一个很好的选项
我不太喜欢它
原因
它不会阻止你有一个巨大的混乱的dags文件夹
好的 即使你忽略了非dag文件
或者即使你忽略了一些文件夹
这也不会阻止你有一个巨大的混乱的dags文件夹
包含许多文件和文件夹
而不是将datasets that py文件放在dags文件夹中
因为它不是一个dag文件
更好的方式 在我看来，比使用airflow文件更好
是将该文件从dags文件夹中移至另一个文件夹
即include文件夹
像这样
好的 所以现在在include文件夹中
有datasets文件
在ecom中，你必须确保使用include dot datasets
同样，提取或dag也必须使用include dot datasets
在我看来，这种方法要干净得多
因为你知道在input文件夹中
所有非dag文件和文件夹都被用于你的dag
在你的dag文件夹中，你只有有dag文件
因为你使用astro ci
include文件夹已经为你准备好了
但如果你不使用astro ci
那么你需要创建该文件夹
并确保将其包括在python path环境变量中
否则你将无法进行那些导入
它将无法正常工作
作为一个快速练习，我希望你
进入extract or dag
并将那些python函数放入include文件夹中的任务
dot py文件中
然后导入那些python函数
这样你就可以在这里使用那些任务
通过这样做，你将使你的dag更小，更易于调试
所以我要花几秒钟
然后我会向你展示解决方案
好的 让我们开始
所以在包含文件夹中
创建一个新的Python文件tasks.py
将那些Python函数复制到新文件tasks.py中
然后从extract或dag中删除Python函数
然后你可以看到 你需要导入数据集cocktail
所以让我们导入数据集cocktail
像这样 然后，在extract dag中，您只需要从include tasks导入tasks
获取cocktail并检查大小
现在您可以看到，DAG比之前要小得多
但也可以在不同的DAG中重用这些任务，如果您想要的话 因为它们集中在这个文件中
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/43_Udemy - Apache Airflow The Hands-On Guide p43 14. Manage task and DAG failures.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们探索如何让我们的dags和任务变得更加健壮
通过使用airflow提供的处理失败的机制
好的 您需要了解的前两个参数
以及可以在dags级别定义的是on_success_callback和on_failure_callback
两者都接受一个Python函数，用于运行您想要执行的任何操作
在dags成功或失败时
例如 假设您想在dags失败时做某事
然后你只需要创建一个对应回调的python函数
假设
处理
失败的dagan
它接受一个上下文
我会在几秒钟内告诉你它是什么
然后你只需要将那个python函数
作为回调的不熟悉的值
现在，在那个函数里你可以做几乎任何你想要的事情
你可以发送一封邮件 你可以发送一个slack消息
这取决于你
但让我看看这个上下文包含什么
为此我们必须让一个任务失败
为此我们可以回到任务文件
在这里我们将引发一个异常
一个airflow异常
所以引发airflow异常
然后你需要导入它
所以从airflow点exceptions导入
导入 airflow 异常
实际失败异常
这甚至更好
好的 所以现在让我们回到 dag
然后在终端输入 us 通过 dev 运行 dags tests
然后 tag id 提取器和一个过去的日期 2025 年 1 月 1 日
它转
让我们看看结果
好的 所以你可以看到，甲板运行失败
这就是你看到的
但如果你仔细查看日志
这里有很多信息
这就是上下文实际上拥有的
我知道这不是阅读所有此类信息的最佳方式
但你有一些有趣的信息
例如，你可以访问dag
你可以访问dagger对象
你可以访问数据间隔开始
你可以访问任务实例
正如你看到的这里
它是extractor点checksize
所以你在上下文中看到
你有很多信息可以用来发送至频道
你想要再次
Slack 电子邮件等等
典型的消息可能是那样的
所以让我们说
图表以任务失败
在这里上下文任务实例
然后任务ID
对于数据间隔
之间和让我们使用三
而不是一个上下文
预
和下一个
DS顺便说一下
Prods对应于数据将开始
但在字符串格式
下一个DS对应于数据间隔并在字符串格式
所以再次
让我们保存文件并再次运行dag
这次你可以看到消息标签失败
与任务检查大小为间隔日期之间和这一个总结
对于dag你有两个回调
不成功的回调和不熟悉的回调
现在关于任务
嗯 显然你也有一些回调
任务有成功回调做某事当一个任务成功
然后你有不成功的回调做某事当一个任务失败
然后你有跳过回调做某事如果一个任务跳过
然后你有不重试回调做某事如果一个任务重试
所以失败 然后它重试最后
但并非最后你有可爱的回调做某事
当一个任务将要执行
你有很多回调
但这里让我们使用不成功回调为检查大小
失败回调
它工作正好像dag级别的不成功回调
所以我们只是创建一个新的回调在这里
dev处理检查大小
失败或让我们说
Mt
大小
在这里我们想要说没有为数据间隔
之间和让我们重用那个
好的 现在你可以复制并使用它在这里的终端中
如果你再次运行DAG
如果你滚动查看
可以看到回调消息的反馈
没有处理该时间段内的鸡尾酒数据
但并非最后 有几种参数可以帮助你更好地控制任务的重试
在失败情况下
让我来展示给你看，首先，因为我们想将这些参数应用到DAG中的所有任务上
我们可以使用所谓的默认参数
所以在DAG装饰器中，你可以使用默认x参数
它接受一个字典
现在让我们使用不同的尝试参数
第一个是重试次数
定义了你希望任务在失败前重试的次数
所以 让我们说，我们希望我们的任务在失败前最多重试两次
然后你有重试延迟
这表示你希望在每次重试之间等待多久
让我们说，我们希望在每次重试之间等待最多两秒
所以是持续时间
秒两秒，你需要导入持续时间从penel
然后有一个参数是重试指数后退
当你设置这个参数为真
每次任务重试时
它将会越来越长时间
所以例如
获取鸡尾酒失败
它等待两秒
然后它再次失败
现在它将等待四秒
然后它再次失败，现在它将等待八秒，依此类推
这就是重试指数后退的想法
直到达到最大重试延迟
让我们说持续时间小时等于一
所以这里我们说
如果重试延迟为一小时
那么任务将失败并且后退不再适用
重试指数后退非常好
如果你有一个查询API或数据库的任务
如果你有很多这样的任务
重试 指数后退
可能有助于避免过度加载你试图访问的资源
现在，因为我们正在访问API
我们可以使用重试指数后退
但只适用于获取鸡尾酒任务
所以，而不是在这个参数中使用默认ar
让我们也移除max retray delay
并且在gate cocktail水平上
我们可以使用retry exponential back off为真和max
我们尝试延迟的持续时间
比如
分钟
15
好的 这是有道理的，因为我们正在查询一个API
我们不想查询太多
因此每次任务被重试时
我们应该等待更长的时间
希望最终在某个时间点能够成功，回到默认
别忘了在这里添加逗号
并且记住，我们希望保持dag干净
因此我们不想在dag文件中再次使用那些函数
让我们利用include文件夹
在这里我们可以创建一个新的文件夹，命名为extractor
并创建一个新的python文件
Callbacks
然后可以将那些回调函数复制并粘贴到这里
然后保存文件
回到dag文件
然后导入回调函数，来自include.dot.extractor
dot.callbacks
导入
和empty size和and field diagram
我们已经创建了extractor文件夹
因为这些回调函数是特定于这个dag的
这就是关于如何处理airflow中的dag和任务失败的内容 下次视频见
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/44_Udemy - Apache Airflow The Hands-On Guide p44 15. Test your tasks and DAGs.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中，你将了解如何测试你的数据流和任务
让我们深入探讨第一个问题：为什么测试数据管道
我相信这个问题对你来说很明显
但让我们想象一下，你在处理一个数据管道
这对业务来说非常重要
然后你决定更新一个给定的任务
然后你将其推送到 staging
你测试了任务，就像你用一些数据运行任务
看起来它工作正常
然后你将其推送到预测
然后不知为何它不工作
问题是
如果你这样测试你的任务
只是运行任务
是不够的
你想要的是单元测试
自动化测试
这将验证你的任务按预期工作
基于不同的输入
在不同的环境和不同的数据等条件下
所以你想确保你的数据管道尽可能健壮
为了避免在生产中出现可能对业务产生负面影响的问题
因此，你的公司 airflow 和 o i 当你使用 i 来运行 airflow 时
提供一些机制来测试你的数据
计划和任务
所以如果我们看一下astro chi
你有一堆评论
第一个是astro dev pa
这使得你能够传递你的dag
确保没有导入你的
例如
如果你有一个有向无环图(dag)
设置dag的最后任务，引用了第一个任务
然后你会看到运行这个命令
你也会确保你没有任何拼写错误和那些东西
然后你会有一个真正的开发pi测试运行
并且像你稍后会看到的那样
项目测试文件夹的所有测试
你有astro dev upgrade test
测试环境与新的空气流运行版本
例如 有一个新的空气流版本
你想要确保你的DAGs在新版本中运行
你可以运行这个命令，非常有用
右边也有一些命令，例如Airflow开源
是Airflow Dags Test
我们已经这样做了，以执行单个DAG
或者你有Airflow Tasks
测试以执行单个任务
在没有将任何必关任务执行情况的数据存储到airflow数据库中
所以这些注释非常有用
我们将在后面的视频中使用它们
让我们看看测试你的dag和任务在airflow的不同方式
第一种是dag测试
这种方法非常棒，但不太为人所知
所以dag测试方法允许你运行dag中的所有任务
在一个单一的序列化的python进程中，而不运行airflow调度程序
这就是为什么这种方法非常快和高效
此外，dag测试方法可以让你更快地迭代，并使用ide调试工具
在开发您的dags时，使用断点和您的ide调试器
您可以跟踪任务
您变量的值
并且x将通过分析您的dag直接显示在您的ide中
这非常强大，正如您在视频中看到的那样
正如您在这里看到的 我们可以在yaml文件中定义一些连接和变量
甚至可以定义dag在下次运行时应该执行的日期
您有这些验证测试
这些测试对于确保您的dag没有导入错误非常有用
就这样继续 比如一些基本的错误
但也要定义自定义dag规则
例如 你想要确保你所有的dag都有retries参数设置为2
所以所有任务应该在失败前被重试两次
那么你可以在这里做
或者你想要确保所有的dag都有描述
你可以在那些验证测试中定义
所以如果你想要确保你的dag遵循一些共同的规则
那些验证测试对于这一点非常好
然后你有单元测试
我确信你已经知道它们了
但在Airflow的背景下，单元测试的想法是检查自定义钩子和操作器
以及由Python操作器或等效的任务装饰器使用的函数
例如，你有一个Python函数
你的Python操作器运行它
然后你想单元测试那个Python函数
或者你又创建了一个自定义操作器
你想创建一个单元测试来验证你的自定义操作器按预期工作
然后是最后 但并非最后，你有dag集成测试
所以集成测试的目的是验证你的任务
你dag的流程符合你的预期
所以想法是运行你的dag与任务
然后你看看如果x来工作如预期
如果一个任务能够从之前的任务获取xcom等等
所以你真的很想知道如果流程最终符合你的预期
所以这就是为什么集成测试通常要复杂得多
比之前我们看到的两种类型的测试
让我们看看测试在实际中的表现
所以回到您的代码编辑器
如果您打开DAG
记住我们检查了大小并且检查大小运行了以下函数 _check_size
如果您看一下该函数在include ten tasks中
记住我们打印了大小并且引发了Airflow失败异常
现在你必须进行更改并添加这个条件
因为我们只想在之前的任务的响应大小低于或等于零时引发Airflow失败异常
所以如果我们没有收到任何响应
所以确保您添加了这个条件
所以一旦您完成了
您可以转到测试文件夹
然后打开DAG文件夹并打开测试DAG示例
所以该文件对应于DAG验证测试
更具体地说
该文件用于确保所有DAG都有标签 重试设置为2并且没有导入您的
然后您可以看到测试是用pi测试编写的
但您可以使用您想要的测试框架
然后我们有一些函数来获取一些导入错误
来获取DAG从DugBag
DugBag是Airflow的一个技术概念
但它实际上就像一个包含您的DAG的袋子
然后我们有测试
所以第一个是文件导入测试
用于验证在DAG文件中不存在任何导入错误
然后您有DAG标签测试用于验证您的DAG是否有标签
如果您设置了一组标签变量
然后测试将验证您的DAG是否有仅批准的标签
否则测试将失败
最后
但我们有测试重试
用于确保您的DAG重试参数设置为2
否则测试将失败
所以这个文件真的很有趣
因为它给您一些示例以便验证您的DAG并运行测试
这很简单 您通过终端运行以下命令ao dev pi test
Dag验证
调试 只是为了确保您得到所需的所有输出请按Enter并等待
所以正如您所看到的 我们有运行pi测试
这将花费几秒钟
现在我们得到了一些错误
实际上有两个测试失败了
所以如果我们看一下这些测试
第一个对应于ecom DAG
并且更具体地说，数量免费重试
因为我们将重试次数设置为1而不是2
所以让我们打开ecom标签
然后在默认ar中设置为
让我们看看其他失败的测试
这是测试像标签
如你所见 我们没有任何标签
这就是为什么我们从提取器标签中收到那个错误
所以我们需要定义一个标签
所以让我们说标签等于
然后保存文件
再次运行命令
这次它起作用了，所以这是为dag验证测试
但如果你仔细看看日志
我们还有一个来自单元测试的测试
所以如果你打开单元测试文件夹
然后测试检查大小
这是一个单元测试，以验证在分数检查大小中
我们拥有的测试在这里按预期工作
好的 我不会在这里深入细节
看看单元测试的文档
所以你可以看到这里 我们正在使用新的单元测试，而不是pi测试
但我们有一套测试来验证
例如，为了验证内容大小等于0
然后任务将引发一个异常
或者内容大小为-1
这永远不会发生
但谁知道呢
任务将引发一个never fail异常
我想强调的是
这里是我们在做xcom池
因为如果你看看任务
我们正在使用这个方法
x compo来自ti
这是一个任务实例
对象并且任务实例对象只有在你运行dag和所以你的任务时才可用
但在测试的上下文中
你不想运行你整个dag和所有任务
为了验证一个特定任务工作
至少不是为单元测试
并且这就是为什么我们使用mock库
所以mock的想法是改变一些现有方法的行为
以便返回或像你希望的那样行动
例如，在这里你可以找到x can pool方法它将返回值0
同样在这里我们正在更改x compo的行为以返回值-1
所以mocking在这方面非常有力
它帮助你修改现有的
好的 最后的 但我最想展示给你们的可能是dag test
记住 dag test是一种方法
它允许你在IDE的调试器中调试你的管道
这非常强大，因为你的dag在一个单一的串行Python进程中运行
它不需要Airflow调度器来运行
因此 看一下提取器dag
然后到文件的底部
在这里你需要创建一个新的变量
让我们假设我的提取器
然后这里你输入如果名称等于
主
那么你想运行我的提取器点测试
好的 这允许你作为Python文件运行dag文件
然后我们想运行这个来自dagger的方法测试
所以如果你保存文件然后点击你的调试器
然后这里你可以点击运行和调试
但在做那件事之前
有几项先决条件，你不必做它们
顺便说一下，你可以只看视频
因为最终，如果你在Windows上，可能会更复杂或更少
例如 如果你在macOS或Linux上
但只是为了向你展示它如何工作
首先，你必须安装Apache Airflow Python包
记住，我们现在是在Docker容器中运行Airflow，而不是直接在我们的计算机上
这就是为什么在这里你必须安装Apache
Airflow
好的 所以这是第一步
然后第二步是将Python路径变量导出
你可能已经设置了一个
首先用下面的值来验证
在我这里是斜杠私人斜杠mp斜杠课程
你应该将Python路径导出到你的Airflow项目中的路径
好的 否则你会在这里从包含文件夹中遇到一些问题
然后是最后 但并非最后，您需要导出 airflow 家园变量，再次
Airflow 项目的路径
最后注释以运行 airflow db migrate
以便创建将用于 dag 的 sqlite 数据库
我们将使用 dag test 运行
所以请记住
即使我们不需要 air force key liner
我们仍然需要一个数据库
所以运行命令
然后你可以看到在左边
现在我们有了这个轻量级数据库
有了所有这些，我们可以回到调试器
然后在任务中设置断点
让我们说在这里，然后吃调试器
在调试器中运行
正如你所看到的
Dag的执行在断点处停止
在左边我们可以查看由我们的任务使用的变量
在这个情况下，任务实例对象
例如 我们可以看到任务实例对象包含Daid
Dag模型 等等
再次 这是非常强大的，可以调试你的任务和Dag
不需要运行Airflow调度程序
所以让我们继续
现在Dag已完成 这就是你可以测试你的Dag和任务的方式
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/45_Udemy - Apache Airflow The Hands-On Guide p45 1. The right way of grouping tasks.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


你的食物摄入量可以变得巨大
正如你在这个例子中所看到的
这很疯狂
你有多少任务
但这并不疯狂
相信我 你有比这更复杂和更大的任务
所以，在这个视频中，你将发现如何使用任务组来分组你的任务
让我们深入探讨
让我们从定义什么是任务组开始
任务组将任务组织成组，在你的DAG中
它们允许你做很多事情
首先，它们可以更好地组织复杂的DAG
例如 你有这个包含许多任务的DAG
正如你所看到的
你可以将它们分组
这将使管理和可视化你的DAG更容易
因此，任务组对此非常有用
此外
你可以将默认参数应用于一组任务
例如
如果你想为这组任务应用默认参数
你可以这样做
例如，你可能想为这组任务设置重试次数为1
而如果你想再次为这组任务设置重试次数为2
你可以通过使用任务组来实现这一点
因此，通过任务组，你可以在任务级别而不是在DAG级别应用默认参数
正如我们之前所做的那样
此外，使用任务组
你可以动态地对一组任务进行映射
这更先进一些
你将在动态任务映射视频中了解到这一点
但你将能够根据动态输入生成任务组
你将在后续课程中看到
然后，最后 但不是最不重要的，你可以将任务模式转换为模块
也就是说，你可能决定这组任务
例如，这组任务应该被共享到不同的DAG中
你可以创建一个包含任务组的Python文件
然后在不同的DAG中导入任务组
这样
你就可以使用任务组创建模块化
这是一个相当强大的概念
现在你知道什么是任务组了
让我们深入代码
好的 如果你打开DAX文件夹中的提取或DAG
我们想在checksize之后添加一个任务
以验证API返回的JSON值包含的字段
我们希望如此
如果你在终端运行那个 DAG
然后输入 astro devon dags
测试提取器和一个过去的日期
顺便说一下 确保你的环境正在以 astro dev start 运行
然后如果你查看终端
好的 所以你可以看到 DAG 已经成功执行
现在你可以打开 docker desktop
并且如果你选择你的环境，就像你可以在这里看到的
交叉并打开调度器容器
然后转到文件并查看 tp
你应该能够看到 cocktail.json
如果你用文件打开那个文件
你可以看到一种鸡尾酒和如何制作它
就像那种鸡尾酒的食谱
所以我们在这里想要验证
这个 JSON 值包含所有那些字段
好的 所以 i dra 你的饮料
你的饮料替代品等等
所以我们想要验证所有那些字段都在 JSON 值中
让我们计算数据
然后回到协调器这里
我将使用 AI 创建那个附加任务
当然，你会在 udemy 的视频下找到代码
让我们做那个
创建附加
任务与 Python 操作符
检查 JSON 数据，存储在由数据集给出的位置
并且那个数据集是集合 cocktail
包含以下
字段
好的 让我们看看它是否起作用
并且我们也可以添加那个文件
就像那样设置
然后回到提取器
进入并让我们看看 AI 是否起作用
好的 所以我们有验证鸡尾酒详情
blah blah
好的 所以我们可以看到变化这里
我们有另一个任务
验证字段看起来不错
让我们接受那个
然后如果我们转到任务文件
我们有这个新的Python函数，包含所需的字段
所以我们可以在这里看到所有字段，完美
看起来不错
好的 让我们接受那个文件
现在，我们可以关闭那个
在这里，我们要做的就是删除那个，使用数据集
所以数据集cocktail点uri像这样
好的，现在我们有了任务
让我们验证它是否正常工作
为此，您需要回到终端并运行与以前一样的指令
让我们等一会儿
正如你所看到的，它完美地工作
让我们打开UI，看看卡片的外观
如果您点击箭头
然后转到图表视图
您可以看到，现在我们有检查大小和验证字段
因此，我们希望在这里将这两个任务分组
为什么，因为它们遵循相同的目的
它们想要验证get cocktail返回的数据是有效的
因此，将那些任务分组是有意义的
回到代码中，创建任务组非常简单
让我们转到extract or die
在这里，您需要导入另一个装饰器
那就是任务组装饰器
然后，在这里，使用任务组装饰器
然后在get cocktail后
您需要创建一个函数
让我们称其为group checks
然后在那个Python函数中，您需要添加两个Python操作任务
检查大小和验证字段，就像这里显示的那样
在任务组中，您还需要指定依赖关系
您仍然希望首先运行检查大小，然后运行验证字段，最后运行
但是，最后，您需要删除这里的这两个任务
然后，您将调用checks
好的 现在，您有了get cocktail
这是任务，然后是任务组中的检查任务
在这里，这就是任务组
在任务组中，您有检查大小和验证字段
然后，您将首先运行检查大小
然后运行验证字段，正如依赖关系这里定义的那样
请记住这一点
在这里，我们处于DAG级别
在这里，我们处于任务组级别
如果您保存文件并返回到Foi
然后刷新页面
现在，您有一个group checks
并且您可以像这里所示的那样展开该组
并且您可以看到两个任务
检查大小并验证字段
正如你所看到的，这真是太棒了
非常容易使用
这将使你的dag更容易调试
监控 可视化
所以我强烈建议您利用任务组
关于任务组的一些更多事项
例如 如果您只想为这组任务应用默认参数
在任务组参数中非常容易
您可以添加默认ox
然后这里 假设您想要重试两次
然后您将重试设置为2
实际上设置为3
所以现在所有这些任务都会在失败前重试三次
而您的dag中的其他任务将如此处理
好的 这相当强大
如果您想再次为任务集应用默认参数
让我们移动到另一件事
您可以肯定在任务组中包含任务组
您可以创建嵌套任务组
如果您想要的话
我不建议您这样做
但您可以这样做
所以 只是为了给您一个例子
让我们在任务组中创建一个任务组
再次使用任务组装饰器
然后让我们说嵌套任务组
然后我们在其中添加验证字段
在这里我们需要调用任务组
像这样调用嵌套任务组
好的 正如您在这里看到的
现在我们有一个任务组内的任务组
保存文件
返回UI
让我们刷新页面
现在我们打开检查
我们有另一个任务组
在这里我们有验证字段
好的 我最后想讨论的是任务
任务组中的任务ID
假设您有另一个任务
假设存储值
并且您想从前一个任务中获取数据
那么让我们假设 验证字段
你将要做的是 ti 然后是 x compo
然后任务id等于任务验证字段
但如果你这样做，这不会起作用
为什么，因为你必须记住，任务组中的任务有一个前缀
所以这种情况下的前缀是任务组的名称
所以这里你应该做 something like that checks
点验证字段，以便从任务验证字段获取 xcom
那就是它在任务组中
好的 所以记住，当你将任务添加到任务组时
Airflow会自动将任务id的前缀添加到任务id中 通过使用任务组的名称
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/46_Udemy - Apache Airflow The Hands-On Guide p46 2. Choosing tasks with branching and conditions.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 你将发现如何根据条件选择一项任务还是另一项任务
这就是分支
让我们发现这一点好吗
假设你有以下数据管道非常简单，任务a，然后任务b，然后是任务c
并且你有以下依赖关系
像这样
所以现在你想要做的是，如果任务a中有些事情是真实的，那么只运行任务b，否则你想要运行任务c
In this video
你将发现如何根据条件选择一项任务还是另一项任务
那么让我们假设您正在处理一些数据
如果数据超过1GB
那么你想运行任务B
但如果数据低于1GB
那么你想运行任务C
那么你如何做出决定呢
这就是你需要使用for task a的地方
分支操作符do
分支操作符允许您在airflow中进行分支
它允许您选择一个或多个任务
根据分支操作符中定义的条件
在这种情况下，如果数据大于1GB
则运行b
否则运行任务c
为此，您只需运行您想要运行的任务ID
现在让我们看看如何将此实现回到代码中
我们将在任务组之后添加一个分支Python操作符
以运行一个任务
如果鸡尾酒是酒精的
则运行一个任务
如果这不是
我们将运行另一个任务
如果你看一下API
我们有这个存储酒精的领域，基本上取决于那个领域
我们将运行一个任务或另一个任务
使用分支Python操作符来使用分支Python操作符
这相当容易 你需要使用任务装饰器
所以在文件的顶部添加任务
然后在这里任务组之后，我们希望添加一个新的任务@task.branch
这就是分支Python操作符
然后我们创建任务
假设分支鸡尾酒类型
在这里我们需要读取数据
所以读取数据 我们可以使用数据集，就像我们之前所做的那样
所以以open数据集
鸡尾酒uri
然后读取作为f
在这里我们希望加载数据
所以使用json.load（f）并且我们需要导入json
所以我们可以在顶部这里做
然后一旦我们有了数据
我们只是想验证鸡尾酒是否含酒精
所以如果data
你的alcoholic
所以正如这里定义的
等于alcoholic
那么我们想要运行任务alcoholic drink
否则我们想要运行任务
无酒精饮料现在您可能会想知道那些值对应什么
alalcoholic drink在这里对应于我们将要运行的下一个任务的任务id
如果鸡尾酒含酒精否则
那么我们想要运行任务无酒精drink
所以，我们需要创建那些任务
所以，在任务分支之后，我们可以再创建两个任务
使用任务装饰器和python操作符
让我们创建alcoholic drink
并且我们现在只是打印一条消息
所以打印
并且我们做同样的事情对于无酒精drink
所以任务
无酒精drink
然后我们打印无酒精
好的
然后，最后但同样重要的是
我们需要定义依赖项
并且我们可以使用这里分支的鸡尾酒类型任务
像这样并且我们想要运行alcoholic drink或无酒精drink
并且为此我们需要创建一个列表
一个任务列表
并且在那个列表中放入alcoholic drink和无酒精drink
好的
记住，我们在这里使用任务装饰器
实际上这是python操作符
但是因为我们使用任务装饰器
我们必须使用括号，正如你所看到的这里
但是，因为获取鸡尾酒没有使用任务装饰器
但是直接使用了python操作符，正如你所看到的这里
我们不需要使用括号
好的 所以，如果你对此感到困惑
这就是解释
好的 所以，到这个点，我们能够根据数据中的值选择一个任务或另一个alcoholic drink
或无酒精drink
如果鸡尾酒含酒精
并且这就是你在测试DAG之前进行分支python操作符的方式
别忘了在存储alcoholic之前添加这
不要忘记在存储alcoholic之前添加这
正如我们要在API中验证一种饮料
好的 所以现在你已经保存了文件，然后回到UI
然后点击Dag提取器，然后图形
正如你所见
我们有分支Python任务和两个任务
酒精饮料和非酒精饮料
让我们运行Dag看看它在页面上是否起作用
让我们等一会儿
好的 正如你所见，如果你刷新页面
我们可以看到有一个鸡尾酒
一种酒精鸡尾酒
因为酒精饮料已被分支鸡尾酒类型任务执行
而非酒精饮料因饮料是鸡尾酒而被跳过
这就是你可以使用分支Python操作符的方式
或者任何分支操作符，以执行一个任务或另一个
实际上 有许多其他分支操作符可以使用
例如，分支SQL操作符，根据给定的SQL查询是否返回结果进行分支
通过force
你有 分支日期周操作符，根据当前日期周是否等于给定的星期参数进行分支
等等
也假设
而不是执行一个任务
你想执行一个任务组
嗯 你可以这样做以执行它
而不是使用下一个要运行的任务的任务ID
你将使用下一个要运行的任务组的组ID
例如 这里是检查
如果你要运行检查
你将使用组ID像这样检查
好的 同样
如果你要运行一个任务，它是在任务组中
记住在这种情况下，你将需要使用组ID和任务ID
例如，如果你想运行验证字段
那么你需要输入检查
点验证字段像这样
好的 所以 请记住，这可能会有用，如果你想运行任务或任务组，直接在分支操作符中使用 直接使用分支操作符
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/47_Udemy - Apache Airflow The Hands-On Guide p47 3. Changing execution behaviours with Trigger Rules.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 你将发现如何改变Airflow中任务运行的原因
让我们开始吧 Airflow有一个称为触发规则的机制
它们用于确定任务在先前任务中应该何时运行
因此，按默认设置
Airflow在所直接上游任务都成功的情况下运行任务
在这个例子中 我们有t one
T two T three If t one and t two are successful
Then t three runs
This is the default behavior and is controlled by the trigger rule all success
So by default All of your tasks in your dags have the trigger rule all success
Now imagine that you want to run t three only if t one and t two fail
Or they are in upstream failed state
How can you do that well
In that case 你可以使用另一个触发规则
这是所有下划线失败的
现在如果t one和t two失败
然后t three运行
但如果t one和t two成功
然后t three将被跳过
你可能会想知道如果t one处于失败状态
嗯，猜猜看
t three仍然会被跳过
因为所有失败都期望直接的上游任务都处于失败状态
现在另一个触发规则是所有下划线完成
所以所有下划线完成
正如你所猜的，它不关心上游任务的状态
任务将在所有数据流任务执行完成后运行一次
无论状态如何
在这种情况下 如果t1处于失败状态且t2成功
t3将运行ts
这就是所有下划线完成的目的
所以如果你想确保任务运行
无论上游任务状态如何
一切都已完成是正确的做法
另一个规则是所有下划线跳过
所以你可以猜到这一个只运行任务
如果所有上游任务处于跳过状态
所以如果t一和t二被跳过
那么t三在t二成功时会运行
那么t三将被跳过
这就是所有跳过的后续触发规则行为
现在你有一个失败的任务
所以，所有的失败都失败了
你有一个失败
这个很简单
好的 如果你有一个直接上游任务处于淡出状态
那么任务就会运行
在这种情况下 如果一个失败
但是两个是成功的
三个运行
你有一个成功的等价物
好的 同样的事情
三个运行，因为你有两个成功的
你已经完成了一个
所以再次 无论上游任务的状态如何
如果至少有一个任务已完成其执行
那么三个运行
另一个触发规则是已知_失败
并且该触发器只会使任务运行
当所有过程任务都已成功或被跳过
例如 这里有两个成功的
并且如果一个是跳过的
那么三个运行
但如果一个是失败的
在这种情况下，三个
我们将以上游失败状态结束
如果你想知道为什么任务三个没有被跳过
嗯 这就是非失败触发规则的行为
最终如果你想知道你的任务将以何种状态结束
你必须尝试使用不同状态的上游任务进行触发规则
现在另一个非常有用的触发规则
是已知_失败_
意味着一个成功
这在分支上下文中非常有用
我们将在几分钟内看到
但只是为了向你展示这是如何工作的
基本情况下，该触发规则仅在满足三个条件时运行
所有过程任务已完成
然后没有过程任务是在失败或上游失败状态
并且至少有一个过程任务是在成功状态
如果你有两个成功的并且一个是跳过的
那么三个运行
但如果一个是失败的
在这种情况下，三个将处于上游失败状态
好的 因为你没有失败
成功一个
但正如我们所知，一个任务在失败状态
这不起作用
然后最后 但并非最后，如果两个上游任务在上游跳过状态
那么t三也会被跳过
好的 两个更多trirules
然后我们就完成了，所以首先
我们知道_它
并且触发规则
任务仅在无上游任务在上游跳过状态时运行
所以上游任务可以在任何其他状态
例如成功，失败或上游失败
例如这里
如果我们有t一个在上游失败状态
这意味着这里有另一个任务
这将在上游失败状态
好的 在这种情况下t一个在上游失败状态
因为t零失败了
然后你有t二在成功
然后t三运行
但现在如果你有一个上游任务在上游跳过状态
例如t u然后t三也会被跳过
最后一个触发规则非常简单
总是
并且你可以猜到
这意味着你的任务总是运行一旦DAG运行
如果t一个还在运行
并且这个一个在成功
然后你有t二
让我们说没有状态
好的 那么t三将立即运行
因为它不在乎 总是立即运行一旦DAG运行
总结，你有所有这些触发规则可用
如果你想要改变 一个任务运行的原因，所有失败
一个成功 一个完成
没有失败 等等
并且一些这些触发规则可能有用
例如 如果你想确保一个任务运行，以便通知你
如果你有一些任务在失败状态
例如 如果你想确保一个任务用于清理
一些你之前创建的东西
比如资源
一个集群 或者类似的东西所有的工作都完成可能是一个很好的触发器
让我们回到我们的代码看看为什么一个非失败的
意味着一个成功在分支上可能是有用的
如果你在UI上查看并查看提取数据管道
并在图表视图中选择你的图表
你会看到类似的东西
正如你看到的，最后我们有分支python操作符，我们使用它
来选择酒精饮料作为下一个任务
或者根据饮料是否是酒精饮料选择无酒精饮料
现在
我们将在最后添加一个任务
看看再次运行DAG时这个任务的情况 打开你的代码编辑器和DAX文件夹中的提取DAG
8：如果我们再次运行DAG
9：看看那个任务的情况
10：打开你的代码编辑器和DAX文件夹中的提取DAG
那么我们就去dag文件的底部
我们要做的是添加一个新的任务
一个最终的任务以删除包含鸡尾酒数据的文件
好的 请记住，这里有一个数据集叫做鸡尾酒
这个鸡尾酒对应于以下路径
斜杠gmp鸡尾酒json
我们要删除那个文件
因为我们不再需要它了
我们希望清理我们机器上所做的一切
所以这里，我们用任务装饰器创建一个新任务
然后我们可以调用它，清理数据
这里我们导入os
这是一个Python库
我们可以做类似于if os.path.exists('data/cocktail_uri')的事情
如果文件存在
那么我们想要删除它
好的 像这样
否则我们可能想要打印一些类似于文件不存在的内容
好的 所以这是一个非常简单的任务
最后，你需要将此任务添加为最后一个依赖项
在酒精饮料和非酒精饮料之后
好的
您想要确保此任务清理数据总是被执行
无论之前执行的是酒精饮料还是非酒精饮料
然而，因为我们在这里使用了分支Python雷达
您将看到一种可能不是您所期望的行为
所以让我们回来看看在您的命令行中发生了什么
运行以下命令为真
运行
然后使用图表ID运行dags tests
这是提取器和日期
假设是2025年1月1日，按回车
让我们稍等片刻，正如你所见
图表已完成成功状态
但如果你回到UI
查看提取器dags的第一个图表
你可以看到clean data已被跳过
但你想运行clean data
而clean data被跳过的原因
是因为上游任务也被跳过
在这个例子中是无酒精饮料
记住，因为我们在这里使用了分支Python操作符
我们总是至少有一个任务处于skid状态
因此clean data总是被跳过
问题是你应该使用哪种触发规则来clean data，以便即使在上游任务处于skid状态时也能运行它
因为现在你可以在信息框中看到，触发规则是所有成功
让我们在代码中修复它，回到数据管道的顶部
你可以导入一个新的类
这是触发规则类
所以你输入from airflow dot
Util dot
Tria rule
并导入触发类
然后你可以回到clean der任务
在这里你可以使用不同的触发规则，以便获得你想要的行为
为了总是执行clean data
为了在任务装饰器中指定触发规则
你使用trigger参数，然后trigger rule dot
所以在这种情况下，你可以使用one success
只要上游任务中有一个处于成功状态
那么clean data就会运行
或者你可以使用none failed
只要没有任务处于失败状态
那么clean data就会运行
这是因为我们使用了分支Python操作符
所以你不会在这里看到有任何任务处于失败状态
其他任务将处于跳过状态
或者你可以使用the non failed mean one success
我认为这是最合适的
因为你不想有任何任务处于失败状态
如果一个任务处于成功状态而其他任务处于skid状态
那么你想运行clean data
这就是当你使用分支Python时想要达到的效果
如果你总是想运行clean data任务
好的 所以 如果你保存文件并再次运行dags
相同的命令
好的 你可以看到，现在图表处于成功状态
如果你回到UI
刷新页面并选择1月1日2025年的图表
你可以看到，现在已成功执行清洁数据
这就是为什么触发器非常强大和有用
你可以真正改变你任务在DAG中的行为 你可以通过利用触发器来创建非常复杂的条件
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/48_Udemy - Apache Airflow The Hands-On Guide p48 4. Templating your tasks.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中，你将发现 airflow 中最强大的概念之一
是哪个模板
让我们深入其中 好的
首先我们需要定义什么是模板化
在Python模板化中，指的是使用模板来动态生成内容。
常常是html
通过将数据插入到预定义的结构中
允许将呈现逻辑与应用程序逻辑分离
所以具体来说 它工作的方式如下
你将会有一个模板
好的 所以想象一下这是
例如
HTML页面
但这可以是一个Python脚本
在那页上，你会有一些占位符，使用特殊的语法
例如，你可以有一个带有elo的字符串
然后有两个花括号对
这是我的价值
好的 所以这就是你的模板了
其他地方
你可能有你的数据
让我们把它放在这里的旁边
好的
这就是你的数据
这里有一些像我的价值等于世界
好的
所以这就是你的模板
让我们把它换成不同的颜色
现在你有了你的数据
当你使用模板行时
模板行的目标是替换你在静态页面上的内容
或者像Python脚本一样，用你提供的数据
例如 这里有一个静态页面，它通过模板行
然后也有你的数据
这将生成一个新的脚本，带有你好
在这里，我的值被世界所取代
这就是它工作的方式
好的 这就是模板的概念
你有你的模板，它是一个静态文件
然后你有一些数据
这个模板有一个占位符，它与你的数据中的名称相同
然后，模板和数据都通过模板化处理
这将产生最终的输出，占位符被实际数据所取代
这在Airflow的背景下是很强大的
模板化允许你在运行时将动态信息传递给任务常量
所以让我给你举个例子
想象一下，你想要从表中获取数据
使用SQL执行查询操作符
并且要从表中获取数据
你想根据日期进行过滤
但你可以猜到这里有一个问题
是的，因为日期是硬编码的
请记住，这项任务在数据管道的背景下运行
因此，由于你硬编码了日期
每次运行这个数据管道
或者每次运行这个任务
你总是获取相同的数据块
如果你的数据管道在2025年1月1日运行
那么你会得到正确的数据块
但是只要你在2025年1月2日再次运行它
那么你仍然会得到2025年1月1日的数据块
所以问题是
你如何解决这个问题
这就是你需要使用模板的地方
并且SQL执行查询操作符有一些可模板化的参数
在这个例子中，SQL参数是一个可模板化的参数
因此，使用模板等效的是这个
如你所见
这里有一个使用双大括号占位符
因此，一旦运行任务
DS将被当前DD图实际日期替换
如果你的DAG在2025年1月1日运行
那么日期将是2025年1月1日
如果你的DAG在2025年1月2日运行
然后ds将是2025年1月2日
这就是为什么模板如此有用和强大
你可能想知道ds来自哪里
嗯 猜猜看，airflow有一些可以在模板中使用的默认值
这些值如下所示
例如 你有数据间隔的开始和结束
如果你不记得这些值是什么
看看对应的视频
但是数据区间的开始
以及数据区间和相应的开始
以及你的数据区间结束，对应你的dag运行
同样 你有逻辑日期
这是delta区间开始的等价物
然后你有数据区间的开始
成功开始日期等等
甚至你有dag任务
如果你想访问你的对象
您需要为任务实例分配时间
您可以访问一些附加值，如变量等
正如您所看到的，Airflow提供了迷你变量，您可以在模板中访问它们
然后如果您的背景运行在日期上
您可以访问which是逻辑日期的等价物
所以请查看这些值列表
它们非常有用
特别是在您需要根据日期从您的数据流程中获取数据时
好的 因为您不想在任务中硬编码日期
记住，模板总是一个字符串
正如你所看到的
这个SQL查询是一个字符串而不是一个SQL查询
你也可以有一个包含你需要查询的文件和模板，这样也会起作用
但现在你也需要知道一个给定操作符的哪个参数是可模板化的，还是不可模板化的
哪个参数接受模板
要了解这一点，你确实必须查看给定操作符的代码源
例如 如果你查看Python操作符
然后在类中你有模板字段
这些字段是你可以使用模板的参数
好的 这就是你可以知道哪些参数可以在代码编辑器中返回的方式
让我们看看如何在python操作中使用模板
即使你将要看到的例子很简单
它非常有力
让我们回到数据清理的末尾，这里为了清理数据
我们只想打印一条消息，告诉我们已经清理了数据
在这里，让我们快速打印一个数据清理的日期
在这里，我们将说2025年1月1日
如果你在终端运行以下命令来运行DAG
Astro和DAGs
测试提取
然后查看2025年1月1日的日志
你可以看到我为日期进行了清理
2025年1月1日
这是有道理的，因为我们在打印中硬编码了那个日期
但由于我们硬编码了日期
如果你再次运行相同的DAG
但这次为不同的日期
就像一月二号二零二五年
在这种情况下你还是会得到一月一号二零二五年
而你不想那样
因为那只是一个打印
但想象一下如果我们从sql表中获取数据
你不想日期被硬编码
否则你将为每个图表获取相同的数据块
所以修复的方法
是使用模板，你可以简单地这样做
两对括号
这就是如何在Airflow中定义一个模板的方式
然后你会使用ds
这又是当前图表的日期
但如果你这样做然后运行egg
你可以在lindera的日志中看到什么都没有发生
就像你真的得到了带有花括号对字符串
因为ds是Python函数的上下文
你用Python操作符或任务装饰器运行
所以这不起作用
记住，你必须使用模板作为给定操作员的参数的值
好的 在这种情况下
你必须在任务装饰ri的参数中指定你的模板
和Python操作符
这里是测试装饰器
这里有几个模板参数
其中一个是templates dict
这接受一个模板值的字典
例如 我们可以创建一个包含当前日期的键的字典
然后值可以是模板ds
这样做 只要Airflow运行这个任务
这个值s会在运行时被当前图表的实际日期替换
为了在Python函数中获取模板tick
我们可以将其作为clean data的参数传递
所以templates dict
这里我们删除那一部分
我们调用模板
与键
当前日期
确保这个字符串是f字符串
所以这里放f
然后一对花括号
好的现在
如果你再次运行egg
这次你能够得到正确的日期
因为我们使用的是图表运行的日期
如果我们尝试使用不同日期
你会看到我们得到了预期的2025年1月3日
这就是你可以利用模板的方式
一个快速提示 因为我不想让你混淆
如果你好奇 为什么我们用一对花括号
这里两对花括号
这里这是一个模板
好的 这个值会在运行时被实际图表的日期替换
当任务运行时
但这里这不是一个模板
这是一个f字符串
这完全是python
所以这里的一对花括号允许我们在字符串中渲染一个变量
在这种情况下，它是来自模板的当前日期
字典参数总结
模板 有助于避免硬编码值
所以，如果您不想在任务中硬编码日期
您应该绝对使用模板
此外，模板在运行时渲染
因此，当任务运行时
占位符对应的模板被替换为实际值
最后，模板在templatable参数中定义在字符串中
使用两对花括号
您有Airflow提供诸如sti等变量
逻辑数据等，这些都是内置的
它们非常有用
您应该查看一下文档以了解列表
最后 但并非最不重要的是
所有操作器的参数都不相容 所以看看模板字段在操作码中以了解哪些是可模板化的
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/49_Udemy - Apache Airflow The Hands-On Guide p49 5. The smart way of storing data with Custom XCOM backends.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 你将发现如何存储 x 来的数据
这些是你想在其他地方共享的任务数据
然后在 airflow 数据库中
你可以存储你想要的任何数据
让我们深入探讨 好的
记住在 airflow 中
当你想要在任务之间共享数据时
你将会创建一个 xcom
这个 xcom 就像一个带有标识符和你想要共享的值的盒子
所以 例如 我们有两个任务
T 一和 T 二，然后 T 一创建 xcom
这个 xcom 默认会存储在 airflow 数据库中
然后 T 二可以从 airflow 数据库中获取那个 xcom
这就是你基本如何在两个任务之间共享数据的方式
再次强调 如果我们有这个小 xcom 这里
假设值为 42
这个 xcom 或者这个值将由 T 一推送到 airflow 数据库中
然后 T 二从这个 airflow 数据库中拉取那个 xcom
现在你在任务 T 二中有 42
虽然这行得通 这种方法有一些限制
首先你必须有一个 json 可序列化的对象
例如 如果你想要共享一个类
在这种情况下你需要实现一个将该类序列化为 json 对象的方法
否则将无法工作
但 xcom 最重要的限制是它的大小
这取决于你使用的数据库
如果你的 airflow 数据库是 postgresql
你将能够共享给定 xcom 的最大值为 1GB 数据
如果你使用 sqlite
给定 xcom 的最大值为 2GB 数据
如果你使用 mysql
给定 xcom 的最大值为 64MB 数据
但如果你想超过这个限制
如果你希望共享超过 1GB 的数据
或者如果你希望使用除 airflow 数据库以外的其他东西
那么你可以做到使用我们称之为自定义 xcom 后端
在这里，而不是使用 airflow 数据库
你将会使用一个 af 三存储桶
现在你可以存储你想要的任何数据
更重要的是，你可以利用 af 三的一些特性
例如版本控制和归档
所以让我们设置存储桶
好的 所以连接到你的aws
我 如果你没有一个
你可以免费创建一个
然后你在你的仪表板上
你点击三个
然后你可以创建一个新的桶
那个桶必须有一个唯一的名字
所以在我的案例中我将其命名为ml xcom
后台udemy再次
你不能使用我的名字
它将不会工作 它必须是唯一的
让我们保持所有默认设置
然后我们创建桶
现在你有 你的桶可以是名字
然后搜索服务
正如我们所需创建一个新政策
所以airflow可以访问您的s three桶
所以在政策中创建一个新政策
然后对于权限你去json
你可以使用下面的j
你需要更改这个值与你的桶名像这样
然后你点击下一步
你可以将该策略命名为airflow xcom
后台aws s three
然后创建策略
顺便说一下，你会在zip文件中找到json代码，视频下方
有了那个策略 下一步是创建一个新用户，我们将专门用于我们的airflow实例
所以去用户并创建用户，然后叫那个用户
让我们说airflow - xcom
然后点击下一步
然后触摸你创建的策略
所以寻找airflow xcom back aw f three
所以选择该策略
点击下一步然后创建用户
然后点击用户并去安全控制并寻找创建访问密钥
点击创建访问密钥
然后选择第三方服务然后确认
点击下一步
然后创建访问密钥并下载访问密钥和秘密访问密钥
这非常重要，你必须将访问密钥和秘密访问密钥放在某处
因为你将不会再次看到这些密钥
然后点击完成现在我们有
现在我们已经设置了三桶
以及我们将用于从airflow访问该桶的用户
下一步是安装我们需要的相应提供商，以便与该桶进行交互
所以去要求
在这里您想要安装提供程序 Apache Airflow Dash Providers
Dash 来吧
你是
然后您想要安装 Apache Airflow Dash Providers
Dash Amazon 加上三
而提供程序 Amazon 用来与桶进行交互
所以保存文件
然后您需要重启您的空气流实例
为此您可以运行 as true
他们重启如果您的空
例如 如果 airflow 已经在运行
或者您可以使用 start
所以在我的情况下
因为 airflow 正在运行
我将输入重启
所以稍等一下
好的 Airflow 正在运行
并且提供程序已成功安装
回到 fli
然后管理员连接并创建一个新记录因为我们想要创建一个新连接
让我们称其为连接
我的 aws c
然后查找 Amazon Web Services 正如您在这里看到的
您必须输入您的访问密钥 ID
以及您的秘密访问密钥 这在您之前下载的 csv 文件中
一旦您这样做了
您点击保存以创建连接与连接创建
最后一步是修改 airflow 的两个配置设置
所以回到您的代码编辑器
然后在 m v 文件. env
您需要配置此 airflow 设置 airflow on the score score core
Underscore score x come back end 为此值
以指定您想要使用 xcom 后端
然后您需要定义以下 airflow 设置
以便定义在您的 s three 桶中的路径
其中 x comes 将被存储
正如您所看到的 我们正在使用我们创建的连接 my debris underscore on
然后您需要更改为此名称
因此对于您来说它是 ml dash xcom
Backend udemy 和斜杠 xcom
这是文件夹其中 x comes 将被存储最后
但是不多您需要定义以下设置
以便定义阈值
您想要存储 xcom 在 s three 桶中
否则您想要它在 airflow 数据库中存储
所以这里 这意味着任何大于1KB的数据都将存储在三个桶中
否则 如果数据小于1KB，它将存储在Airflow数据库中
在我们这个案例中，我们希望将所有数据存储在三个桶中
所以你可以做的是在这里放0
好的 现在保存文件并重启你的f
例如使用astro dev restart
顺便说一下 如果你是第一次看到这种配置Airflow的方式
让我给您快速解释一下
所以基本上当你这样做时
你将覆盖Airflow配置文件中对应的设置
例如 这里我们有Airflow on the score score command underscore io xcom object storage path
这意味着我们覆盖了在Airflow配置设置中的命令部分下的xcom object storage path设置
以下列值
好的 现在Airflow又重新运行
让我们验证一下我们做的
为此你可以打开提取器Dag
如果你记得我们之前有一个任务
Get cocktail返回XCOM
所以如果你打开tasks.py，你会看到get cocktail
并且你会看到在这里我们推送XCOM
对应于我们从这个API获取的数据大小
因此，这个XCOM应该存储在三个桶中
所以，在你的终端运行ao dev run dags tests
你想要测试的想法
那就是提取器
然后日期
让我们说2025年1月1日
进入并稍等片刻
好的 所以你可以看到我们在使用这个我们创建的连接
并且该Dag已成功执行
让我们验证我们在S3桶中是否有XCOM
所以如果你回到你的终端并选择你的S3桶
让我们刷新桶
并且，正如你所见 我们有一个新文件夹
XCOM 打开它
然后打开Dag的ID对应的文件夹
我们刚刚触发的Dag
并且对于每个创建XCOM的任务
你有一个文件夹
所以让我们看看get cocktail
并且你可以看到创建的XCOM
我们创建的XCOM
所以现在你可以在你的任务之间共享数据
但是这些数据不会存储在Airflow的数据库中
而是存储在你的S3桶中
这样你就可以突破每个XCOM1GB的限制
如果你使用Postgres数据库
但是 要小心，因为你可以在S3桶中存储任意多的数据
但是要记住，如果你的F
例如 没有正确地设置大小来处理你想要存储和处理的数据
你可能会得到一个内存流错误
因为最终你还是要在Airflow中获取这些数据
如果你要获取TB级的数据
而你只有2GB的内存可用
那么显然它不会工作
你必须知道你如何继续处理你的数据
在本课程中 我们不会选择XCOM后端
所以为了使用Airflow数据库
你可以只输入-1
这样所有数据都会存储在Airflow数据库而不是XCOM后端
好的 所以请确保更改这个值 因为我们在课程的剩余部分不会使用XCOM后端
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/50_Udemy - Apache Airflow The Hands-On Guide p50 6. Using variables to avoid hardcoding values.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 你将学习如何避免在同一个多个dags中硬编码相同的值
通过利用Airflow中的一个简单但强大的功能
让我们深入探讨Airflow中的变量
变量是一个键值对，可以用来在Airflow环境中存储信息
例如 假设你想要存储一个数字
你可以创建一个变量，然后将这个数字
例如设置为42，变量有一个键附加到它上
并且这个键是它的唯一标识符
所以例如这里我的号码
然后你会使用这个标识符来获取你的任务中的变量
请注意，这个变量将存储在Airflow元数据库中
如果你想要创建一个Airflow变量
有很多方法
第一种是通过使用Airflow UI
所以如果你去管理并选择变量
你可以添加一个新记录并从那里创建你的变量
所以你在这里可以看到 它期望一个键
你必须指定一个键
例如我的号码和值四二
我推荐你总是定义一个描述
然后点击保存，你已经成功创建了你的变量
正如你在数据库中看到的
那个变量是加密的
所以这是值得记住的
因为变量默认存储在airflow数据库中
那个变量的值是用我们称之为fernet密钥加密的
这有点高级
但如果你怀疑你有一个有限的钥匙
它用于加密变量值并且这个钥匙
你可以在airflow的配置文件中更改它
那么，话又说回来 创建变量的第二种方法是使用airflow cli在你的终端中
你可以在终端中输入astro dev run然后变量设置你的变量的钥匙
让我们说my number two和值
让我们说八十四按回车
好的 所以，正如你所看到的，变量
我的数字下划线2已经被创建
如果你返回到fli并刷新页面
你可以看到这种新的变量
现在
创建变量的第三种方式是通过使用无关变量
所以如果你回到点环境文件这里
创建Airflow变量的一种方式是通过导出环境变量，所以
例如 你会这样做airflow
下划线
划掉我的号码
等于
假设是128
好的现在
为了获取那个变量
你必须重启你的空气流
好的 因为记住
因为它是一个环境变量
Airflow不知道这个环境变量
除非你重启你的空气流环境
所以让我们作为重启来做
好的 Airflow正在运行
让我们回到Airflow
Ui刷新页面
并且正如你所见，变量没有显示
嗯 这很奇怪，对吧
让我们看看终端
所以如果我们运行ao dev run
然后变量和列表
你可以看到，我们只有两个变量
那么你认在这里发生了什么呢，嗯
这是一个非常有趣的特性，用于创建Airflow环境，通过导出环境变量
环境变量是可用的，所以你尝试从你的DAG中获取它
你不会有任何错误
然而，你不能在UI或Airflow CLI中看到它
所以如果你想隐藏一个变量
这是一个很好的方法
使用环境变量创建的变量的第二个酷特性
你没有连接到Airflow数据库来获取那个变量
因为它在一个非生存环境之中
所以在你的环境中
你不必连接到一个数据库
好的 那就是第三种方式
创建有效Airflow的最后一种方式
是通过从DAG或任务中程序化地做它
例如 如果你在这个功能中打开任务文件在鸡尾酒
你可以从airflow点models那里做类似的事情
导入可变
然后可变点set
让我们说我的四号值是200
在这种情况下，你会创建变量
我的四号，一旦任务运行，诚实地
我根本不推荐这种方法
在我看来，你不应该那样创建变量
想象你有许多任务，创建了许多不同变量
如果你不知道哪个任务创建了哪个变量
好的 所以不要那样做
相反，使用ui或cli甚至环境变量创建一个变量
好的 如你所见，我们有这个api
而不是那样硬编码值
使用变量会更好
所以如果我们有另一只狗需要这个API
我们不必再次硬编码值
所以让我们在这里创建一个变量
我们将使用Luci
所以你可以输入astro dev run，然后变量设置键是api
然后值是这个
好的 所以让我们这样做并进入
好的 所以，正如你所看到的
您刚刚创建了一个新的API变量
可以通过输入astro dev run来验证
然后输入variables get并指定键为api
可以看到它返回以下值
如预期 现在问题是如何在获取鸡尾酒任务中获取该变量
更具体地说，在函数_get_cocktail中
您只需要做以下导入
从airflow.dotmodels导入viable
然后这里不再使用字符串，而是使用viable.get
你指定API密钥
这样你就可以在API变量中获取这个值
这样你可以在多个任务中重用API
而不必一次又一次地硬编码相同的字符串
不过你可能会疑惑，为什么不在函数外部获取值
就像那样对吗
这样我们就可以在这里直接使用API
这样如果我们有其他需要API的任务
我们就可以在任何地方获取它
问题是每次调度器通过DAG时
记住，调度器默认每30秒会执行这个操作
调度器会向你的数据库发出请求，获取那个变量
即使你不使用它
即使你的DAG没有运行
每次调度器经过你的DAG
你都会向数据库发出请求，获取那个变量
这就是为什么作为最佳实践，你不应该这样做
总是应该在任务内部获取变量，而不是在外部
否则你可能会负面影响Airflow数据库的性能
最后，我想向你展示的变量
当你将变量与模板混合在一起时，这就是发生的情况
所以如果你回到提取器DAG，然后到DAG的底部
你可以看到任务清理数据
你可以在这里看到，我们使用了带有参数模板的模板
字典 但是假设你想获取API
在这里，你可以这样做
所以你创建一个新的关键
让我们说my_api
然后从这里访问变量
你不使用viable点get，因为再次
每次调度器通过DAG
这将使API查询无济于事，不不不
相反，你将使用模板，模板如下
所以两对花括号
然后var点value
点你想要检索的变量的键
在这种情况下，api
就像那样
一旦清理数据运行
这将在你的变量中获取API
所以让我们验证它是否工作
我将复制我的API，然后打印另一个消息在这里
API与模板标记像这样
保存文件 然后运行DAG
所以在你的终端中输入ao dev run
然后DAGs tests的id
这是extractor
和一个日期
一月一号2025年，按回车
然后等待一会儿，看看它是否起作用
你可以看到DAG成功了
如果你滚动一下
我们正像预期那样获取API
使用模板
这就是关于变量的
它们非常强大且易于使用 每当你有一些字符串，它们在迷你DAG或任务中使用时，请使用它们
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/51_Udemy - Apache Airflow The Hands-On Guide p51 1. Executing tasks sequentially with the SequentialExecutor and SQLite.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中，你将发现气流中最重要的组件之一
执行者
让我们开始吧 如果你记得课程开始时
你看到了气流的组件
调度器 数据库
网络服务器等等
但要运行气流
你只需要两个组件
调度器和气流数据库
甚至不需要网络服务器
因为你如果不关心UI
仍然可以使用气流
所以我这么说
调度器与数据库通信
但有一个组件你看不到
但它存在于调度器中
而这个组件就是执行者
所以调度器内部作为一个进程
你有执行者
所以现在你可能想知道执行者的作用是什么
执行者的作用是决定任务应该在哪里以及如何运行
例如 假设你手动在本地机器上安装气流
然后通过默认方式初始化数据库
你会得到一个sqlite数据库
然后执行者默认是第二控制
这个执行者定义了任务将作为调度器进程的子进程运行
任务将顺序运行一个接一个
这是因为我们使用了sqlite，并且sqlite
不允许同时进行多个写入
所以你必须一次运行一个任务
以那个DAG为例来说明顺序执行者
这意味着获取鸡尾酒运行
然后它成功
然后你有处理鸡尾酒a、b和c
因为它们是并行的
它们可以同时运行
但因为我们使用了顺序执行者和sqlite数据库
这意味着处理鸡尾酒a必须首先运行
然后它成功
然后处理鸡尾酒b可以运行
然后一旦它成功
处理鸡尾酒c运行
最后它成功
然后处理鸡尾酒也可以运行
好的 这就是第二控制执行者的工作方式，一个接一个地运行任务
如果你想知道为什么处理鸡尾酒a首先并且那是b实际上它可能是b首先
或者可以是c
这完全是随机的
有两种气流配置设置你必须知道
如果你想要改变执行者
你用这两种设置是以下
你有气流数据库
Sql alcheon默认值序列轻气流家
所以基本上Sql lite db的路径
然后你有气流在那些concore core
在congo执行者上，顺序控制执行者
默认的执行者是哪个
显然，顺序执行者是唯一能与sqlite一起工作的执行者
记住，sqlite不允许在这里有多个权利
我将向你展示你将最终得到的东西
如果你决定手动安装airflow
你不必这样做
因为最终我不知道你的环境
这就是你可以有一些错误等等的地方
所以请看我将要做的事情，并在你的电脑上自由地做
如果你愿意 但是除此之外这不那么重要
好的 你几乎永远不会那样做
对于预测注册你将永远不会那样做
所以让我们这样做
我将设置一个python虚拟环境与uv them
然后我一旦有这个环境
我可以通过source dot them激活它
然后bin activate
好的 从那里我可以安装airflow包
我们可以使用pip安装apache airflow，现在空中流已安装
我需要导出环境变量airflow_home
记住，这是airflow用于定义sql数据库路径的
让我们这样做pwd获取当前路径
然后导出airflow_home等于这个路径
好的 所以，我一旦初始化airflow数据库
这将在当前文件夹中创建sqlite数据库
让我们通过运行airflow db migrate来验证这一点
正如你看到的，左边
我有一些新的文件夹看起来
这个文件f four db是sqlite数据库
而airflow. cfg文件是airflow的配置文件
打开那个文件 你可以访问airflow的配置设置
你会找到我刚才提到的配置设置
比如executor默认值设置为control executor
然后我们有sql lite数据库就在这里
好的 所以记住，我们可以通过导出环境变量来覆盖这些值
例如，你可以这样做
所以我们使用这两个环境变量
然后，我们在此创建新的.env 文件
然后我们粘贴这两个环境变量
如果你通过 source.env 导出这些环境变量
这将覆盖 airflow 配置文件中对应设置
好的 记住，它是如何工作的
例如，这里有 airflow 的数据库称为 scholars_score
scholars_score 使用 sql alchemy
所以，我们在此想要覆盖 sql alchemy 设置
在这里，您可以在这里的 database 部分找到它 airflow 配置文件
总结如下
Executor 促进 airflow 任务的运行，也就是任务实例
airflow 调度器决定任务何时运行
但是，Executor 决定任务在哪里以及如何运行
例如，在 kubernetes 或在本地机器上运行等
Executor 在 airflow 调度器进程中运行
如果您愿意，您可以编写自己的 Executor
但是，显然这相当高级
好的 这就是您需要了解的关于 Executor 的所有内容
默认情况下，您有 sequcontrol Executor
它按顺序运行任务，一个接一个
最后，不要忘记
您可能想知道 因为我使用 uv 创建了我的 Python 虚拟环境并安装了 airflow
查看他们的网站 您会喜欢的 它比人们认为的要好得多
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/52_Udemy - Apache Airflow The Hands-On Guide p52 2. Executing tasks in parallel with the LocalExecutor and Postgres.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中，你将发现本地执行者
实际上，我们已经使用了本地执行者有一段时间了
因为我们在使用astro cli
这是默认的执行者
让我们深入探讨 好的
所以，第二个控制执行者非常有限
是的 我的意思是你一次只能运行一个任务
你可以想象在生产环境中
这绝对不是你想要的东西
相反，另一个执行者更好，即本地执行者
本地执行者在调度进程内运行任务
但你可以在本地机器上运行尽可能多的任务
所以你看，这取决于你本地机器上有多少资源
但如果你有一台相当强大的机器
那么你可以使用本地执行者并行运行许多任务
因为你使用本地执行者并且可以同时运行多个任务
你不能使用sqlite
因为sqlite无法同时处理多个写入
相反，大多数时候你需要使用不同的数据库
以following tag为例，这将是postgretaking
为了说明本地执行者
这相当直接
第一个任务get cocktails运行
它是唯一运行的任务
然后它成功
一旦这个任务成功
进程cocktail a
B和c可以并行运行
因为我们使用的是本地执行者
所以所有这些任务
同时进行
然后也许Cocktail A会首先成功
或者B或C，whatever
但只要它们全部成功
就会触发Cocktail R
好的 所以请记住，使用本地执行者时，你的任务可以并行运行
因此你会以更快的速度完成DAGs
那么如果你使用的是第二个控制执行者
你可能想知道那两个配置设置的值是什么
嗯，猜猜看
正如我们使用命令行界面（CLI）一样
你可以自己验证这一点
所以让我们去你的代码并确保你在通常的环境中
你在课程中一直使用的那个
然后如果你打开docker桌面
你应该能看到你的空气流环境
如果你在使用和我相同的文件夹
这是课程 然后你应该看到课程和一个ID
如果你展开它，你将看到对应于你的空气流实例的Docker容器
如你所见
我们正在使用PostgreSQL数据库
我们有三个组件
调度器 触发器和Web服务器
让我们打开调度器
为此你点击它
然后你去执行并然后斜杠bin
斜杠bash
现在你在调度器的Docker容器中
更重要的是
如果你输入e和v
并且仔细查看环境变量，你可以看到
Airflow核心执行者是本地执行者
然后Airflow数据库SQL Alchemy是一个链接到PostgreSQL数据库
这就是你可以配置你的空气流实例的本地执行者的方式
你必须指定数据库
在这种情况下，PostgreSQL数据库用于SQL Alchemy
然后本地执行者用于执行者设置，顺便说一下
如果你想知道你可以使用什么数据库
如果你想使用除PostgreSQL以外的其他数据库
嗯 你必须确保这个数据库与SQL Alchemy兼容
确实，Airflow使用SQL Alchemy库来处理其元数据库
所以你可以使用Oracle、PostgreSQL、MySQL
以及Microsoft SQL Server和当然SQLite
这就是关于本地执行者的全部
老实说，关于它没有其他可说的了
它是一个相当简单但强大的执行者来使用
因此，作为最佳实践
我推荐你从本地执行者开始
并看看你能运行多少任务
如果你在某个时候感觉你不能运行你需要的所有任务
然后你可以转向更复杂但可扩展的执行者
例如 Seri执行者或Humanities执行者 你将在后面的课程中学到
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/53_Udemy - Apache Airflow The Hands-On Guide p53 3. Concurrency settings to control how tasks and dags run in parallel.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


一旦您使用与顺序控制执行者不同的其他执行者，例如本地执行者
您可以并行执行任务
这意味着您可以同时执行多个任务
由于这一点，Airflow提供了许多设置
以便您可以控制任务的并行度
所以让我们探索一些这些设置
因为它们非常重要，第一个参数是并行度
并行度定义了在每个节点上可以同时运行的最大任务数
在一个单一的Airflow环境中
例如
如果将此设置设置为3，并且您有一个调度程序 那么一次最多只能运行3个任务
在所有Dag中
例如
假设我们有两个Dag
如果您决定并行度为2
这意味着您不能同时拥有多于2个任务
处于排队或运行状态
例如，这里有一个来自Dag1和一个来自Dag2的任务
它们运行并成功
但是，这里有3个任务，因为并行度为2
所以只能同时运行2个任务
例如，可能是Dag1的C任务和Dag2的B任务
因为并行度设置为2
如果您想知道应该为并行度设置什么值
老实说
最好的办法是
如果你的任务在调度状态停留了很长时间
你可能想增加这个值
好的 但最终，
这需要大量的尝试和错误
并且最终取决于您在Airflow环境中需要运行的任务数量
接下来是第二个Airflow配置设置
您有dag_max_active_tasks
这个设置定义了同一时间可以调度的最大任务数
在所有Dag的运行中
让我们以下面的例子来说明
你有一个Dag有两个Dag运行
Dag1和Dag2的两个实例
假设你定义了dag_max_active_tasks为2
这意味着您不能同时运行多于2个任务
在所有Dag的运行中
所以Dag1和Dag2可以运行任务A
在这里我们有A和A在运行
因为dag_max_active_tasks设置为2
在所有Dag中，我们还可以同时运行这两个任务
Dag1的A任务和Dag2的A任务
然后它们成功
但是，这里有一个
Two Three four tasks that could run in parallel at the same time across the diagrams
But because we have max active tasks per dig to two
That means only two tasks can run across dagan one and diagram two at the same time
So here may be c in one runs
And then maybe b in two runs
And then as soon as for example
B from the ground to completes the task
C in two runs
And then see one completes
And finally be in diagram one runs again
That's because we have this limitation here
So keep in mind
Max active tasks per dag
Controls the maximum of tasks that can run in parallel across all the runs of a dag
And the default value is sixteen
Another setting to know is max active runs per dag
And this one is pretty simple because it determines the maximum number of active agg runs per dag
That the airflow scheduler can create at a time
So if we go back to the previous example
Let's say we have one dig with two diagra
But because we have max active runs per dag equals to one
That means only one run at a time will run so
For example Here dagan one starts running
And but tagran too has to wait until diagram one completes to start just to make that clear
Let's say a succeeds from diagram one
And then b and c run and then succeed
And finally d from the one runs and then succeed now the ground one is completed
Diagram two can start running
To sum up You have parallelism that defines the maximum number of tasks that can run concurrently on each scheduler
Within a single airflow environment
And the value is sorry to by default
Then you have max active tasks product
That defines the maximum number of tasks that can be scheduled at the same time
Across all runs of a dag
And you have the equivalent at the dag level
Remember that max active tasks per dag is in the configuration file of airflow
So you can define that at the dag level using max active tasks
And then you have max active runs per dag
That determines the maximum number of active dig runs per dag
That the airflow scheduler can create at a time
And again you have the equivalent at the dag level with max active runs
Just to show you an example at the dag level
So in your dag object you can define those two parameters
Max active tasks to two
So that means you cannot have more than two tasks running at the same time
Across all runs of this dag of my dag
And then you have max active runs to one
这意味着你不能有多于一个任务在时间上运行
我想最后讨论的参数是最大任务并行数
这个参数是在任务级别定义的
不是在Dag级别或在Airflow的配置文件中
而是在任务级别,当你定义Dag级别的最大任务并行数为一时
你的意思是数据池任务在所有Dag运行中不能有多于一个的任务实例
好的 这意味着如果你有多个Dag运行我的Dag
你不会在同一时间有多于一个数据池任务的实例
这就是关于控制你任务并发设置和运行参数的所有内容
在Airflow中
随意在你的Airflow实例中玩这些参数和设置 我在下一个视频中等你
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/54_Udemy - Apache Airflow The Hands-On Guide p54 4. Start scaling Airflow with the CeleryExecutor.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 你将发现如何使用薪水执行进行气流扩展
让我们开始吧，当你处理的任务越来越多时
到了一定时候 你可能需要对气流进行扩展
扩展气流的一种方式是通过更改你正在使用的执行者
并且不再使用本地执行者，你可以使用薪水执行者
记住，执行者决定了任务在哪里以及如何运行
在这个案例中，因为我们使用薪水执行者
我们想要使用薪水
也许你不知道什么是分布式任务队列
如果你好奇什么是分布式任务队列
任务是用于跨线程或机器分配工作的机制
这在我们的案例中与airflow有关时很重要
我们希望使用celery作为任务队列
以分配工作
以将任务分布在多个机器上
让我们看看它在概述中如何工作
你将有一个我们称之为队列的东西
这个队列可以是 reis 或 rim
Q 那么我们假设reis是我们的案例
这个队列的目标是接收技能或执行者发送的任务
在这里调度程序将任务发送到队列
然后你将有我们所谓的多个airflow步行者
所以，airflow步行者是负责执行任务的
所以，你不再有一个子进程
就像本地执行者的情况一样来执行一个任务
现在你将有一个airflow步行者
这是一个在远程计算机上运行的进程，执行您的任务
所以例如 这里有两个工作者
所以工作者一和工作者二
好的
这两个工作者将从队列中拉出任务来执行
所以从redis
这样你就可以将任务分配给多台计算机
好的 记住那些工作者是远程计算机
假设你有一个任务a
因此，任务a由执行者发送到队列中
现在，任务a的状态为排队
然后，工人1或2将从队列中拉取任务a
假设是工人2
现在，任务正在运行
然后，一旦任务成功
然后工人2将在Airflow元数据库中更新任务状态
这就是序列执行者的工作方式
显然，工人可以同时执行多个任务
实际上，这由名为worker concurrency的参数控制
默认值是十六
默认情况下，walker可以同时运行至多十六项任务
如果不够
你可以增加一个新的walker
你可以增加你的空气流实例的吞吐量
你环境中的worker越多
或者你的worker越大
你同时运行任务的能力就越强
是时候看看celery executor的运行情况了
我们不会使用astro cli
因为astro cli依赖于本地executor以简化操作
现在你需要做的是打开docker desktop
确保你没有运行任何空气流实例
如果你看一下你的容器
如果你看到类似的东西
当然 如果你看到与空气流相关的容器
请选择所有容器并停止它们
如果你查看官方的空气流文档
你会找到这个评论，它下载了官方docker compose文件以在本地运行空气流
那个docker compose文件的好处是它运行空气流
使用celery executor
如你所见
在docker compose文件中有多个服务定义
例如调度器
Web服务器
walker
触发器 Postgreand
Redis 这正是我们需要运行空气流以使用celery executor
让我们这样做
复制那个命令
然后进入你的终端
创建一个新文件夹
让我们称之为
然后进入该文件夹
在这里让我们打开我们的cody
让我们运行命令下载compose文件
如果你打开它
你可以看到不同的服务
对应于一个空气流实例
Postgre reis我们需要的队列，用于celery executor
Web服务器
调度器 walker以执行任务等
让我们运行这个docker
Compose文件，运行docker
Compose up enter
让我们等一会儿
现在 流正在运行
如果你在UI上
然后输入airflow作为用户名和密码
这与CLI中的click sign in有所不同
如你所见 你有许多DAG示例
但没有新内容
我的意思是 你不能真正看到是否在使用celery executor
然而 如果你打开下一个
在容器中寻找与你的富裕立场相对应的容器
这里是celery文件夹
因为我创建了一个salary文件夹来运行airflow
所以这可能与你的情况不同
如果你展开它
你可以看到与你的airflow实例相对应的不同airflow容器
但更重要的是
你可以清楚地看到，我们还有两个额外的组件，这是我们到目前为止没有的
第一个是经纪人立方体 reis 和执行者，用于执行任务
所以我们确实使用了系列执行者
你也可以在配置文件中验证这一点
例如 所以如果你打开调度程序对应的docker容器
然后转到exec然后斜杠bean
斜杠 bash 并键入 e envy
如果你仔细看一下 airflow 环境变量
你可以在这里看到 airflow 核心执行者
定义为系列执行者
我们也在使用一个PostgreSQL数据库，正如预期
但使用序列执行之前必须定义两个额外的设置
第一个是代理队列
你可以在这里看到，我们有Airflow3代理URL，指向Redis
然后你有Airflow结果后端，指向PostgreSQL数据库
Celery的后端用作数据库
其中任务的状态将在执行后将被存储
好的 这就是为什么我们使用与Airflow元数据库相同的数据库
好的 所以为了说明细胞执行者
我们首先会做几件事
回到docker compose文件
然后往下滚动一点
我们将更改这个设置
Airflow调用加载示例来强制
因为我们不想加载dag示例
我们将创建一个新的dig
所以现在你已经完成了
你可以打开dax文件夹并创建一个新的dag
那么我们就叫它芹菜
但是派
在这里让我们创建一个新的数据管道
所以从airflow装饰器导入dug和任务
然后从pendulum导入daytime
我们创建一个新的dag带有起始日期
让我们说2025年1月1日
然后调度可以是known和catch up to force
好的 然后我们创建一个对应于dag的功能
所以siri
在这里我们将创建一个任务
或者实际上让我们创建多个任务
所以任务a打印一条消息
让我们说a
然后我们将创建三个更多任务
像这样
然后see也是如此
好的 最后d
好的
这里没什么复杂的
想法只是想要展示当你使用serie执行者时发生了什么
然后最后但同样重要的是我们可以创建依赖关系
在这里我们想要运行a
然后我们想要同时运行bc和d
为此我们创建一个列表
并将任务放入该列表中
最后我们可以调用dug在最后
好的 非常简单
这里没什么疯狂的
但这是完美的来说明serie执行者是如何工作的
除了在任务中模拟一些工作
我们将添加一些slip
在顶部从time导入sleep
然后在任务a中我们将添加sleep十五秒
我们将做同样的事情b以及c和d
好的 确保每个任务睡眠十五秒
然后保存文件
回到docker
Compose文件 并确保您已保存文件
现在转到您的终端ctrl c c并让我们重新启动air
例如 因此您需要重新运行命令docker
Compose up
好的 看起来airflow已经重新运行
如果你返回到UI并刷新页面
你将看到你的DAG薪水
所以点击它并转到图表
然后你可以看到任务A、B、C和D
所以现在让我们启用这个DAG的切换
并触发它以查看当你使用串行执行进行DAG运行时会发生什么
为此我们将查看Walker的日志
所以在手动触发DAG之前
因为时间表是已知的
打开Codex停止
好的 确保你有权访问
触发DAG
再次打开并点击对应于Walker的容器
并查看日志
如果你仔细查看日志
你可以在这里看到
运行任务实例
Celery 手动触发
好的 现在处于Cute状态
然后你可以看到我们有额外的日志
任务实例为任务B
然后我们有任务C的日志
然后是任务D的日志
所以他们现在在Walker中正在执行
他们已经成功完成
这就是你在这里看到的成功
成功和成功
好的 这就是串行执行进行的工作
而不是在调度器中执行您的任务
您将在Walker中执行您的任务
这就是你可以在这里看到的
正确你可以看到Walker接收了三个命令
然后这些命令就在这里
所以你有Airflow任务
运行DAG中的DAG薪水
然后是任务B、D、C，然后任务被触发
你有每个任务的运行状态
最后他们在成功中
但你可能会说这太好了
现在我可以在CT机器上运行我的任务
所以想象这个Walker实际上在CT机器上运行在单独的计算机上
但像这样查看日志并不方便
所以有更好的方法监控我的空气流Walker
当我使用Celery时
有更好的方法 这就是Fluent 发现下一期视频
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/55_Udemy - Apache Airflow The Hands-On Guide p55 5. Track your tasks using Flower with the CeleryExecutor.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


当你使用薪水执行进行你的任务时
一种更好的监控你的任务方法是使用fluor和flora
这是一个开源的Web应用程序，它带有薪水
它提供了关于你的空气流工人和任务的实时信息
让我们回到你的终端，你在那里运行你的空气流实例
你需要按ctrl c或command c来停止你的实例
然后你将再次运行它，使用相同的docker compose up
但你需要添加一个选项，即--profile
然后是flu
好的 它开始
通过这样做，您将运行相同的 airflow 组件与另一个一起
这就是 fluweb 应用
一旦 f 启动并运行
您可以返回您的浏览器
然后这里寻找 local host
五十五按回车
您就在 fluweb 界面
从那里您可以获取许多有趣的实时信息关于您的工作者
任务和队列
如果我们看一下漫步者
你有一个在线研究的漫步者可用
这意味着它准备好从队列中拉取任务来运行它们
那些任务会有不同的状态
活跃 处理
失败 成功
我们尝试过
最后一列是负载平均值
并且这与在大约最后一个时间段内平均的系统运行q相当。
五和十五分钟
因此，你所处理的任务越多，你的walker就越忙
那些数字会越大
如果你点击漫步者
你将会得到关于那个特定步行者的额外信息
并且游泳池标签最重要的信息是并发最大值
我记得我在课程早期告诉过你
但是默认情况下，Airflow中的行走者可以同时运行多达十六项任务
但是如果你想增加这个数字
你可以做到，而为了做到这一点，你需要改变的只是空气流动
设置walker并发性
例如，如果你想将这个数字从16增加到更高的数字
让我们说32
你可以回到你的docker compose文件
在这里你导出以下环境变量
Airflow u underscore the score
Celery underscore score walker concurrency
在这里你输入32
好的 所以记住，通过导出那个环境变量
我们将会覆盖在Airflow配置中worker并发设置的设置
在salary部分设置值为32
所以如果你保存了文件
然后重启UI
例如用之前相同的命令
像这样，确保你仍然在低并发运行
一旦Airflow运行起来
刷新页面，回到你的walkers
然后点击漫步者
在这里你可以看到，现在
Max并发为三二
所以你可以在这个Airflow漫步者中同时运行多达三二个任务
如果你想知道这里可以放什么数字
最终取决于你的任务是否资源密集以及你的工作者有多少资源
因为记住在现实世界中漫步者可能是一台电脑
所以你会分配CPU和内存
或者它可能是一个集群中的pod
在一个Kubernetes集群中
但你会定义一些内存和CPU
所以在这里定义那个数字
你必须考虑到你的任务运行所需的资源
以及你已分配给Airflow工作者的资源
然后在池旁边你有经纪人
这就是我们使用的经纪人队列
这就是为什么你可以看到Redis
如果你记得Celery执行者作为一个队列
并且那个队列是由经纪人队列创建的
经纪人队列可以有多个队列
但是如果你查看这里的队列
默认情况下你只有一条名为default
这意味着调度程序将任务发送到这个队列default
而这个walker将从这个队列default中拉取任务以便运行它们
实际上 如果你点击控制台消费者
那么这位worker将无法运行发送到队列default的任务
别担心 我们将在下一个视频中回到队列
我想在最后向你展示的是flu的任务
所以任务允许您监控正在执行的任务
或者由给定的walker处理的任务
所以让我们验证一下
如果你回到UI并刷新页面
确保你还能访问UI
点击你的DAG然后触发你的DAG
所以让我们触发它
好的 现在正在运行
让我们回到flo并回到walker
你可以看到有一个任务正在运行，有一个任务已被处理
好的 所以你可以看到这里，现在有三个任务正在运行，四个已被处理
所以你拥有所有这些信息
实时的 你可以清楚地看到哪个步行者正在运行哪些任务
事实上 如果你点击步行者并转到任务
你可以看到有空气流任务
运行工资A 所以你知道这个步行者执行了以下任务
让我们刷新页面
现在你可以看到这个步行者的活动任务
R d c和B
好的 所以这个界面是完美的
如果你想监控你的步行者和Airflow任务
以及使用Salary Executor
现在所有的任务都已被处理
你可以通过点击任务来验证
你可以看到所有任务d c b和A，使用该步行者
这就是关于Flo
到目前为止，情况很好
但它可以更好，对吧
因为我们只使用了一个步行者
所以我们实际上看不到任务如何分配
嗯 猜猜看，下一段视频
你将添加一个新的步行者 你将发现如何使用不同的线索
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/56_Udemy - Apache Airflow The Hands-On Guide p56 6. Add new workers and configure queues to distribute your tasks.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


到目前为止，我们一直使用单一工作者来运行我们的任务
在这个视频中 你将发现如何添加另一个工作者
以及如何使用提示来分配你的任务给那些工作者
那么让我们开始吧首先
返回到你的docker
Compose文件
你需要添加另一个工作者
为此查找服务工作者
在这里找到服务空气工作者
复制那个服务
像这样并在第一个下面粘贴服务
你需要重命名这个空气工作者服务
让我们称它为空气工作者二
那基本上就是所有
这就是你需要做的
你可以保存文件
现在停止你的空气
例如，在你的终端中用ctrl c或command c停止它，然后运行你的f
例如，再次使用以下命令运行f
确保你有-dash-profile
floor it enter并稍等片刻 好的
如果你打开桌面 你应该能够在你的项目中看到额外的工作者
在这里你可以看到
你有空气工作者二和空气工作者二
你可以看到在这里
你有空气工作者二和空气工作者二
对于工作者一同样如此
更重要的是，现在f已经运行
如果你回到flour并刷新页面
让我们稍等片刻，直到它再次同步
我们有一个工作者在这里，还有一个工作者
现在让我们回到UI并再次运行标签以验证
我们是否能够将任务分配给这些工作者
让我们回到dag siri
然后触发dag
让我们看看任务是否在多个工作者上执行
你可以在这里看到，我们有一个活跃的任务在这个工作者上
让我们稍等片刻
好的 现在我们在这个工作者上有一个活跃的任务，而在这个工作者上有两个
这意味着我们能够将任务分配给不同的空气工作者
总结一下
假设你有越来越多的任务需要运行
而不是垂直扩展你的工作者
你的机器有更多的资源，如cpu和内存
另一种方式可能是添加另一台机器
添加另一个工作者来运行更多的任务
所以，你拥有的工作进程越多
你就能再次运行的任务就越多
你必须想象，你看到的每一个工作进程
都是一个远程机器
如果你好奇如何识别一个机器作为Airflow的工作进程
有一个简单的命令，那就是Airflow Celery Work
所以你只需要在这个机器的终端运行这个命令
那么这个机器就会作为你的Airflow工作进程
话说回来，有一个很好的特性你应该知道，那就是Celery Executor
因为拥有多个工作进程是很好的，但是你能够选择哪个工作进程执行哪个任务是非常强大的
那么让我来向你展示，默认情况下当你有串行执行者时
你有一个默认的q
因此所有线程
线程一和线程二在那个例子中
从那个默认队列拉取任务
假设你有两个你想运行的任务a和b
那么发生的事情是这两个任务被执行者发送到默认队列
然后可能线程一挑选任务a来运行它
然后线程二挑选任务b来运行它
但是现在想象一下，a需要的cpu比gpu多
并且b需要更多的gpu而不是cpu
然而b正在运行在第二个工人那里，它有更多的cpu而不是gpu
而任务a正在运行在第一个工人那里，有更多gpu而不是cpu
所以，你是如何运行的
任务a在第二个工人上
然后任务b在第一个工人上而不是
这就是你需要使用不同队列的地方
所以，而不是有这个默认队列
你可以直接删除它
然后你将创建两个队列
q gpu
然后队列 cpu
现在 qcpu 连接到有更多 cpu 的工人二
q gpu 连接到有更多 gpu 的工人
接下来将任务 a 发送到工人二
可以使用一个任务级别参数
这就是 q 参数
所以这里你会有像这样的东西 q 等于队列的名称
在这种情况下 cpu
然后对于任务b
你可以定义参数q
但这次
发送到q gpu
所以现在执行者将任务a发送到队列cpu
然后walker到像素
任务a从qcpu运行
然后执行者将任务b发送到q gpu
工人一号选择任务b运行
这就是如何根据各自的资源将任务转发给不同的工作者
因此，因为工人二拥有比GPU更多的CPU
在工人二上运行任务A更合理
因为ASA需要更多的CPU来运行
而ASB需要更多的GPU来运行
因此，在拥有更多GPU的工人一上运行ASB是合理的
这是一个非常强大的功能
通过Salary Executor，将不同的队列分配给不同的工人非常简单
因此，您只需打开您的docker-compose文件
然后查找Airflow Walker服务，第一个Airflow Walker就在这里
对于命令 您只需添加选项-dash queue
然后定义队列，这里
让我们说GPU
而对于第二个工人，您将使用相同的选项-dq
但这次是CPU，如果您保存文件
然后回到您的终端
然后按Ctrl + C停止
例如，最后一步是打开您的DAG
假设您想在工人一上运行任务B
在这里，您定义参数q GPU
因为q GPU与工人一连接
然后对于任务C
您将定义队列CPU以在工人二上运行该任务
然后对其他任务也这样做
您想运行
那是在工人二上
甚至让我们说任务A二
也在工人二上
好的 所以现在只有任务B在工人一上运行
所以现在如果您运行Air
例如再次
并稍等片刻，并确保所有Docker容器都在运行状态
回到Flour
刷新页面
确保您可以看到工人
如果您点击第一个工人
然后我们有页面和队列
您可以看到这是一个GPU q
如果您回到工人并点击第二个
然后刷新页面
您可以看到在队列中，这个与CPU队列连接
所以现在如果我们运行DAG
好的，触发DAG
好的
回到Flour
一旦所有任务都完成
我们应该看到第一个工人有一个任务处理，对应于GPU q
以及第二个工人有三个任务处理，对应于CPU队列
正好与我们在DAG代码中定义的一样
然后几秒钟后，这就是我们得到的结果
第一个步行者有一个任务，第二个步行者有三个任务
这里有一个问题
因为我们回到dag
想象我们有另一个任务
所以让我们在这里创建一个
让我们称之为任务 E
但这一个没有定义cue
好的 在这里，我们希望将其作为队列中的最后一个任务运行
所以如果你这样做，然后回到airflow
I
然后运行dag再次
好的 所以现在我们有任务e任务a正在运行
然后我们将有任务b c和d
好的 现在如果你仔细看一下任务e
任务卡在队列状态
为什么，好
因为记得我们创建了两个cue和cpu
并且一个步行者链接到cpu q
另一个步行者链接到gpu q
但是cpu队列呢
如果你不为任务定义一个队列
那么任务将发送到默认q
但由于默认q没有链接到任何airflow worker
没有可能运行来自该队列的任务
所以你应该总是至少有一个airflow worker
从默认队列拉取任务
所以如果你回到您的docker
Compose文件
一个很简单的解决方案是添加一个默认
例如到第二个步行者
如果你保存了文件
然后在UI
你可以简单地标记这个tarun为失败
你点击图表
然后你标记为失败
好的
确保你不在cpu和默认之间有空格
所以它应该是cpu然后默认像这样
然后设置文件并重新启动air
例如 所以ctrl c
然后运行例如再次
所以让我们等一会儿一旦airflow运行
回到flow UI
然后刷新页面
好的 再运行一次调度
看看现在我们是否能够运行任务e
让我们等几秒钟
任务a现在正在运行
任务b、c和d正在运行
最后，任务e正在使用默认队列运行
这就是关于序列执行进程的内容
这是一个非常强大的执行进程，可以将您的任务分配到多台机器上
因此，Airflow Walker和队列是一个非常好的一个特性 因此，您可以选择哪个执行进程运行哪个任务
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/57_Udemy - Apache Airflow The Hands-On Guide p57 7. Quick introduction to Kubernetes.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


你已经发现了工资执行者
现在是时候发现另一个了
这在使用airflow的很多公司中被广泛使用
这是kubernetes执行者
让我们开始吧 正如你所猜测的，kubernetes执行者在kubernetes集群上运行任务
如果你不知道kubernetes是什么
你应该绝对看一下它
但你可以把kubernetes看作是一个开源的容器编排平台
它自动化了部署过程中涉及的许多手动过程
管理和扩展容器化应用程序
实际上 谷歌写了一篇关于人文科学的非常好文章
因为它很久以前就被Kubernetes开源了
如果你好奇使用Kubernetes的好处是什么
你可以在这里看到有一些自动化的操作
基础设施抽象和服务健康监控
如果你好奇为什么不使用Docker而使用Kubernete
因为我们正在讨论容器化应用
你有关于Kubernetes与Docker的这份非常好的笔记
记住，Kubernetes使用Docker进行部署、管理和扩展容器化应用，所以这不是一个使用Kubernetes而不是Docker的问题，而是Kubernetes使用Docker来管理你的容器化应用，使其可扩展。
Kubernetes用于管理和扩展容器化应用
所以这不是使用Kubernetes而不是Docker的问题，而是Kubernetes使用Docker来管理你的容器化应用
好的 我想在这里说得很清楚
在这门课程中你不会学习人文学科
我的意思是 否则那个课程至少需要六个小时
所以，如果你想学习人文学科
请参加一个专门的课程
这是科技行业广泛使用的工具
我们将专注于你必须知道的最重要概念
以便更好地理解如何使用airflow进行人文学科
话虽如此，关于人文学科
你必须知道至少三个概念
第一个概念是概念
但关于o持有一个或多个容器
所以你可以在一个pod中拥有一个或多个容器
人类中存在的最简单的单元是pod
所以，pod中的任何容器
共享资源和网络并且可以相互通信
所以，让我们想象一下，如果我这里只有一个pod，而不是两个
这两个容器可以相互通信
它们共享同一个网络
共享相同的资源
在同一个部分
第二个概念 你知道
节点的概念
所以，节点很可能是由云服务提供商托管的虚拟机，或者是数据中心的物理机
通常，节点可以有一个或多个部分
例如，
这里我们可以有另一个垫子，上面有一些容器，你也可以有多个节点
而不是只有一个工作节点
你可能有另一个，再次对应
物理机或虚拟机
突然间，所有的
你有两台机器，上面运行着不同的服务单元，每个服务单元在不同的容器中运行
如果你仔细看一下那些节点的名字
它们被称为工作节点
这是因为kubernetes的架构设计方式
它的设计是一种主从架构
除了工作节点
你还有一个我们称之为主节点的节点
这个主节点控制服务的部署
因此，工作节点
主节点有多个组件来控制工作节点，简而言之
你有我们所说的调度器
所以 调度器负责将pod调度到工作节点上
然后你有控制器
它负责将应用从当前状态移动到期望状态
举个例子 假设你想要为你的空气流实例有两个调度器
控制器将确保你总是为你的空气流实例有两个调度器运行
想象这个容器是调度器之一
由于某种原因它被终止
控制器将检测到
然后它会确保之前崩溃的调度程序重新启动
主节点的其他组件
但现在让我们保持简单
如果我们稍微退远一点
所有这些都是你的kubernetes集群
因此，集群实际上运行由人类管理的容器化应用程序
它是一个连接在一起的一系列节点
现在你对kubernetes有了更好的了解
什么是不同的概念和组件
你可能会想知道什么是f
例如 看起来在kubernetes的背景下
嗯 它可能看起来像这样
例如 这里有三个工作节点和仍然不变的主节点
无论你在kubernetes集群中运行什么应用
但如果你仔细看看工作节点
这里有airflow组件
所以 例如，在第一个工作节点上，我们有一个运行Airflow调度器的Pod，
另一个Pod运行Airflow Web服务器
然后在第二个工作节点上
我们有一个运行Airflow数据库的Pod和一个运行消息队列的Pod
因为假设我们这里使用Seria执行器
记住，你想要在人类环境中运行Airflow，
你可以这样做，你可以使用Seria执行器
好的 它不必是Kubernetes执行器
你将在下个视频中发现
然后，你有另一个工作节点，Airflow工人在那里运行
等待执行任务
所以这是运行你的应用程序的一种方式
例如，在Kubernetes集群中
记住，它可以不同
例如，你可能只有一个Airflow组件在一个工作节点上运行
例如，只有Airflow调度程序
然后在另一个工作节点上
你可能有Airflow数据库
经纪人队列 和网络服务器
最终决定权在你，你想要如何运行你的空气
例如，在kubernetes上
说到在kubernetes上运行airflow
有一个工具我们将使用它来部署应用程序在kubernetes集群上
那就是helm
你可以把helm看作是python包管理工具的python
但这里是kubernetes
所以你有什么我们称之为图表，图表就像python包
但是再次对于kubernetes
所以你可以把图表看作是可以在人类集群上部署的应用程序
例如 如果我们点击图表然后搜索airflow
你可以找到官方图表在人类集群上部署airflow
所以你有要求
例如人类集群必须在这个版本或更高
然后helm必须在这个版本或更高
然后你有一些特性等等
所以再次kubernetes一点都不容易
这就是我们为什么使用helm来在集群上部署airflow的原因
为了设置这个集群
我们将使用docker desktop
因此，确保你使用的是docker desktop的最新版本非常重要
如果你打开它
确保这里没在运行任何容器
也要确保你有足够的资源
因为我们将要创建一个kubernetes集群
这会消耗资源，所以如果你去设置并选择资源
确保你有至少8GB的内存
否则你将无法跟随那个视频然后转到kubernetes
这就是你可以创建你kubernetes集群的地方
使用docker桌面
所以启用kubernetes功能
然后在集群设置中你将选择类型
所以确保你已经登录到docker
正如你所看到的
我已经登录 如果你没有
你将无法使用kind
所以确保你有docker next账户
并且你已经登录
然后你可以切换kubernetes版本
让我们选择最新的一个
最后你可以选择节点
我们将运行一个kubernetes集群有两个节点
然后你可以点击
应用并重启
让我们这样做
好的
现在直到你有你的kubernetes集群运行起来需要几分钟
并且运行有两个节点
所以让我们等几分钟
几分钟后你应该能看到你的kubernetes集群运行状态
因为我们使用了codex
停止创建那个kubernetes集群
如果你打开一个终端
你将有一个新的指令
这是cube l和kubectl允许你与你的kubernetes集群交互
所以我们将在课程中使用那个指令
最后一件事是你必须安装helm
记住helm允许你在kubernetes集群上部署应用程序
所以我们需要安装它
安装helm非常简单
所以你去他们的网站
然后在文档中你有快速开始指南
但更重要的是
如果你去安装helm
你有根据你的操作系统安装helm的不同方式
例如我在mac os
所以我将做brew install helm
但如果你在windows
你可以使用chocolate tree或scoop
甚至wind jet
所以取决于你
你想要哪种方式
所以让我们安装helm
我将使用brew install在我的情况下
所以回到终端
brew install helm让我们等
已安装helm
你应该能够输入helm
然后得到一些东西
例如我可以使用的不同命令
如果你没有得到那个
尝试打开一个新终端并输入helm
我们也可以验证我们的kubernetes集群是否如预期运行
为此我们可以使用cube l get nodes
你应该能够看到两个节点
一个是控制平面
记住我们有主节点控制工作节点
我们有一个工作节点
我们确实拥有一个由两个节点组成的集群
好了，到这个点你已经成功地创建了一个由两个节点组成的kubernetes集群
一个工作节点和一个主节点
你已经安装了helm以便在kubernetes集群上部署airflow
但在那之前 让我们在下一个视频中介绍kubernetes executor
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/58_Udemy - Apache Airflow The Hands-On Guide p58 8. Introduction to the KubernetesExecutor.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在介绍人文执行之前
我们必须回到序列执行之中
因为你可能会疑惑为什么
从自我执行之迁移到kubernetes执行之中
嗯 让我们发现使用celery执行之时遇到的一些缺点
所以如果你记得当我们开始使用airflow时使用celery tor
我们实际上如何运行两个额外的组件
经纪人队列用于将任务分散到不同的空中工作者
然后流体
这是用于监控您任务状态跨您 airflow 工作者的用户界面
因此您有两个额外的组件需要设置和维护
当你使用细胞执行者时
同样与细胞执行者
您有 工作者和那些工作者等待任务被推送到一个队列中以便运行它们
但在它们等待时
它们什么也不做
因此它们浪费资源等待任务被推送到一个队列中
这一点实际上取决于您如何运行您的空
例如你运行你的应用
例如，在带有细胞执行器的Kubernetes集群上
你可以实际上自动扩展你的工作者
所以 例如 如果你没有东西要运行
那些工作者可以被扩展到零
所以你不会浪费资源
但设置起来是一个更先进的架构
使用自执行器第三个缺点是管理你步行者之间的依赖关系
因为请记住，所有任务都会在共享环境中执行
在一个给定的空气流工人中
想象一下你有一个任务
让我们说任务A
并且A需要python3.12来运行
这个任务被发送到walker1来运行
并且这是可以的，因为walker1上安装了python3.12
但是假设你有另一个任务
让我们说任务b
但这一个需要python三.十一
如果调度器将任务发送给walker一
这个任务会失败，因为它需要python三.十一来走
但只安装了python三.十二在这个walker上
如你所想象的 管理依赖项可能会变得非常困难
当你有需同个python包的任务时
但是不同的版本
但并不是因为任务在同一个环境中运行
一个失败的任务可能会影响在同一个工作者中运行的其他任务
好的 当你使用单元格执行之时，任务彼此之间不隔离
所以现在你对执行之的缺点有了更好的了解
让我们看看kubernetes执行之
kubernetes执行之的目标是运行你的任务
使用kubernetes集群
在这里我们有人文科学的执行之
然后与单元格执行之的第一个不同是，你不再需要使用floor和broker cue了
好的
所以你实际上可以去掉所有这些
然后你就不再有等待任务的气流工人了
确实，使用kubernetes执行者
当任务运行时
kubernetes执行者会创建一个pod
所以你总是有一个任务
一个pod
这意味着什么
这意味着，而不是在这里有walker
你将会有一个像这样的pod
然后你在那个pod里会有你的任务在运行
例如 任务a
如果你有另一个任务需要运行
让我们再次设置任务b
Kubernetes执行进行会创建一个新的pod来运行任务b
因此 使用Kubernetes执行进行的第一个优势是
你的任务彼此完全隔离
你看到任务b在一个pod内的容器中运行
然后任务A在一个容器内运行，容器位于该pod中
这些pod可以相互通信
但它们是隔离的
也因为它们是隔离的
你可以为特定的pod分配特定的资源
例如 你可能说，这个pod需要
假设
五颗CPU和1GB的内存来运行
好的 也许任务a需要那些资源来运行
所以你将这些资源分配给pod
然后也许任务b需要
让我们说一个cpu
但5GB的内存
所以你可以看到
你可以在任务资源分配方面非常精细
不仅限于资源
也包括依赖关系
例如 你可以肯定地让这个pod在3.11上运行
因为任务A需要3.11版本的Python才能工作
然而，对于任务B
在这一部分，你可能需要3.12版本的Python
因为ASB需要3.12版本的Python才能运行
好的 所以你看 多亏了这个隔离
你有更多的灵活性来定制特定任务的环境
比使用串行执行者时更灵活
你可能想知道在kubernetes执行者中任务如何被调度
所以让我们删除那个并记住在一个kubernetes集群中
你有我们所谓的主节点
而这个主节点有不同的组件
例如控制器
调度器等等
并且主节点中的一个组件是cube api
因此 cube api的目标实际上是管理你在kubernetes集群中的资源
让我们想象一下，你需要运行一个任务a
调度程序与立方体API通信，告知它需要运行一个任务task a
因此，立方体API创建一个pod，它将用于在其中运行任务
好的 所以这就是这个舱
然后在那个pod中，你有一个运行Docker容器
在那個docker容器中，安裝了一個airflow環境
对应于你在你的kubernetes集群上运行的相同气流环境
并且以下命令被执行
哪个是使用任务ID和任务运行ID运行任务的气流
这将运行相应的任务
所以Docker容器中的pod任务A
一旦任务完成
任务状态为成功或失败取决于
Kubernetes API将删除pod
这就是Kubernetes Executor如何在您的Kubernetes集群中调度任务
你可能想知道Airflow数据库和其他东西发生了什么
它们在哪里运行呢
记住，它们也运行在它们专用的pod中
这就是Airflow数据库，它在pod中运行
好的，调度器的设置也是一样的
调度器正在运行
也在一个专用的pod中运行
好的 也许这两个pod实际上在同一个节点上运行
因为记住在kubernetes架构中
你有主节点控制工作节点
这里是工作节点1
如果你有一个只有一个工作节点的kubernetes集群
那么你的任务将在工作节点执行，其中其他组件也在运行
但是如果你有一个拥有两个工作节点的Kubernetes集群
你可能会发现你的任务在第二个工作节点上执行
所以最终取决于你的系统架构
总结 Kubernetes执行进程在每个任务中运行一个单独的Kubernetes Pod
所以你总是有一个任务一个Pod
然后Kubernetes执行进程向Kubernetes API发送消息
以创建Pod并在其中运行任务
请记住，如果技能层需要触发任务
那么Kubernetes执行进程就会与Kubernetes API通信
Kubernetes API就会创建一个包含运行任务的Docker容器的Pod
然后当一个任务完成
其Pod就会被终止，资源就会被返还到你的集群中
这与Celery执行进程不同
在Airflow工人持续运行的情况下，资源就会被持续使用
任务彼此隔离
这意味着你会有更精细的资源分配和管理
例如
你可以为一个任务分配5GB的内存和一个CPU
而对于另一个任务
你可以为它分配1GB的内存和5个CPU
并且由于它们是隔离的
你可以说他们会使用相同的依赖项的不同版本，而不会产生冲突
好了，理论够了，下一期视频
你将在Kubernetes集群上运行Airflow并运行你的第一个任务 好的
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/59_Udemy - Apache Airflow The Hands-On Guide p59 9. Installing Airflow on a Kubernetes cluster.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中，你将在你的Kubernetes集群上设置和运行Airflow
让我们开始吧 好的
在将Airflow安装并运行在你的Kubernetes集群上之前
有一些事情需要验证
首先验证你没有运行任何Docker容器，正如你看到的这里
所以如果你打开Docker桌面和容器
确保你没有运行任何容器
也确保你的Kubernetes集群正在运行
正如这里显示的那样
记住，你必须去你的设置
然后人文学科启用Kubernetes
确保你已经选择了两种节点
然后应用并重启
你应该拥有与我相同的集群，两种节点
然后这版本，所以一旦你有了这些，你可以打开你的命令行
然后验证你有访问两个命令
第一个是cube l和第二个命令是helm
Cube cl一旦你创建你的Kubernetes集群使用docker就安装了
helm必须你自己安装
为此你必须回到helm网站
然后安装helm并选择你想要的方法，根据你的操作系统
好的再次 你应该已经在之前的视频中已经完成了
所以现在让我们回到终端这里
运行命令kubectl
获取节点以验证你有两个节点对应于你的Kubernetes集群
所以这里有主节点或控制平面
然后walker节点与Kubernetes集群
就绪 下一步是添加拥有对应于Airflow应用程序的hand shot的仓库
好的 记住我们将使用helm以便将Airflow部署在我们的Kubernetes集群上
一个图表对应于一个应用程序
在这个情况下我们想要安装Airflow
所以为了获取Airflow的图表
我们需要运行以下命令，helm
然后repo Add apache
Dash airflow和链接，airflow
点apache
点org好的
所以它进入了 所以你可以看到这里，apache airflow已经存在，具有相同的配置
我已经添加了那个仓库
但你可能会收到不同的消息
一旦你完成了
你可以输入helm repo update
以确保你有Airflow helm图表的最新版本
所以它进入了，让我们等一会儿
你可以在这里看到，成功从apache airflow图表仓库获取了更新
在这一点上，我们已经获得了airflow图表的最新版本
我们有一个拥有两个节点的kubernetes集群
主节点带有一些特定的kubernetes组件
而工作节点上没有任何运行中的服务
按照最佳实践
当你有一个kubernetes集群时
总是建议创建命名空间
命名空间允许你将集群组织成虚拟子集群
所以它们是有帮助的
当你有不同的团队或项目共享同一个kubernetes集群时
所有资源都会在特定的命名空间中隔离
你可以有多个命名空间
所以在我们的案例中，我们将创建一个命名空间
这个命名空间将被命名为airflow
在这个命名空间中，我们将安装我们的airflow应用
请记住，在现实世界中，你可能有两个不同的环境
例如，一个用于开发，另一个用于生产
这两个命名空间将对应于你的不同环境
开发环境和生产环境
你有各自独立的资源
所以其中一个 例如
一个用于生产环境，一个用于开发环境
为了这个视频 我们将保持简单
所以让我们回到 airflow 命名空间
所以回到你的终端
然后让我们创建那个命名空间
所以你只需要运行以下命令 cube
Cl
创建命名空间
Airflow
就这样，你成功创建了你的命名空间 airflow
你可以通过输入 cube l get name spaces 来验证这一点
在这里，你可以看到我们刚刚创建的命名空间 airflow
好的 接下来，我们需要在那个命名空间中安装 airflow
记得要安装 airflow，我们需要使用 helm，非常简单
我们将使用 helm 来安装
你需要运行命令 helm install
然后输入你的部署名称
例如你的应用名称
比如 airflow
然后选择你想要使用的图表
比如 apache airflow airflow
记住在 helm 中图表基本上是你想要安装的应用
你想要在你的 kubernetes 集群中部署的应用
我们想要部署 airflow
然后选择你想要部署的命名空间
所以命名空间是 airflow
最后我们可以添加一个选项 -debug
只是为了在安装 airflow 时能有更好的可见性，了解正在发生的事情
现在它将花费
可能需要几分钟才能在您的 kubernetes 集群上启动 airflow
所以等待几分钟，大约两到五分钟后
你应该能看到这条消息
这基本上就是感谢您安装了 Apache Airflow，版本如下
这很有趣，因为这不是最新版本
所以我们将看看如何升级 airflow 实例的版本
我们在我们的kubernetes集群上刚刚部署了它
但是然后你会有一些有趣的信息
例如如何访问Airflow实例的用户界面
你已经在人文领域部署的
然后，使用管理员权限访问dii时，登录到troll的信用
管理员等等
但在这里做任何事情之前
让我们打开一个新标签页
在那个新的标签页中运行以下命令
使用kubectl命令获取名为airflow的命名空间中的Pod列表，请按回车键。
这就是你所有的空格，它们对应着你的空气流实例的不同组件
如果你仔细看看这些组件
你可能能够识别出当前你的空气流实例正在使用的执行者
确实 我们有ready和walker
所以如果你记得你在课程中看到的
你应该知道我们现在正在使用celery执行者
这意味着 即使你的空气流正在运行在kubernetes集群中
你肯定可以使用celery执行者
如果你想 你不必使用Kubernetes执行者
记住，Kubernetes只是用来管理你的应用程序的规模
好的 这是一个架构
这是你的选择 这不是另一个应用程序的选择
稍后我们会看到如何修改它
所以我们将使用Docker执行者而不是Kubernetes执行者
我们不再需要Walker或Broker队列
所以看起来一切正常运行
我们应该能够访问用户界面
而为了做到这一点，我们需要运行以下命令
让我们复制那个命令
像这样 在新标签页中
让我们运行该命令并按回车
好的 所以你可以看到，我们在将端口8080从kubernetes集群转发到我们的本地机器
所以如果你打开你的网页浏览器并访问localhost 8080
然后输入admin admin
你应该能够访问你的airflow实例的用户界面
但请记住，airflow实例运行在你的kubernetes集群中
实际上，你可以通过返回到你的终端来验证这一点
在这里，你可以看到与你刚刚连接到UI的连接相关的所有日志
好的 祝贺你
到这个点 你已经成功地在kubernetes集群上部署了airflow
在下一个视频中，你将了解如何将airflow升级到最新版本以及如何部署dags
因为目前我们没有任何dags，所以这将是很好的 所以运行一些
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/60_Udemy - Apache Airflow The Hands-On Guide p60 10. How to configure Airflow on Kubernetes.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 你将发现如何配置你的airflow实例
它运行在你的kubernetes集群上
然后学习如何部署一些dag
首先你必须确保你的pod正在运行
记住，pod对应于你的air
实例 并且确保你可以使用以下命令访问airflow用户界面
然后配置使用helm部署的应用程序
你需要修改一个名为values.yml的文件
这个文件描述了你的应用程序设置
例如部署的空气流量版本，使用的executor等
等等 在获取该文件之前
让我们创建一个新文件夹
让我们称之为airflow_dags
然后进入该文件夹
然后在该文件夹中执行以下命令
helm show values
apache
airflow
所以图表 我们希望将值存储在以下文件中
values yaml
击 回车
好的 现在
让我们从这个文件夹打开代码编辑器
在我这种情况下，我正在使用cursor
但请随意使用您想要的代码
然后我从这里打开一个新终端
如果你仔细看
我们有一个名为values.yml的文件
如果你打开它 你可以查看所有可以修改的设置来配置你的空气流量实例
on humanities
如你所见
我们有airflow版本
这就是我们要做的第一件事
我们想要更改airflow版本
我们使用最新的版本，即我录制视频时的版本
目前最新的版本是2.10.5
所以请这样做，并确保在这里也更改了版本
所以更改为2.10.5
并确保文件中没有任何错误
文件中的任何地方
所以你可以看到这里
这是你需要更改 airflow 实例版本的两个地方
保存文件 现在我们想通过应用到配置文件值 tml 来升级我们的部署 airflow
记住我们是使用
实际上你可以验证你当前的 airflow 部署使用
命令 helm 是 dash n 命名空间
然后 airflow 这里
你可以看到部署名称是 airflow 正如你之前指定的在命名空间 airflow
然后你只部署了一个部署到目前为止当它被更新
所以最后一次更新
然后研究部署使用了以下图表
好的 所以你使用以下图表部署了 airflow
所以现在我们有这个想法
让我们升级部署并应用在文件 values tml 中的设置
为此类型
Helm upgrade
然后 airflow 部署名称
图表 apache - dash airflow airflow 在命名空间 airflow
然后 df 文件 values tml
正如你想要将文件中的设置应用到你的新部署
然后 -
调试它进入并等一会儿
这可能需要几分钟
在两到五分钟后
你应该能看到与第一次部署 airflow 前一样的消息
感谢您安装 apache airflow
但这次是最新版本
我录制视频时
实际上如果你复制那个命令像你之前一样做并回到终端
然后这 ctrl c 停止端口转发并再做一次
然后转到你的 web brow 和 local host
八八 如果你输入 admin admin
你可以看到底部左下角我们正在使用版本 to the ten to five
如果你回到你的终端并运行与以前一样的命令
Helm
Dash n airflow 看一下你使用 helm 制作的部署
你可以看到现在版本从一增加到二
这意味着你已经部署了两次
每次你修改 values tml 中的设置
然后升级你的部署使用 helm
升级这将增加你看到的正在这里的数字
这是你知道你是否成功在 kubernetes 上部署您的应用程序的好方法
现在是你的时间
所以作为一个快速练习
尝试重新部署 airflow
但这次使用不同的 executor
kubernetes executor
所以尝试在值中找到设置tml
然后创建一个新的部署以使用kubernetes执行者
你可以暂停视频了
然后我会向你展示解决方案
所以让我们看看解决方案
首先你需要找到设置执行者
你可以在这里看到，我们有执行者celexecutor
但我们想使用kubernetes执行者
所以选择那一个并粘贴在这里
好的，从这里基本上就是这样
你可以保存文件
然后在你的终端中创建一个新的部署
使用 helm upgrade
就像你之前做的那样
输入并等待一会儿
大约两到五分钟后，您应该能看到通常的消息
感谢您安装 Apache Airflow
版本和所有内容 但现在我们应该使用 humannities 执行者而不是 salary 执行者
你怎么验证这个呢？非常简单
你只需要看一下你的空气流实例的pod
所以如果你输入cube l get pods和airflow for the namespace
可以看到airflow现在是readies
broker queue和walker不见了
为什么？因为我们使用的是humans executor
并且使用kubernetes executor
你不需要broker queue或walker
记住，每次你使用humans executor运行任务时
你将会创建一个pod
这就是它 现在你能够配置你的f
例如，在人文学科中使用helm和值文件 让我们看一下下一个视频来部署你的第一个act
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/61_Udemy - Apache Airflow The Hands-On Guide p61 11. Deploying DAGs with Airflow on Kubernetes using GitSync.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


到目前为止 你已经能够将 airflow 部署到 kubernetes 集群并配置它
然而，你还没有运行 DAGs
所以这不太有趣对吧，嗯
在本视频中 你将了解如何使用 git sync 方法来部署你的第一个 DAGs
好的 使用 git sync 部署 DAGs 的想法相对简单
首先，你需要有一个包含你的 DAGs 的 Git 仓库
在这个例子中，我们有一个包含 dags 文件夹的 Git 仓库
在我里面，我py
然后你在你的f上配置你的gsync
例如 在你的kubernetes集群中运行
会发生的是，任何需要访问dag文件的airflow组件都会有什么
我们称之为侧车
或者更具体地说
一个侧车容器是第二个容器，它运行
与主应用程序容器在同一个pod中
所以，例如
假设你有一个需要访问DAG文件的调度器
你有一个对应调度器的pod
然后在那个pod中，你有一个对应调度器的docker容器
好的
如果你配置它
源
在这种情况下，你会有一个另一个容器
侧车
那就是它同步
并且那个侧容器的作用是来同步你有的dag
在你的git仓库中，包含dax文件夹的scalar容器中
所以基本上获取同步
从 git 仓库获取奶酪痕迹
在调度容器的dax文件夹中添加那些标签
并且它在预定的时间间隔内这样做
好的 所以现在你对什么有了更好的理解
同步的方法是将你的DAGs部署在你的眼中
例如 让我们看看如何做到这一点
首先，你需要有一个包含d的git仓库
猜猜看，我已经为你创建了一个git仓库
你可以直接fork
然后你会有一个dug
平行dag 所以如果你打开dag的文件
这是一个相当简单的文件
基本上，它同时运行三个任务
这三个任务除了睡觉30秒之外什么也不做
然后最后一项任务运行，就这么简单
但是那将足够向你展示我想要向你展示的东西当你运行任务时
使用Kubernetes执行者
所以现在你有了这个仓库，再次
确保通过点击那个按钮来分叉仓库
下一步是进行配置
同步 因此它可以从这个仓库获取dag
并将它添加到调度容器中的dax文件夹中
回到你的代码编辑器
打开values.ml并搜索git sink
不是这个，而是这个，所以是这个
如你所见，在这个部分
Dags允许您配置您的同步
这里有几件事需要配置
首先，您需要启用同步
在这里输入true
然后，您需要指定仓库
您将从哪里拉取dags
那个仓库就是我们这里并行dags的仓库
所以你应该复制这个仓库
然后点击代码
然后ssh，在这里复制那个链接
然后你在这里粘贴
所以记住这不应该是我的用户名
但这应该是你的
好的 所以再次
确保你使用的是你的仓库
此外，如果你好奇为什么我们在现实生活中使用ssh而不是gps
大多数时候，你会使用ssh协议而不是https协议
然后对于分支
我们希望从主分支拉取dags
如你所见，我们现在在主分支上
这就是并行dags的作用所在
让我们在这里放上main
然后我们可以保持默认设置
在这里我们也放上main
好的 所以对于深度
我们可以默认保留
最大失败次数也是一样
这里我们有子路径
正如你所看到的
仓库内的子路径
狗所在的地方
应该是这些
如果狗在仓库的根目录
这正是我们的情况
正确 平行挖掘器在这里
没有dx文件夹
所以我们只需要像这样涂两层底漆
因为dial tag在仓库的根目录下
然后如果你继续向下滚动
你会看到像这样的东西ssh密钥秘密和ssh密钥秘密很重要
因为这就是我们要指定秘密的地方
人类秘密的秘密，包含我们的ssh密钥
所以记住，因为我们使用ssh协议
我们需要生成一个ssh密钥
以便连接到git仓库要生成那个ssh密钥
你在终端中输入ssh key工具，使用以下方式
T e two five
Five one nine
然后dc你的电子邮件地址
或者你的名字并不重要
然后你按回车
在这里你可以指定保存密钥的文件
让我们说
Udemy - airflow
repo it按回车，再次按回车
现在你已成功生成ssh密钥
如果你仔细看一下左边的文件
你有两个新文件
一个对应你的私钥，一个对应公钥
现在让我们验证你可以从这里拉取仓库
如果你回到git仓库
然后代码，然后ssh并复制
然后输入git clone与ssh链接仓库
如果你按回车
你可以看到它不起作用
那么如何修复呢，嗯
很简单 你需要回到github并在这里仓库
如果你点击代码
然后ssh你有这条消息
所以你需要添加你生成的公钥
点击 添加一个新的公钥
然后为标题你可以输入udemy -
Airflow dash repo
然后对于密钥输入
它是认证密钥
然后粘贴公钥
回到您的代码编辑器
打开公钥
然后复制那行
然后粘贴在这里
然后点击添加ssh密钥
好的 现在你应该在终端中看到你的ssh密钥
添加到你的github账户
如果你输入命令ssh -i udemy f repo私钥
然后-t get @ github.com
你应该在这里看到相同的消息
你已经成功验证
但是github不提供shell访问
这意味着你已经成功将你的ssh密钥添加到你的github账户
你可以连接到github
最后一步是运行以下命令
SSH -A add
然后你提供udemy airflow密钥
repo it输入完美
现在你已经完成了
你应该能够克隆你的仓库
如果你输入git clone
然后你去代码SSH并复制SSH链接
然后粘贴在这里，输入
你应该能够克隆仓库
如左边所示，气流标签顺便说一下，对于我的Windows用户来说
如果你出于某种原因难以生成那个SSH密钥
我推荐你跟随以下博客帖子
它详细说明了在Windows上生成SSH密钥的不同步骤
但说实话，它与我们做的非常相似
但在我的探索中，好吧
最后一步是创建Kubernetes密钥，它将存储我们的私有SSH密钥
顺便说一下 如果你不知道Kubernetes密钥是什么
它非常简单 这只是一个对象，它能够安全地帮助你存储和管理敏感信息
创建那个秘密
你需要运行命令 cube l 然后创建秘密通用
然后秘密的名称应该是 airflow ssh 秘密
实际上我们可以取消注释那行像这样
然后从文件
我们指定 git ssh 密钥
如你所见
密钥需要 是 git ssh 密钥
所以我们复制它
然后我们把它放在这里
等于我们私钥对应的udemy airflow repo文件
最后加上-d n airflow
因为我们想在namespace airflow中创建那人类的秘密
在我们airflow实例运行的地方
按回车键，正如你所见，你已成功创建了那个秘密
如果你输入cube tl get secrets
然后加上-d和airflow
你应该能在这里看到你的秘密
airflow-ssh秘密
好的 基本上我们做了这么多
我们已经配置了同步
以便使用ssh从该仓库拉取dags
并且我们为那创建了ssh密钥
然后那个ssh密钥存储在kubernetes密钥中，引用了这里
好的 所以现在一切都配置好了，让我们看看它是否会起作用
我们需要升级我们的airflow部署，以便进行这些修改
所以你输入helm upgrade
然后气流部署的名称 apache airflow
斜杠 airflow
然后命名空间 airflow 斜杠 f 值
配置文件 斜杠 斜杠 debug
回车 显然要确保你已经保存了 tml 值
让我们等一会儿，几分钟后
你应该能看到与平时相同的消息
好的 感谢您安装 airflow
但是现在如果你输入cutl
获取pod和airflow并仔细查看皮肤
你可以看到调度程序pod在运行有三个docker容器，在此之前它只有两个
如果你看之前的视频
你会看到调度程序有两个docker容器在运行
但是现在有三个docker容器在运行
为什么，因为有一个新的
那就是新的git同步侧车容器
定期拉取dag
实际上 如果你好奇它会从仓库拉取数据多少次
嗯 你需要查看 gsync 的配置设置
在某个时刻 你应该能看到它每五秒尝试拉取一次数据
好的 现在让我们看看是否有数据在我们的服务器上
例如 所以记得运行以下命令 cube l port forward
然后八十八十等等，按回车
好的 现在我们正在将端口转发到 Kubernetes 到我们的本地机器
八十八十
让我们回到我们的 Web 浏览器
好的 输入 admin admin
现在我们可以看到我们已经成功拉取了一天
并行 dag 它在我们的 git 仓库中
恭喜 因为到这个点你已经成功配置了 Kubernetes 和你的空气部署
所以你可以从一个git仓库中拉取dags，在一定时间间隔内进行操作
使用gsync
现在让我们触发那个dag，看看在人文学科中会发生什么
当你有任务正在运行时
所以开启开关
然后触发dag
好的 所以如果你点击并行标签
然后转到图表视图，然后刷新页面
你可以看到任务正在排队
所以让我们等到它们开始运行
这可能需要一些时间因为我们正在使用kubernetes executor
好的，现在任务正在运行，如果你回到你的终端
然后输入cube
Ctl get pods和airflow
你可以看到三个pod对应于正在运行的三个任务
这就是kubernetes executor的工作方式
记住每次任务运行时
kubernetes executor都会创建一个pod
这就是你可以在这里看到的
一旦这些任务成功完成
它们会消失，pod会被终止
这就是kubernetes executor的工作方式 这就是kubernetes的工作方式
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/62_Udemy - Apache Airflow The Hands-On Guide p62 1. Introduction.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这一部分 我将向你展示如何设置一个kubernetes集群
使用密钥和ren
然后我们将安装airflow
并直接从云开始使用kubernetes执行者
本节旨在给您提供一个快速概述
如何在aws中使用airflow
而不是给您提供所有关于aws的细节
例如配置您的集群等
显然我需要创建一个ario课程来涵盖这个主题
尽管如此 如果你感兴趣
请通过询问告诉我
一个udemy的部分或给我发一条私人消息
另外 你应该已经看过关于在本地使用kubernetes训练airflow的视频
从第五部分
如果没有 我强烈推荐你看他们，话说回来
让我给你概述一下这个部分
首先我们将创建一个e
C 两个实例 为了运行venture rancher，这是一个开源的多集群编排平台
简化Kubernetes集群的创建和管理
一旦e
C两个实例与renry设置
我们将快速创建用于e的iam用户S所需的仓库
用于推送我们的Docker镜像的仓库也将设置
这个仓库对于升级airflow和推送新的dags非常有用
然后我们将使用rencher创建一个并配置好的关键字集群
以便能够部署并使用kubernetes executor运行airflow
最后你将看到当使用kubernetes executor触发dag时发生的情况
好的 不再等待 让我们开始吧
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/63_Udemy - Apache Airflow The Hands-On Guide p63 2. Quick overview of AWS EKS.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 我将给您快速概述AWS EKS
您应该已经熟悉Docker和Kubernetes
所以我不会再次讨论它们
如果不 我强烈建议您查看视频
第三节中的Docker提醒和第五节中的Kubernetes提醒
话虽如此
让我们继续前进
所以AWS E是什么
如果您已经使用AWS服务
您可能听说过AECs，即弹性容器服务
AWS E是AWS E
并提供了一种特定的解决方案，用于在云中运行和管理Docker容器
而Kubernetes是一个开源的容器
调度程序，亚马逊选择了专有解决方案
AWS ECS
但在2017年，某些东西缺失了
AWS宣布支持Kubernetes，并与AWS EKS一起推出弹性
Kubernetes服务EIS是一个管理的Kubernetes服务
用户可以在AWS上轻松部署一个高可用
可扩展且完全管理的Kubernetes控制平面
与ECS一样
EIS的目标是通过管理
当你开始处理人类的痛苦时，它会让你的生活更容易
EIS现在很好地集成了AWS，并且可以与各种服务一起使用
例如Amazon
或用于容器图像
弹性负载均衡
或ELB用于负载分配
IAM用于身份验证
等等 让我看看我们通常想要的这种Kubernetes架构
我们有三个可用区A
B和C 为了保持高可用性
然后对于每个一个，我们需要设置一个主节点
一个etcd实例
这是Kubernetes的数据存储
一旦一切就绪
我们可以开始部署Kubernetes工作节点
所以您需要自己管理三个主节点
三个etcd实例
以及许多分布在不同可用区的Kubernetes工作节点
好消息是大多数事情都可能失败
这将由您来完成，所以之前
如果您真的在AWS上设置自己的Kubernetes集群
您将付出很多努力
自从管理了您的主节点和etcd实例以来，事情变得更容易了
它将自动扩展和修复您的服务，如有需要
更具体地说
它将控制平面实例分布在各个区域并检测
并替换不健康的控制平面实例
如果你不记得
什么是kubernetes的控制平面
你有你的etcd实例
Cube控制器
Cube API服务器等等正在运行
基本上，这就是你不想处理的一切
即使ek负责最大的端口
你仍然需要创建kubernetes
工作节点 以及iam工作
VPCs
子网 SSH密钥等等
我们将看到，即使这部分也可以通过使用像crcher这样的工具更容易设置
好吧
我希望现在对什么是清楚的 以及使用它之前的明显好处
在下一个视频中，我想给你一个大大的警告
确实
不是免费的 一个集群大约需要20美分每小时
即144美元每月
此外，你还需要支付e
C
两个实例 因此可能会相当昂贵
如果你忘记停止运行你的集群以及其他资源
别担心
我会向你展示如何清理一切，以及在本节结束时 好的
现在是时候前进并设置airflow 与人类执行者一起运行
aws eks 见下一集
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/64_Udemy - Apache Airflow The Hands-On Guide p64 3. Practice Set up an EC2 instance for Rancher.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


为了使用rancher
我们将设置一个e
C Two实例
为什么否则您需要具有公共IP地址的机器
以及端口22
八十和443未阻塞
这个e C Two实例不是免费试用
因为Shaneeds至少2GB的内存来运行
所以本节结束时
别忘了终止它
好的 我假设您已经登录到您的aw第二
如果不是这样
请花时间做
然后一旦您连接到
转到e
C Two服务
从那里我们将创建一个新的e
C Two实例，这是aws中用于运行应用程序的虚拟服务器
点击实例并启动实例
您可以选择许多不同的亚马逊机器图像
它们是包含操作系统的模板
应用程序设置要求您启动实例
一些是免费试用
对于其他您必须为其使用支付费用
选择第一个图像Amazon Linux Two
然后您必须选择实例的配置
例如虚拟CPU的数量
内存的数量
实例存储
等等
在我们的情况下 自从需要至少2GB的内存
我们必须选择t2.小实例类型
我们保持默认配置
没有什么可改变的
同样的存储
没有标签要添加
这里我们需要添加两个单词，除了SSH规则
以便解锁运行所需的端口A和443
然后点击添加规则
然后不是q TCP
我们选择HTTP的第一个规则
和HTTP的第二个世界
好的 点击审查并启动
好的 为了连接到你的e
C 在ssh中连接实例
如果您还没有任何对等体，您需要创建一个密钥对选择
创建一个新的密钥对
选择一个名字像airflow - 密钥
例如
不要加载守护程序
你应该将此文件保存在安全且易于访问的位置
最后，我们通过点击这里来启动实例
好的 让我们检查我们刚刚创建的实例
在这里 等待实例状态变为运行状态
这可能需要一到两分钟
现在实例正在运行
让我们连接到它
选择操作并连接这里
你有多种连接到实例的方式
但我们将选择基于浏览器的ssh连接
这样你就不需要在自己的电脑上配置任何东西
我们保持默认的用户名并点击连接
现在我们通过网页浏览器连接到了实例
为了能够开始设置rencher，我们需要执行一些其他命令
首先更新未存储的软件包和软件包缓存
通过输入sudo yum update来更新软件包
dash y
好的 然后 我们通过执行命令"sudo amazon-linux-extras install docker"来安装最新的Docker社区版。
Linux附加组件
在商店的docker
现在docker已经安装，开始使用伪服务启动docker服务：sudo service docker start
接下来我们添加一个用户e
C 两个减号用户到Docker组
为了能够无需选择伪类型伪用户模式就直接执行docker命令
Dash a
Dash j docker
E c two Dash user
And sure
And we log out by closing the window
Connect to the instance again
And the new docker group permissions have been applied in order to start venture on the instance
We just have to execute the docker one dd
Does does restart equals unless they stopped
这个名字叫牧场主
牧场主这个名字
牧场主 P :
A
那就是P 443:
443
牧场主
殖民2点
三点 2
并且运行uncher，因为docker镜像在本地不存在
Docker需要下载镜像
一旦完成 在尝试打开之前等待几秒钟
UI
好的 Docker容器正在运行
我们可以通过输入 doc ps 来检查这一点
现在将公共ip地址复制到这里
在新标签页中粘贴地址
进入
点击高级继续
在这里我们点击你想要的密码
继续并保存url
最后我们到达了美丽的vo用户界面，好了，在创建ecluster之前
我们需要创建一个新用户 让我们休息一下，并在下一期视频中见
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/65_Udemy - Apache Airflow The Hands-On Guide p65 4. Practice Create an IAM User with permissions.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 我们将创建一个新用户
因为Rancher要求从您的AWS控制台设置集群时需要设置用户
点击服务并输入i
选择它
然后转到用户并添加用户
在那里我们需要填写一些关于用户的信息
我们希望添加的第一件事是给用户名
在我这个案例中我将输入a rodash用户
然后我们需要选择用户将如何访问AWS
因为当它要求访问密钥ID和秘密访问密钥时
选择第一个选项程序化访问并继续
在这里我们可以设置对新用户的不同权限
我已经有一些组
可能并不适合你
但是别担心
你不需要它们 因为我们将直接将现有策略附加到用户
尽管如此请记住你应该创建具有正确策略的用户和组
取决于你想在生产环境中给予的访问权限
现在我们将保持非常简单
点击附加现有策略直接
如你所见AWS为您提供许多策略
在我们的情况下 我们将给新用户应用管理员策略
以便它能够访问一切
这真的只是为了测试目的
并且在生产环境中绝对不应该这样做
如果你想了解更多内容
不要犹豫在问答部分告诉我
在这里搜索admin
并选择管理员访问
然后点击下一步
我们没有任何标签要指定
审查
好的 一切都看起来不错
我们可以创建用户
完美现在用户已经创建
您绝对需要下载包含安全凭据的.dsv文件
不要关闭此文件
因为你将无法再次下载它
最后我们关闭窗口并拥有新用户
我们刚刚从用户面板创建了一个用户，很好
这部分到此结束 让我们快速休息一下并在下一集中见到你
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/66_Udemy - Apache Airflow The Hands-On Guide p66 5. Practice Create an ECR repository.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在之前的视频中
我们已经设置好了e
C Two实例，运行Rancher并创建了一个新的AWS服务用户
在这个视频中 我们将配置一个eo仓库，以便存储、管理和部署本地容器镜像
eo仓库在持续集成/持续部署(CICD)流水线中非常有用 你可以将包含标签的镜像推送到仓库，然后更新你的空气流实例
让我们从你的AWS控制台开始
查找服务或
并点击开始
需要注意的是，eo
C
或并不是免费的
但成本相当便宜 例如，每GB传输费用仅为9美分
但重要的是要知道你可能需要为使用这项服务支付费用
好的
这里
首先需要给仓库起一个名字 让我们命名为airflow 请注意，你可以在名称前面设置一个命名空间
这是最佳实践，以便根据环境（如dev、staging和prod）划分仓库
说完了，我们点击创建仓库
完成，仓库airflow已创建
现在我们可以将Docker镜像推送到它
为了推送我们的Docker镜像
我们需要安装AWS命令行界面
打开一个新标签页，搜索安装AWS CLI
点击第一个结果
页面底部，根据你的操作系统选择链接
因为我使用的是qui
点击这里，然后按照说明操作
让我们复制这个链接
然后在终端中
粘贴并执行这些命令
我已经输入了我的密码
工具已经安装
如果我输入aws --version
会得到以下输出
现在
我们需要用之前创建的安全凭证配置这个工具
我希望你还保存着shorts
点csv文件
我们将使用它
在终端中输入aws configure
然后按照提示输入信息
完成后，你就可以使用AWS CLI了 例如，你可以使用aws EC two describe-instances命令列出你的EC two实例
希望你能找到这个教程有帮助
配置
这里 复制并粘贴您从文件中获取的访问密钥ID
然后是访问密钥
您想要使用的默认区域名称
以我为例 这是你的三
我们保持默认输出格式
现在我们在easy或dashboard中完成
在页面顶部如果您点击这里
你可以再次访问推送命令
选择与你的操作系统相对应的
然后按照说明操作
所以复制这条线
把它粘贴到终端
别忘了把aw换成aws
二
和二
然后我们有了日志
已成功 下一步是构建一个docker镜像
检查你是否在material和flow章节七的文件夹中
然后打开docker文件夹
这里有一个docker文件，包含airflow配置文件airflow_at_cg_in_config
以及入口文件entry_point.dot.sh在script文件夹中
你应该对这些文件已经很熟悉了
因为它们与前一个视频中使用的文件相同
唯一的区别是，目前kubernetes包也已经安装好了
因为我们将要使用airflow与kubernetes executor
所以输入命令docker build -t airflow.
等待账单完成
好的 然后我们需要将Docker镜像导入到我们的
网络浏览器仓库中
复制这个命令
然后粘贴
将flight test替换为
给你的Docker镜像一个版本，以便你知道在你的集群中运行的是哪个版本
如果你需要回滚或添加小修修补补
总是最好遵循这种格式
好的
如果我们滚动查看
这里 这很好
现在 最后一步是从你的网络浏览器拉取docker镜像
复制最后一条命令
将其粘贴到你的终端并更改latest为v one dot zero
这个过程可能需要几分钟
好的 如果我们回到浏览器并关闭说明，任务已经完成
然后点击仓库
我们找到了带有标签v1.0的docker镜像airflow，正如我们所期望的
每次你想要拉取这个镜像
你只需要复制并粘贴这个链接，好的 这个视频就到这里 我们稍作休息，下一期再见
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/67_Udemy - Apache Airflow The Hands-On Guide p67 6. Practice Create an EKS cluster with Rancher.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在AWS的所有设置都已完成
我们可以使用Rancher创建一个Nicky集群
如果你已经看过第五部分的前几个视频
你应该已经熟悉了这个界面
因为我们之前使用过它 为了导入本地的Kubernetes集群
这次我们不是导入集群
我们将创建一个并展示它
它会为我们做这些事情
如果你准备好了
点击 添加集群从那里
它会给你很多选择
根据你有的或者你想要的Kubernetes集群类型
例如 你可以从一个现有的节点创建集群
或者使用运行你的Kubernetes引擎
这是一个经过认证的Kubernetes分发，它完全在Docker容器内运行
它运行在裸金属和虚拟化服务器上
它的目的是解决你可能已经知道的安装复杂性问题
如果你曾经尝试过自己安装Kubernetes
这是一个不错的分发
所以我强烈建议你查看一下
创建集群的另一个选项是从可用的主机Kubernetes提供商中选择
选择亚马逊
E 从那里你需要提供一些信息来设置你的集群
别担心
这将非常容易 首先为集群名称
我们输入airflow cluster
然后成员词没有什么需要更改的
标签
和注释 在账户访问面板中
这里是你需要指定安全凭据的地方
从你之前下载的CSV文件中获取
对于区域
选择与你在AWS控制台中设置的相同以检查区域
如果你去你的AWS控制台
在右上角你有最近的城市
根据你的位置 在我这里是巴黎
但显然这可能对你来说不同
点击城市
在它旁边
你有区域
在我这里是你的West
你应该指定你的区域
而不是我的back to ren
选择正确的区域
现在我们需要输入凭证短语
打开你的凭证短语CSV文件
复制访问密钥ID
并将其粘贴到这里
然后复制秘密访问密钥
并将其放在那里
完美点击配置集群，在这里你可以选择你想要的Kubernetes版本
最终选择最新版本的Kubernetes
在我这个案例中是1.14
我们保持服务滚动的默认值
接下来，对于VPC和子网也是如此
我们让Sure做所有的工作，没有什么需要改变
我们选择想要的实例类型
这无非就是e
C 两种实例类型
我们可以指定节点数量
节点的存储容量
等等 现在，我们保持所有默认设置
然后点击创建
好的 现在 创建EKS集群需要10到15分钟
在此期间
你可以从AWS控制台查看集群状态
确实，Rasa通过使用Cloud Formation模板启动安装
如果你查找服务Cloud Formation，从服务中
你可以实时查看不同堆栈的创建状态
此外，如果你查看EKS服务
你可以跟踪你EAS集群的状态
在处理结束时
你应该从Run Chaui旁边得到活跃的状态
所以我现在暂停视频
我会在完成后回来
欢迎回来 所以，经过一段时间，集群终于活跃了
如果我们点击它
我们会到达美丽的仪表板
它给我们提供了一些关于集群的基本信息
如CPU和内存使用情况
运行中的Pod数量
不同Kubernetes组件的状态
等等
在下面，你有集群的事件，这可以是解决一些错误的巨大帮助
一个非常重要的按钮是这里启动一个Cube City L会话
如果你点击它
你将连接到你的集群并可以执行任何Cube L命令
完成
例如
我们可以输入cube l版本
以显示客户端和服务器上运行的kubernetes版本
请注意版本不同
因为目前只支持版本1.14
好的 如果我们输入tul get nodes
我们可以得到节点的列表
目前我们只有一个节点
最后，kubectl get pod
减 或减namespaces
允许您列出集群中运行的所有Pod
您可以使用cube l进行许多其他操作
但我们将专注于我们需要的
我们可以关闭终端
现在你有一个美丽的eks集群在aws上设置并运行 让我们休息一下，并在下一集中再见
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/68_Udemy - Apache Airflow The Hands-On Guide p68 7. How to access your applications from the outside.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


当你在你的kubernetes集群中部署像airflow这样的应用程序时
你的网络服务器将运行在一个pod中
为了从你的集群外部访问你的网络服务器
kubernetes提供了多种不同的方法
这些方法复杂程度不同
这取决于你的使用场景
让我给您快速概述一下这些可能性
所以 假设你有一个kubernetes集群，其中pod运行着一个网络服务器
理论上你可以直接与这些pod进行通信
但如果运行你Pod的节点挂了会怎样呢
你的Pod也会随之挂掉
Kubernetes会创建新的Pod，它们的IP地址也会不同
因此 你就无法再用之前的IP地址访问你的Web服务器了
正如你所猜测的，这不太实用
最好是能有一个固定的IP地址
无论节点发生什么情况，这就是服务解决的问题
先简单回顾一下
服务是一种抽象
定义在你集群中某个地方的一组Pod
当创建时
服务获得一个唯一的IP地址
也称为集群IP

允许在您的Kubernetes集群内运行的应用程序相互通信
通常不可能访问一个集群
互联网服务

这真的不是推荐的方式
因为你需要以已认证的用户身份运行 kubectl 才能启动代理
这种方法仅用于调试您的服务
尽管如此，我确实很想提醒您一下服务
以及什么是集群
IP 说到这里
让我们转向从外部访问您集群的实际方法
将外部流量直接路由到您的服务的最原始也是最基本的方式是使用节点
端口服务 正如您所猜测的
根据其名称 节点端口服务在所有节点上打开一个特定的端口
任何发送到该端口的流量都会被转发到服务
这就是在这里的例子中你可以看到的
外部流量被发送到端口三十一万
然后转发到服务，具有访问端口的权限
如果你在思考为什么端口号这么高
这是因为你只能使用三十千到三十二千之间的端口
七百六十七
或者northport有一个重大缺陷
那就是如果一个节点的IP地址发生了变化
你将无法再访问你的应用程序
你的外部访问与节点的IP地址绑定
作为最佳实践
你不应该使用这种解决方案进行预测
它更适合于不需要始终可用的应用程序，如演示
例如
将您的服务暴露给互联网的第二种方法是使用负载均衡服务
负载均衡器取决于您的云提供商
但一般来说，您将获得一个IP地址，您可以使用它来访问您的服务
所有外部流量都将转发到服务
负载均衡器的大问题是，您每次使用负载均衡器暴露服务
都会获得一个需要您支付费用的IP地址
如果您有很多服务需要暴露
这可能会变得相当昂贵
最后，我们将看到的最后一种方法是通过使用入口
与先前的解决方案不同
入口不是服务
但它应该更薄作为一个入口点，外部流量被发送到正确的服务，根据使用的路由
流量路由由入口资源中定义的规则控制，例如
如果您从Web浏览器尝试访问此链接
那么入口将重定向您的流量到以下服务
以便返回预期的应用程序
这允许您配置多种不同的服务以满足您的需求
入口可以配置以使服务对外部可访问
URLs
负载 平衡
流量 终止 SSL TLS并且通常名为虚拟主机
尽管使用入口是暴露您的服务最强大和健壮的方式
它也可能是设置起来最复杂的解决方案
尽管如此
我强烈建议您采用这种方法 如果您计划在生产中使用Kubernetes
好了，现在 您对不同方式有更好的了解，可以暴露您的服务，并将它们从集群外部使用到 让我们继续部署引擎X入口
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/69_Udemy - Apache Airflow The Hands-On Guide p69 8. Practice Deploy Nginx Ingress with Catalogs (Helm).ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 我们将在kubernetes集群中安装nginx ingress
幸运的是
在rencher上安装nginx ingress是一个简单的任务
多亏了目录
在rencher中，目录只不过是一套helm图表
为了在不必配置一切的情况下部署复杂应用
这是一个避免创建许多yaml文件并遭受神秘错误的好方法
因为你试图自己安装一切嘛
实际上有些人确实这样做了
这就是为什么现在部署一些应用以更简单
好了，够了 闲话
让我们从run share用户界面移开
点击airflow cluster
选择全局并点击这里的应用
我们还没有运行任何应用
但首先让我们启用目录
响应官方helm仓库
点击管理目录
并点击这个按钮旁边的手目录
我们可以启用它
如你所见，它已激活并开始将所有hand图表与rencher同步
所以让我们等几分钟，直到同步完成
好的 现在完成了，如果我们再次点击应用并点击启动
我们可以看到所有应用，我们可以使用目录自动安装
这里有grafana f k
如果你想要elastic stack
Hadoop
Kafka等等
注意，每组目录都按仓库分开
它来自哪里
所以library
如果native目录是frontier
但如果我们继续滚动
我们会得到ham shorts
好的 让我们找到nx ingress类型在x中
并选择x ingress
从那里你可以得到详细描述你想要安装的应用
以及一些配置选项
首先 我们必须定义一个名称
让我们说 nginx ingress
然后，我们可以选择我们想要的模板版本，默认情况下
rencher会选择最新的一个，目标项目是我们想要运行应用的地方
这是默认项目在airflow cluster下
这里有两种升级策略选项
无论是通过滚动更新还是同时更新所有应用，我们都会保持默认值。
同时，我们会保持默认值。
最后，在可用的工作中，
我们选择集群，以便应用程序可以访问集群中的任何资源。
在这里，您可以通过values tml文件覆盖默认值。
大多数图表包括一个名为values tml的文件。
它以结构化格式提供模板的默认配置数据。
您可以通过点击预览查看此文件。
如您所见，
您可以通过answers all right覆盖不同的键和值。
点击启动。
正在安装应用程序。
点击它。
此页面显示您在kubernetes集群中部署的组件概览。
您可以检查on points。
工作负载。
入口规则。
服务，卷。
等等。 例如，
如果我们点击nginx控制器，
我们可以访问部署对象并获取更多关于运行中的pods的详细信息。
环境事件，
变量设置，
等等。
还可以通过点击此处检查特定pods的日志。
然后查看，
或者通过点击此处访问容器。
返回应用程序仪表板。
转到服务并找到nginx控制器服务下方。
您可以通过点击端口80找到端口440，
三和80。
例如，
我们可以从外部访问kubernetes集群中的engine x后端。
此消息由genx ingress为我们创建的默认后端生成。
它还提供了一个默认页面，当某人尝试访问应用程序中不存在的路由时。
现在，
入口已准备好，我们可以从外部访问集群。
让我们转到下一个视频，
我们将在那里将我们的集群安装在airflow中。 在那里，我们将在集群中安装airflow
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/70_Udemy - Apache Airflow The Hands-On Guide p70 9. Practice Deploy and run Airflow with the Kubernetes Executor on EKS.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


最后这个视频的最佳部分
我们将安装并执行
使用kubernetes executor在我们的集群中运行我们的第一个dig airflow
就像我们为nginx ingress所做的那样
我们将使用目录来在集群中安装airflow
因为我已经为使用kubernetes executor运行airflow创建了一个模板
第一步是添加包含模板的目录
点击airflow集群全局应用并管理目录
我们需要添加一个名为airflow的新目录
E
并输入URLhttps://github.com/moomeri/airflow-helm-chart.git
com
slashmoomeri
slashairflow
helm-chart
dotgit
好的现在 点击创建
检查分支master是否已正确选择，范围是否为全局
如果默认范围不是全局
关闭窗口并重复添加新目录的步骤
在点击创建之前
确认您的输入与我相同
如果一切正常
点击按钮目录已添加且处于活动状态
现在我们可以安装airflow
点击应用
启动并选择airflow kubernetes
E
就像我们为nginx ingress所做的那样
我们需要定义一个名称
让我们称之为airflow kubernetes
E s
模板版本是最新的
目标项目是默认的airflow集群
我们保持升级策略并将世界从项目切换到集群
好的点击启动以开始在集群上部署airflow
现在next到nginx ingress应用
airflow正在安装
点击它
等待工作负载
路径 调度程序和web处于活动状态并运行这个过程可能需要十分钟
所以在这段时间内
您可以点击任何工作负载以获取更多信息
我将暂停视频并回来当一切都准备好时
现在您应该与我有相同的状态
所有工作负载都处于活动状态并运行
如果您去应用
然后选择nginx ingress
点击在nginx控制器服务下方一点的地方
你将在airflow用户界面学习完成
你有一个全新的airflow安装在eas集群内部运行
使用kubernetes执行进行配置以证明我没有说谎
在执行这些之前让我们执行一些任务
从左上角选择你的集群并点击这里保持它
一旦我们连接了输入cube city
我将获取namespaces
并复制以airflow开头的namespace
然后输入watch cul
获取pods
在这里粘贴namespace
这个命令允许你实时查看你的pods
如果你触发了一个dag
你应该能从这个视图看到你的任务正在运行
在你的集群中
在ui中
打开dag的切换
在分数上打开并行 返回并回到renshow ui
正如你所见，你的任务对应的pod正在eas集群中运行
使用kubernetes执行进行配置
这就是这个视频的全部内容
到这个点你已经学会了所有需要的知识
来设置你自己的ecluster并运行airflow
并使用kubernetes执行进行配置
如果你对此更感兴趣
请不要犹豫通过udemy的q部分或消息联系我
我会很高兴回答你的
让我们休息一下并期待下一个视频 让我们快速休息一下并期待下一个视频
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/71_Udemy - Apache Airflow The Hands-On Guide p71 10. Practice Cleaning your AWS services.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个短片中 我们将清理到目前为止我们所设置的一切
这很重要
因为否则你将被收取费用，直到你不终止你的服务
让我们从错误的地方开始，向你展示顶部左上角
点击它并选择全局
然后从这个按钮点击删除
注意以下消息告诉你，与aws创建的相关资源，如卷
负载均衡器和网络将不会自动删除
所以在从rancher删除集群后
我们将检查从aws控制台一切是否被终止
确认
等待集群从rancher消失
UI
好的 现在 从那里打开你的aws控制台
点击服务并转到e
这里集群正在被预期删除
接下来 我们需要检查服务云形成
这是rancher用于设置一切的
点击服务并点击云形成
如你所见 堆栈也在被删除
记住，你可以在过程中了解到
例如 在我情况下vpc堆栈失败了
所以我不得不再次从aws删除它
接下来要看的是e
C 二实例我们为rancher创建的
从服务类型e
C 二
并从仪表板点击运行实例
选择实例
点击操作实例状态并告诉我奈特
是的
现在等待实例终止
好的 现在 让我们删除负载均衡器
好，最后去vpc仪表板
并移除由rancher创建的vpc
点击它操作
删除vpc
完美，现在一切都应该是干净的
你可以安心睡觉，不再害怕被收费 下期视频见
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/72_Udemy - Apache Airflow The Hands-On Guide p72 1. Introduction.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


监控你的空气流实例以及你的任务流应该是你的首要任务
当你在生产环境中没有监控系统时
你没有任何方式知道是否有任何事情出错
何时以及为何
基本上有一天你会来到办公室
你会接到一个电话告诉你这个团队没有按时获取数据
发生了什么而你无法给出答案
由于Airflow是你的调度器
它可以迅速成为你数据架构的杰作
所以它在这个部分值得特别的关注
我将向你展示Airflow的日志系统是如何工作的
以及如何自定义它
然后我们将开始处理远程存储
例如AWS S3
我们将了解如何使用Airflow将日志存储在Elasticsearch中
以及Elasticsearch
我们将设置文件比特
Lostash和Cubana
以便监控我们的日志并基于收到的日志创建仪表板
在使用Elk堆栈后
我们将学习气流如何产生其指标
我们可以监控哪些类型的指标，一个 tig stack 将会被设置起来
"Where ti just stands for telegraph" 翻译成中文是："ti 仅仅代表电报"
足球和grafana
这将使我们能够创建一个仪表板
为了基于Airflow发送的矩阵构建可视化。
然后我们将创建一个警报，以便在出现问题时使用grafana进行警告。
最后这个视频如果你在生产环境中使用 airflow 将会非常有用
你想要保持你的实例干净

但非常有趣 所以你仍然很差，下次视频见
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/73_Udemy - Apache Airflow The Hands-On Guide p73 2. How the logging system works in Airflow.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 你将会学习到Airflow的日志系统是如何工作的
让我们从基础开始
Airflow的日志系统基于Python标准库的日志模块
它提供了在配置方面很大的灵活性
仅作为一点提醒
日志描述的是按时间顺序聚合的流
从输出收集的有序事件
以及流处理和后端服务的输出
正如你所知 Airflow依赖于许多组件
例如Web服务器
调度器
以及太多的工作者
每个组件都会生成自己的日志流
默认情况下，这些日志会被存储到文件中
使用日志模块
这个模块允许创建一个日志对象
该对象负责根据定义的日志级别获取我们想要的日志
例如info
调试 错误
或运行
然后根据文件tg中的配置格式化日志
如我们稍后将看到的那样
最后 日志被发送到指定的目的地
这取决于使用的处理器
处理器本质上是一个决定如何处理日志的对象
有许多可用的处理器
但最重要的处理器是文件处理器
流处理器和新处理器
文件处理器将输出写入磁盘文件
流处理器将输出写入流
例如标准输出
而新处理器什么也不做
它只适用于测试和开发
实际上你永远不应该使用它
除非你是Airflow的贡献者
默认情况下使用文件处理器
并将日志存储在Airflow文件夹中的指定目的地
基于score_log
可以更改处理器以自定义Airflow的日志系统
为了更好地理解日志系统是如何创建的
这里是直接从代码中显示的不同步骤
首先 从日志模块获取日志对象
然后实例化处理器和格式化器
注意参数
simple_underscore 登录格式
在airflow的tfg文件中可以找到这个参数
它允许你指定你日志的输出格式
如果你想要在时间
日志级别等
格式被设置到处理程序，处理程序被设置到日志对象
最后，日志级别参数应用到日志
为了根据设置的级别过滤日志
调试等
现在我们已经看到了如何设置日志系统
这里是一个概述，说明它如何工作
因此，日志对象被创建和初始化
正如我们之前所见
当Airflow的组件想要输出一个日志时
它会调用日志对象，使用与该日志级别相对应的方法
例如
Dependencies或met是一个信息性日志
因此，调用info方法
如果你的ftg文件中的日志级别参数设置为info级别
然后你就能看到那个日志
然后在日志存储到默认位置之前
斜杠 用户斜杠
本地斜杠 由文件处理器处理的Airflow日志
由formatto处理
应用正确的格式
好的 Airflow中有许多处理器
多亏了他们 我们可以在不同的位置写入日志
例如标准 我会把aws放在这里
sf 三 elasticsearch
gcs等等
一些这些处理器需要连接
例如三和gcs
由于有外部存储
参数remote_underscore
日志_ C_ id必须与系统连接设置
和参数remote_on_ score
日志必须设置为true
好了现在 你知道如何日志系统工作
是时候转到实践部分 再见下一视频
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/74_Udemy - Apache Airflow The Hands-On Guide p74 3. Practice Setting up custom logging.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在前一个视频中 我们了解了Airflow的日志系统是如何工作的
在这个视频中 我将向你展示Airflow的日志系统在实际中的应用
以及如何自定义它
让我们开始吧 首先确认您在文件夹中
Airflow材料
Airflow部分八
并使用命令docker compose f启动运行Airflow的Docker容器
分解了序列执行者tml abdash
D
好的 使用命令docker检查容器是否正在运行
Ps
完美了
我们将查看调度器生成的日志
使用命令docker logs
复制并粘贴调度器的连续性
这里您将获得调度器的日志
没有新内容 你应该已经熟悉它们
让我们转到co打开文件airflow
配置文件在文件夹中
空气材料和空气部分
八个月空气
好的 从那里 让我先指出一些重要参数
基于日志的参数
在分数文件夹中指定日志文件的存储位置
然后如果你向下滚动
我们有日志级别
默认设置为info
如果我们将此值更改为warning
并从终端保存文件
执行脚本 现在我们启动dot sh
现在流被重启
Taps
然后使用容器ID查看sker的日志
如你所见
我们得到了更少的日志
因为我们只保留警告消息
好的 回到您的代码编辑器
像之前一样将learning改为info
接下来是fabbonus分数的日志
下划线level决定了flask a builder的日志级别
UI 由于Flow的Web服务器基于Flask构建，因此您可以根据此级别获取额外的日志。
这是一个构建框架。
您可以根据此级别获取额外的日志。
但是，请保留默认值。
在ScoreConfig类中，有一个名为logging的参数。
这个参数允许您定义描述日志配置的类。
别担心，我会很快回来的。 最后，我们有与日志格式和日志文件名格式相关的参数。
这三行代码只有在终端是TTY时才会应用彩色效果。
这些三行代码只有在终端是TTY时才会应用彩色效果。
分数格式中的参数log
以下列出的字符串定义了你的日志格式
基本上，这里给出的格式
包括日期时间
文件名 日志级别等等
给你看终端中的日志时看到的日志行
如果你滚动向上
在某个点你会看到与格式对应的以下行
好的 回到代码编辑器
让我们更改格式
这里你可以看到的关键词实际上是日志
记录由python日志模块提供的属性
如果你想要详细的列表
请查看下面的链接
好的 假设我们希望知道日志是从哪个进程ID产生的
以及用于锁定调用的名称
要做的就是在aac时间之后
输入以下字符串
保存文件并返回终端
执行脚本 重启 sh 以应用修改
好吧，现在我们又赢了
输入 docker Ps
并查看我们的日志，查看调度器的连续性
如您所见
我们有进程ID以及日志器和流设置名称
顺便说一句 注意，所有日志都不会受到我们刚刚定义的格式的影响
如上日志所示
这些日志是系统日志
因此依赖于另一个日志对象
好的 我们已经看到了如何自定义Airflow日志系统的基本知识
现在是时候转向高级部分了
当你想真正自定义Airflow的日志系统时
你必须创建一个Python文件来描述你想要的配置
这个Python文件是一个字典格式的文件
内容可以从airflow的仓库中找到
打开你的网页浏览器
并输入以下链接
这个文件看起来相当复杂
但是别担心
我会一步一步解释你需要知道的一切
一步一步 所以f on the score local underscore settings
Py包含根据日志对象使用的格式化器类型
打印日志事件
你想要的日志级别等信息
例如 定义了任务日志器 airflow task
负责生成与任务相关的日志的日志器是 handler task 定义的
日志级别 info 在这里
指的是 airflow 中的日志级别参数
如果我们检查上面的 handler task
我们可以看到使用成熟 airflow 的文件处理程序
日志将存储在 base underscore 定义的位置
使用模板名称细文件夹
在 Airflow 的 TFG 配置文件中定义的名为 underscore_template
让我给您快速展示那些值
在这里一切都正确
在继续前进之前最后一件事
如果我们看一下成熟的气流
我们可以看到登录日志
我们之前设置的范围格式参数被使用了
除了最佳实践之外
你应该使用这个文件而不是使用 airflow cfg 文件
为了自定义Airflow的日志系统
好的 现在是时候创建您自己的定制配置了
首先 复制这个文件的内容
从那里去你的开发工具
创建一个名为 log 的新文件
下划线 配置 点
在文件夹下运行airflow
关闭airflow
像这样
然后粘贴你复制的代码
保存文件，好的
这里有几件事需要解释
当你想要自定义airflow的日志系统时
你必须在默认位置创建一个名为conf的文件夹
斜杠 用户 切分 airflow
在这个文件夹中
你需要放置两个文件：log_on_a_score_config.py 和 log_on_a_score_config.py
这两个文件对应于日志系统的自定义配置
以及文件 _score.py 在 _score.py 中
以便 log_config.py 可以运行
Py 在便携式
请注意文件夹
Conf 这里被绑定到运行 airflow 的 Docker 容器
如果你打开Docker文件
Docker已经创建了一个cell executor或tml
运行Airflow的服务从主机获取/mnt/airflow/golf目录
与/user/local/airflow/golf在容器中绑定
如此处所示
注意这里设置的Python路径，告诉Python在哪里找到logon配置文件
现在所有事情都设置好了
打开文件airflow.cfg并查找参数logging_config_class
这里你必须指定描述日志配置的类
在我们这个案例中 它是 log_ config
默认的日志配置
保存文件
注意，这个类是大字典，你可以在 logon 文件中找到它
我们创建的 log_config.py 文件
所以回到文件
查找默认的 on the score
日志配置 on the score config
在这里是 formatres
操作员和观察者定义正确
检查Airflow是否考虑了自定义配置文件
转到终端并执行脚本
我们开始点sh
没有类型的ps
并查看我们的日志调度器容器
如果你看到以下行告诉你用户登录配置已成功导入
那么干得好 这意味着一切正常
否则我们观看视频以检查
如果你没有犯错，好吧
在转到你的网络浏览器的下一个视频之前，我想再给你展示一件事
在新标签页中打开本地主机：80 80
然后在 score dug 中打开 dag 日志开关
这样任务就可以开始执行
现在，在你的代码编辑器中
从左侧面板打开四个日志
日志中有三个文件夹
包含 airflow 处理你的 dag 的相关日志
包含 score 处理器的相关日志
给定一个暗定义文件列表
数据处理器经理负责传递和分析您的DAGs
查看应运行的任务
在数据库中创建适当的任务实例
我们正在调用任何奖项
杀死任何属于DAG的任务实例，这些DAG很长时间没有发送心跳
等等
如果您打开文件agonprocess on dot log
您可以看到DAGs在由参数定义的时间间隔内处理
在Airflow配置文件中，mean underscore file underscore process on the score interval
好的 接下来 文件夹调度器包含与您任务调度相关的日志
如果我们查找任务t1
这是score dag上的日志任务
您可以看到任务何时被安排和触发
最后，当您安排dag时
会在日志文件夹中创建一个与dag同名的文件夹
然后为每个任务生成一个相应的日志文件夹
例如 如果我们点击这里的日志标签，点击分数标签，选择最近期的执行日期
然后打开一个点log文件
我们获得了任务t one执行产生的输出
注意这里的1对应了任务尝试的次数
所以这里t one执行了一次
如果任务被重试了两次
那么我们将看到两个点log而不是一个点log
好的 现在 回到文件
登录得分 配置dot py并滚动到默认得分
按下下划线
登录 下划线配置
这个字典允许您自定义黑暗进程或经理的日志
来自文件进程得分经理日志
我们可以看到使用leinfo的日志
让我们修改这个日志配置py
我们将级别放在这里从得分级别到字符串critical
保存文件到你的命令行中
停止容器通过执行stop dot sh
好的 现在到你的代码编辑器中
删除文件diagonal processor
删除文件_ manager dot log
像这样，再次启动docker容器使用start dot sh
好的，回到呼叫
文件agonscore processor on the score manager dot log已经被重新生成
但如果你打开它，这次你将什么也得不到
为什么，因为多亏了我们做的修改
我们只保留关键的日志消息，我们不会有任何
还没有 这是您如何自定义Airflow每个日志器的一个例子
并设置它们的日志级别
这是从文件Airflow中不可能的
Tfg
好的 所以让我们恢复我们做的修改
保存文件
并通过执行我们启动容器
我们启动h
好的 这就是这个视频的全部内容
这里提供了大量的信息
在下一个视频中 我们将了解如何使用AWS来读取和写入Airflow日志 快速休息一下，下一期视频见
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/75_Udemy - Apache Airflow The Hands-On Guide p75 4. Practice Storing your logs in AWS S3.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


我们已经学会了如何设置Airflow的自定义日志配置
现在是时候看看如何在AWS S3中存储和读取日志
在继续之前
让我再提醒你一下什么是AWS S3桶
S3桶是一个公共云存储资源
桶用于存储对象
对象包含数据和描述数据的元数据
你可以将其视为云中的硬盘，既然如此
让我们继续前进
我假设你已经有一个AWS账户
所以 如果没有 请花时间创建一个，然后完成后再回来
好的 默认情况下，Airflow将其日志文件存储在本地，无需配合
如果你运行大量作业，甚至少量频繁作业
磁盘空间会很快被消耗 在S3上存储日志
不仅免除了磁盘维护考虑
这些文件的耐久性将得到更好的保证，与大多数
这些制造商和文件系统提供的相比
前往AWS控制台
一旦你连接到账户
第一步是创建S3桶
Airflow将存储日志
点击服务，选择S3，然后选择第一个选项
点击创建桶
从那里
我们需要配置一个桶
这里应该唯一 在我这种情况下
我将输入mark l dash f low dash logs
请注意，你需要输入自己的名称
不要使用我的名称
它将不会工作 选择最接近你的桶的区域，对我来说
它是巴黎
点击下一步
我们不需要更改任何东西
因为我们不想允许公共访问桶
我们保持这里的复选框选中
好的 一切都好
我们可以点击创建桶
新桶已经创建
完美 如果你点击它
你可以看到目前它是空的
在创建了用于存储日志的桶之后
我们将创建一个新用户，仅具有对该桶的读写权限
确实，这样做是一个好做法，可以避免潜在的安全问题
点击服务类型iam
选择服务
然后点击用户
我们需要定义一个用户名，创建一个ad用户
让我们说 Airflow log a three
我们选择程序访问
因为我们需要访问密钥ID和秘密访问密钥
以便从Airflow创建到aws three的连接
点击下一步
选择直接附加现有策略
然后点击创建策略
我们将创建一个策略
仅给予我们之前创建的存储桶的读取和写入权限，选择
选择服务并选择a three
然后在操作下访问级别和列表
选择列表存储桶
在读取中 选择获取对象
在写入中选择上传对象
删除对象
复制对象和恢复对象
现在操作已设置在资源中
点击警告
从那里我们需要指定用户可以执行的资源
首先 我们限制访问权限，仅允许访问我们创建的存储桶，点击
添加a或n
在此输入您为存储桶定义的名称
在的情况下 在我这种情况下
Mol dash
Airflow Logs
检查您没有犯错误并点击添加
好的 我们做同样的操作，添加对象
输入相同的存储桶名称
选择所有Airflow日志
在我这种情况下
并选择任何对象名称
点击 添加完美
我们可以查看策略
我们需要为策略命名
所以让我们说
读取，写入a three Airflow Logs
最后点击创建策略
好的 回到用户视图
点击这里查看政策并选择重写s三个airflow日志
点击下一步
再次下一步并创建用户完美
用户在这里已被良好创建
我们获得了访问密钥
ID和秘密访问密钥我们将在airflow中使用以连接到a three
所以现在下载凭据
因为你将无法再看到这一页并保存到安全的地方
点击关闭所以我们完成了aws
让我们在你的终端中进入airflow
检查你是否知道该文件夹
空气流动材料
气流部分八
并且使用 start dot sh 启动 Docker 容器
正如你所见，Docker正在构建一个新的Docker镜像来运行Airflow
为什么，因为为了将您的日志存储到aws s3
你需要将包添加到三个
为了更具体，需要安装Airflow
去你的代码编辑器
检查你的文件夹 airflow materials airflow section eight
并打开 docker 文件夹中的 docker 文件
你有设置 docker 容器运行 airflow 的不同命令
如果你看一下 airflow 使用 pip 安装的行
你可以在这里看到
包 s three
以及其他一些常见的包
例如 scripture
Celery Postgres 等等
所以如果你想访问airflow的三
这个包必须在终端中安装
我现在会放视频
我会在构建完成后回来
好的 构建已完成 airflow应该正在运行
我们输入docker ps
容器正在如预期运行
现在完美运行在你的网页浏览器中
打开一个新标签页并输入本地主机：80
从Airflow UI点击管理员连接并创建，在这里我们将设置连接
我们需要从Airflow访问S3存储桶
让我们输入af3_log_storage作为名称
然后选择S3作为连接类型
最后在额外字段中
输入以下JSON值
注意，每次访问密钥时
你应该从你之前下载的CSV文件中输入你的凭据
像这样，点击创建完美
连接已成功创建，现在从你的命令行编辑器
打开文件airflow.cfg
以便使用外部存储来记录你的服务器日志
必须定义三个参数remote_logging
我们remote_on_score_log_on_score_id
以及我们remote_base_on_score_log_on_score_folder
让我们从第一个开始
remote_logging设置为true
允许Airflow从远程位置写入和读取日志
所以将其值从false更改为true
然后我们remote_log_on_on
Id对应于我们刚刚创建的用于连接到AWS S3的连接
所以这里我们必须输入连接IDaw s three log storage
最后我们remote_base_log
On_score_folder
指示从远程存储读取和写入日志的目录
所以输入s3:/
你的桶名，这里是mark l - airflow logs
以我的情况
以及我们希望保存日志的文件夹
这是airflow logs
最后你应该有和我一样的行
除了要输入你自己的桶名
好的 保存文件
如果你回到AWS
S3仪表板
你应该在这里找到你的桶
如果你点击它
它应该仍然是空的
好的 是时候看看一切是否如预期般工作
从你的命令行
执行restart.sh
以便我们修改的应用
好的输入ps
太好了现在输入docker logs
Dash f与容器
Id的工人，任务将执行在狗
此命令的目的是显示你的空气将通过你的a s
S3桶进行连接，以读取和写入日志
在你的网络浏览器
转到for you
I并点击
从那里切换Dag日志
以安排它，并等待Dag完成
这个流程图只有两个任务
一个是什么也没做的，另一个是打印出一条消息到标准输出
好的 流程图完成
点击流程图
然后抓取视图
点击t one并查看日志
从那里注意这里的第一行
表明您正在阅读的当前日志是从以下文件远程获取的
确实存储在您的s三桶中
如果您回到s三仪表板
并点击此图标刷新桶
您可以看到一个名为flow-dash-logs的新文件夹
如果您点击它
您将获得一个名为logan-to-score-tag的文件夹
然后进入它
您将获得一个每个任务的文件夹
点击t one
最后一日期点log打开
它与afflui中显示的日志相对应
您现在能够将airflow产生的日志存储并读取到aws s三中
在继续之前
回到您的终端并执行script.sh脚本
最后，在您的代码编辑器中
我们将变量remote_on的值设置为true
log_on的值设置为true
remote_on的值设置为true
并将remote_on_log的值设置为logan-score文件夹
然后将remote_logging的值设置为false 保存文件，我们完成了完美，让我们快速休息一下，并在下一视频中见到您
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/76_Udemy - Apache Airflow The Hands-On Guide p76 5. Elasticsearch Reminder.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 我将快速提醒您什么是Elasticsearch
以及它是如何工作的
那么什么是Elasticsearch
Elasticsearch是一个开源搜索引擎
提供了一个分布式全文搜索引擎，具有HTTP Web界面和文档模式
它允许您存储
搜索和分析大量数据
它非常适合在规模上收集和聚合日志数据
以查找趋势
统计 汇总
以及更多实时数据
它是如何工作的
在elasticsearch中需要了解两个概念
第一个是文档
文档可以被视为关系数据库中的一行
它与在elasticsearch中存储的json格式的数据相对应
所以基本上，文档可以是来自日志文件的以下json数据，包含不同的字段
例如id,名称和日志级别
除了相关的值
因为你的日志文件包含迷你日志事件
每个日志事件都是一个文档
接下来是一个唯一的ID
这些文档存储在一个索引中
索引是一个文档的集合
你可以从关系型数据库的角度将索引视为一个数据库
出于可扩展性的原因
最佳实践是为每一天创建一个新的索引
当你处理Elasticsearch中的日志事件时
通过这样做 您可以根据给定日期或日期范围请求您的数据
避免请求所有数据
这将非常慢且非优化
这就是为什么我在这里在索引名称中设置了一个日期
然后，索引可以有一个或多个映射类型，这些类型用于将文档分为逻辑组
在同一个索引中
映射定义了如何索引文档以及其字段的如何
这些字段的索引和存储
您可以将映射视为关系数据库中的表
基本上每个文档都有一个定义的类型
正如你从字段_type中看到的
最后 一旦你的文档被映射并存储到索引中
你可以通过elasticsearch提供的rest api来请求它们
好的 你必须知道的一件事是当我们谈论elasticsearch时
我们通常指的是elk标签，即elasticsearch，logstash和kibana
确实，elasticsearch是一个以文档为导向的数据库，组织你的数据
但你如何实际将数据发送到elasticsearch
一旦它们被存储
如何很好地可视化它们
这就是lostash和cubana发挥作用的地方
让我们从kibana开始
cubana是一个开源的数据可视化工具，用于日志分析
应用程序监控
以及更多 它为您提供了一种强大且简单的方法，以便在elasticsearch中存储的数据上创建仪表板
存储在elasticsearch中
别担心 我们会看看如何使用
请关注下一期视频
鹿堆的最后一个组件是日志堆栈
日志堆栈是一个服务器端数据处理管道，可以从不同的来源摄入数据
类似于新的和转换它，然后发送到你想要的输出
例如，Elasticsearch
STD Kafka等等
换句话说
它允许您摄取不同形状和来源的数据
然后按顺序处理每个事件以构建一个共同的格式
最后将处理后的数据发送到一个或多个系统
如果您想对日志事件应用聚合
过滤或转换
在转发之前
那么lostash可能对您有用，好的，让我们继续实践
在elk堆栈中实际上还有一个我想谈论的工具
那就是filebit
filebit是一个轻量级的
作为代理工作并在您的服务器上作为代理安装时，日志数据的集中更便宜
Filebit监控您指定的日志文件或位置
收集日志事件
并将其发送到Elasticsearch或Logstash进行索引
就像Logstash一样 您需要定义一个输入和一个输出
但你可以对你的数据应用的转换非常有限
尽管如此 Filebit有一个非常小的CPU和内存占用
这就是为什么它通常安装在同一节点上的原因
木材在哪里
为了迫使它们锁定储藏
它在另一个节点上运行
因为它消耗了大量资源
好的 这就是这个视频的全部内容
现在你对每个工具的功能有了更好的了解
让我们转到实践部分，我们将设置一切
以便使用elasticsearch监控airflow 下一期视频见
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/77_Udemy - Apache Airflow The Hands-On Guide p77 6. Practice Configuring Airflow with Elasticsearch.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在快速回顾了elasticsearch之后
Kibana（中文翻译：基巴纳） 在这个视频中，日志存储和文件位
我们将配置airflow以便与elasticsearch一起工作
在这里设置用于写入和读取日志的结构
Elasticsearch中的f事件
基本上，我们将有三个docker容器，对应于lostelasticsearch和kibana
除了另一个容器之外，在这个容器内运行的 airflow 工人
我们将安装文件位来获取日志并将它们发送到丢失的地方
一个重要的观点要记住
那是否气流不会直接将日志事件写入elasticsearch
我再说一遍
与aws s三不同
与airflow将日志写入三到elasticsearch不同
它只能读取它们
因此，为了在最终导入时使导入生效，您必须设置文件比特
当deck被触发时
给定任务的日志事件将存储为json文件到本地日志文件中
这些日志文件将位于由参数指定的路径上
基于score_log在score文件夹中
这是斜杠 用户斜杠
默认情况下，本地斜杠空气日志
然后每次生成一个新的日志文件
文件位将处理它
向每个日志事件添加偏移量并将输出发送到丢失
丢失将获取日志
我们将应用一些转换以生成所需的空气日志字段
最后将它们发送到elasticsearch
一旦数据存储在elasticsearch中
我们将能够通过关键标志监视我们的狗，创建美丽的仪表板
在实践之前
有两个重要的点你必须注意
为了从elasticsearch读取日志
Airflow假设两件事首先
您的日志事件应该有一个名为offset的字段
这将用于以正确的顺序显示日志
这个offset由filebeat在处理给定文件时自动创建
第二件事是Airflow假设字段log_id
对应于dag_id_task_id的连接
执行日期和列车号如此处定义每个日志事件
日志下划线id字段将被airflow使用
以便从elasticsearch检索日志
该字段不是由lostash自动创建的
因此，你必须定义一些转换，这些我已经为你做好了
为了生成它，最后一个点
为了与airflow一起使用elasticsearch
你必须安装elasticsearch包，就像我们在a three所做的那样
好了，不再等待
让我们转到你的文本编辑器中的实践
检查你是否在文件夹中
Airflow材料
Airflow部分八
并打开文件
Docker composer
Serexecutor Elk tml
所以关于Airflow没有什么新消息
但如果你滚动下来
你可以看到服务elasticsearch和cubana
主机上的环境变量server必须设置为o o o o
否则您将无法从您的网页浏览器访问Cubana的用户界面。
在我们下方，我们有一个服务丢失，以下为容量
本卷对应包含日志堆栈管道的文件夹
如果你打开文件 airflow，日志将被运送和处理到哪里。
日志点会议在四个斜杠日志星斜线管道
这是你为我制作的管道
日志将通过 airflow 进行处理
这条管道分为三个部分
我们在端口上指定日志事件来自文件位的输入
五四四
然后过滤器，我们在其中定义多个转换，以便让json数据通过
生成logan id字段，如图所示
并将字段偏移量移动到json文档的根部
而不是默认情况下在字段log下
最后 输出部分的最后一部分是
处理后的日志事件被重定向到elasticsearch
请注意这里的索引格式
这样每个索引名都将以airflow - logs开始
当前的日期
将日志按天分批导入elasticsearch被认为是最佳实践
为了优化请求，让我们继续
配置airflow与elasticsearch工作
打开文件 airflow.py
首先 因为我们将要使用远程存储来存储日志
我们需要设置远程_日志参数为true
现在，与其他远程存储（如s3或jcp）不同
我们不需要创建连接
既不在基于分数日志的分数文件夹上设置为远程
确实查找 Elasticsearch 部分
在这里我们有需要定义的不同参数
让我们从主机开始
这是 http 冒号
斜杠 Elasticsearch 冒号 192.0
点
因为我们使用 Docker
注意这里elasticsearch对应运行的服务名elasticsearch
这就是elasticsearch
实际上需要定义的下一个参数是write_std_out
这个参数用于将任务日志写入walker的标准输出，而不是默认的文件
因为我们实际上想将日志存储在默认文件中
因此我们更改了值并让参数m
这确实很重要
我不知道这是不是一个错误
但如果你保持参数设置为false
并且启用了elasticsearch
那么到目前为止日志文件将不会被产生
保持参数为空
接下来设置参数jon
将score_format设置为true
因为elasticsearch期望json数据
好的
参数g_fields这里显示了将添加的字段 除了您的日志事件
我们将有日志的时间戳
文件名
以及它被打印出的位置 你可以在以下链接中找到所有可能的字段
最后
但并非最不重要的是参数log_score_id
在skeleton plate这里对应于日志id值的格式
用于从elasticsearch中检索给定任务的日志
从airflow ui
默认情况下
它由dag_id, task_id, execution_date和number_of_tries组成
所有由破折号分隔
请记住，在这里给出的模板应该与logstash管道中定义的相同
好的
所以我们已经完成了airflow的配置
我们可以从你的终端开始启动架构 通过输入docker compose up
Docker正在启动所有服务
好的
我们可以通过访问localhost:9000来查看elasticsearch
通过访问localhost:5601来查看kibana
通过访问localhost:8000来查看airflow 好的 我们可以通过访问localhost:5601来查看kibana
好的 我们可以通过访问localhost:5601来查看kibana
如你所见 我们有三个新的docker容器
它们是elasticsearch, kibana和logstash
现在让我们检查一切是否正常工作
通过在你的网络浏览器中输入localhost:9200
通过在你的网络浏览器中输入localhost:5601
O
完美 我们获得了一些关于elasticsearch的信息
所以elasticsearch正在运行，打开一个新标签页，我在host 56中查找
哦，1
我们得到了预期的kibana，在新标签页中再次输入local host：80
airflow ui正在运行，完美
所以每件事都工作正常 让我们继续前进，回到您的终端
通过输入docker exec连接到airflow worker
容器ID的工人
斜杠豆
斜杠bash 进入
在这个点上
F已经配置为从elasticsearch读取任务日志 最后一步是设置文件位，以便将日志发送到_lodash
以便最终将它们发送到elasticsearch
以便我们能够从flow中读取它们
首先我们需要下载filebit，通过命令curl -O
和以下链接
输入
让我们等待下载完成
好的
解压文件star xv f 和文件的名称
像这样
然后进入新文件夹并输入
好的，在这里
我们有filebit二进制文件和filebit配置文件file.dot tml 用vm打开file b.dot tml
首先我们需要配置file应该查找的地方
以找到日志文件
以及它们应该如何被处理
为此
您需要指定输入file提供了许多不同的输入
例如日志文件
kafka s three reis等
您可以在以下链接中找到详细的列表
在我们的情况下 我们保留输入log，当我们需要读取日志文件时使用
然后下面我们启用输入，设置参数为true
接下来，我们需要设置日志文件的路径
如果您记得，当任务被执行时
会自动创建多个文件夹，分别对应于ID
任务ID
下一个，我们有一个关于如何处理日志文件的设置
执行日期
最后，日志文件
以及由该任务的重试次数定义的文件名
在这里我们替换这个路径
为斜杠 用户斜杠
本地斜杠 Airflow日志
这是日志文件的默认路径
我们输入斜杠通配符
斜杠 通配符点
这里每个通配符对应一个dag id
任务id
执行日期和文件分别
检查路径是否正确
否则文件位将给你带来很多错误
告诉你json格式不正确
我不会告诉你任何事情
如果路径指向没有日志文件的地方
好的 现在最后 但不最不重要
我们必须设置输出
查找 lostash 并输入斜杠
丢失
在这里输入
并输出路径. lostash
除了定义黑洞的线条
将localhost替换为lostash，这是Docker-compose文件中lostash的服务名称
文件
像这样 然后在注释上方注释掉行
主机 并输出elasticsearch点从elasticsearch输出
完美保存文件
现在设置好了
我们可以开始文件位
输入冒号q退出文件，并输入点斜线文件bit
E dc
文件b点 Ml
这里有双引号
发布
双切
这将根据配置文件file.tml启动file bit
并启用调试选择器发布以过滤file bit的日志
好的 一个比特正在运行
让我们转到UI
打开dag_log下的切换按钮
挖掘
然后等待图表完成
完美现在
去kibana
然后点击探索我自己
然后从左侧面板点击这里最后一个图标索引管理
如你所见 我们有一个名为f-dash-logs的索引
当前日期对应于日志处理的时间
点击索引模式并创建索引模式
从那里点击与下面的Elasticsearch索引匹配的索引模式
我们输入airflow-dash-logs
通配符
好的 模式与索引匹配
下一步在这里我们选择时间戳
这是用于按时间过滤数据的字段
注意到这个领域是自动由elasticsearch生成的
当一个日志事件被存储时
点击创建索引按钮
完美，我们在这里获得了日志文档的映射
这意味着我们可以从cubana请求这些字段
以便于制作过滤器
聚合 统计
并且可以在左侧面板中做更多的数据操作
点击这个封面，就是在时钟下面的那个
这就是它们
您的任务日志事件
来自Airflow的日志事件存储在Elasticsearch中并可查询
通过点击这里展开第一个日志
如果您向下滚动
您可以看到任务的黑暗想法
一些关于主机的信息
文件批处理在哪里运行
日志级别
日志ID
偏移量
任务ID
等等
请注意，您还可以在字段中查看我们从lostash到日志的转换
在这里我们有日志偏移量
这是文件比特默认创建的
并通过我们的转换我们复制了相同的值
但名称是偏移量
这是需要的，因为airflow在名为偏移量的字段中请求，而不是日志偏移量
最后，如果您回到UI
然后点击logger在分数标签灰色dani u和vlog上
如你所见 Airflow能够从elasticsearch中检索日志，完美无缺
所以现在你知道如何设置一切以便使用elasticsearch与airflow
现在是时候构建一个仪表板了
为了使用kibana和elasticsearch监控你的狗 所以保持一切运行，并在下一视频中见到你
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/80_Udemy - Apache Airflow The Hands-On Guide p80 9. Practice Monitoring Airflow with TIG stack.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们设置堆栈以便监控气流
首先确认您在文件夹中
Airflow材料和气流部分8
并打开文件docker
Compose sertor t i
G
Dot tml如常
我们有不同的服务以运行气流与单元格执行者
如果您滚动向下
您将到达tig堆栈的服务
那里仍然有一个图表有一个卷绑定配置文件telegraph
Com从montelegraph到服务
我们将在几分钟内看一下
就在Influx db服务初始化时设置默认端口
Grafana也是如此在端口
三千
配置telegraph
打开文件telegraph dot
Com在montelegraph并查找section outputs dot influxdb
如您所见，所有下面的行
这对应于输出插件
它们非常棒 您唯一需要做的就是取消注释您想要启用的内容
默认情况下 输出index db已启用
但我们必须首先进行配置
取消注释第三行urls
并替换ip地址为influx db
记住这里的值对应于服务名和influx db的端口
在docker compose文件中
取消注释与数据库相关的行
以便指定目标数据库，其中矩阵将被发送
默认情况下为telegraph并取消注释参数
保持score数据库在score创建下，并将其设置为true
因为我们将自己创建数据库
从influx db容器
然后取消注释参数time i以定义时间
我将为http消息
就在下面 我们取消注释用户名和密码
以便定义将由telegraph使用的用户帐户
以将指标发送到influx db
让我们保留用户名
Telegraph并更改密码为telegraph
Pass
像这样和用户也是如此
就是这样
输出插件influxdb已配置
并且telegraph已准备好将其发送到它
所以输出已设置
但是输入的地方，电报会接收
来自airflow的矩阵仍然缺失
让我们为这部分的输入点修复这个外观
让我们取消注释该部分的所有行
就像那样 我不会解释每个参数
既然上面每个都有描述
但是基本上，电报会开始在该端口上监听一个状态
八十一二五在udp从metrics来自airflow
保存文件
让我们配置airflow，使它能将矩阵发送到电报
打开文件 F点tg并查找该部分
这里有四个参数需要定义
首先 将std_on的值从false更改为true以激活它
以便激活它
然后这里输入localhost by telegraph
这是运行电报的容器名称
最后，保持最后两个参数的默认值，完美，现在已配置
将矩阵发送到输入插孔的电报策略最后一步
在日志记录中查找远程参数
如果尚未完成，则将其设置为false
因为我们不打算将日志文件存储在远程存储中
没有必要保持其活跃
保存文件，现在是开始一切的时间
打开您的终端并检查文件夹
Airflow材料
Airflow部分8并输入docker compose f
Docker分解的surveytor t i
G Dot tml abdusty
等一两秒钟
按照顺序输入 rps 以检查一切是否正常运行
完美，现在我们需要创建数据库 telegraph 以及用户 telegraph 在 db 中
这将由 telegraph 用于发送矩阵类型的 docker 确切 g
I
复制并粘贴 influx db 的容器 id
并在 flux 中输入
现在我们通过 influx db shell 连接到 fix db
如这里所示
创建数据库的类型
电报
输入确认
然后通过执行创建用户
创建用户电报
密码
两个简单的代码
电报密码
用户电报已创建
按Ctrl + D退出Docker容器
然后打开你的网页浏览器，输入localhost:3000
回车
输入admin作为登录和密码
然后登录
我们跳过这一步，最后到达美丽的仪表板图形
从那里，我们需要遵循两个步骤，操作在这里
添加一个数据源，对应于InfluxDB实例
设置Airflow数据存储的位置
点击添加数据源
然后选择InfluxDB
好的 URL是http
:// influxdb
:8086
在下一节InfluxDB详细信息中
数据库是telegraph
用户是telegraph
密码是telegraph pass
我建议你添加路径这里，然后复制并粘贴值
这样你就可以确定密码是正确的
好的 检查你是否有和我相同的设置
然后点击保存并测试
如果一切正常
你应该在这里得到相同的消息
告诉你数据源正在运行
点击这里返回主页面
添加新的仪表板和查询
在这里 你需要定义将从InfluxDB检索的查询
矩阵
例如 如果我们点击选择测量
所有前缀为airflow的测量
underscore对应于Airflow可用的不同指标
打开一个新标签并转到https://
airflow.apache.org
如果你滚动浏览
你将得到矩阵列表及其描述
按类型分组，如图所示
假设我们想了解Airflow下的标签加载数量，类型为gauges
matrix_dag_runs_oscarsize给我们提供了这些信息
因为matrix_dag_runs对应于调度器加载的狗
实际上我们需要激活不同的标签
打开一个新标签页并输入localhost:8080
猎人
在这里打开两个狗的切换开关
完美，回到grafana
选择分数bag_underscore_size像这样
点击这里我们移动
点击加号
选择器
这样我们就可以获得调度器加载的最后标签数
删除分组，因为最后数字是从整个表格获取的
然后在左边点击可视化
选择显示下的仪表盘
点击
选择最后
我们得到一个美丽的仪表盘，数字2对应于dags
logo_underscore_dag和数据在分数dag imported by airflow这里
如果你想可以自定义可视化，通过设置显示
设置阈值等等
接下来点击通用并设置标题标签数量
这样我们知道图表的含义
好的 最后页面顶部
点击保存仪表盘
给名字airflow
点击保存
你刚刚创建了你的第一个监控你afro实例的仪表盘，使用grafana在db和telegraph，干得好
保持一切运行，下一集见
好的 保持一切运行，下一集见 我会告诉你如果出现任何问题如何报警
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/83_Udemy - Apache Airflow The Hands-On Guide p83 1. Introduction.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


管理你的应用程序安全非常重要
特别是如果你在生产中使用 f
本节的想法是从非常不安全的开始
例如 并一步一步向你展示
如何使你的应用程序更安全
通过调整一些参数
我们将看看您的密码是如何加密的
什么是有限密钥
以及如何更改它
然后 我们将快速看一下如何从UI中隐藏您敏感的变量
我们将了解如何在UI上激活密码验证
以及根据所有者参数对DAG进行过滤以创建用户帐户
最后，在Airflow的最新版本中
可以启用基于世界的访问控制，以更好地定义用户的权限
例如，限制对特定视图的访问
功能和性能
等等
所以准备好
这一节将会非常激动人心 下一期视频见
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/88_Udemy - Apache Airflow The Hands-On Guide p88 6. Practice RBAC UI.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


到目前为止，我们已经看到了如何创建一个用户帐户并根据用户过滤DAGs
并且DAG中定义的所有者参数很棒
但是，在某个时候 你可能需要一个系统
让你能够更具体地控制哪些类别的用户可以做什么和不能做什么
例如 你可能有两支不同团队的数据工程师
例如营销团队和金融团队
每个团队只能访问与他们技能领域相对应的特定DAG集合
好吧 多亏了基于新世界的访问控制功能Airflow
你现在可以创建一组用户及其权限来应用这类规则
每个权限集都有一个特定的墙
假设数据工程师无法访问查看变量
嗯 Airflow提供了五个世界，分别是管理员，用户，查看者，公共
和公众
每个世界都有自己的一组权限
如果你查看这里的文档
你可以看到每个世界的描述以及权限
例如 管理员拥有所有可能的权限
包括为其他世界授予或撤销权限
观众可以阅读狗的信息
但不能修改空气流动数据库的状态等
这里展示的是不同的权限设置给世界的观众
在这个有限的网页视图中
好的，不再等待
我们先从你的代码编辑器创建一些用户及其对应的世界
检查您的文件夹和材料
打开 airflow 限制 9 的文件
打开 airflow 特征
然后查找参数或 b
C
将其设置为 true 以启用 orb c
保存文件并打开您的命令行
检查您处于 airflow material airfield section 9 的文件夹下
带有 Docker 容器运行
如命令docker ps所示
接下来我们通过命令docker compose up -d 启动web服务器
启动docker compose的自定义web服务器
docker compose启动自定义的web服务器并重启
好的 web服务器再次运行
输入docker ps查看
然后查看你的日志
在web服务器的持续运行中
如你所见
有一些日志表明还没有创建用户
世界正在同步
权限正在获取
等等
所以让我们创建一个新的账户类型
Docker Exec 我执行
粘贴Web服务器的连续性
斜杠bean bass
并且
现在如果你在分数用户上输入airflow list
我们会有一个警告告诉我们还没有创建用户并且实际上没有有效的用户
如表中所示
首先我们必须创建用户admin来做到这一点
执行airflow create
下划线user
减号admin
设置用户名admin
设置密码admin
设置电子邮件admin@airflow.
com
这是名字mark
这是姓氏
Lombarty
基本上这个命令会创建一个名为admin的用户并且密码也是admin
世界是admin
这意味着用户将可以访问一切
注意参数
电子邮件 名字和姓氏
这些也是必须输入的
好的 让我们检查用户是否被成功创建
输入airflow list在分数用户上
用户
并且像你所看到的没有警告了
并且我们看到了admin账户如这里所示
完美回到您的Web
打开一个新的标签
并且i local host colon eighty eighty
并且我们看到了认证页面
你可能没有注意到
但是这个页面实际上与前一个视频中看到的页面不同
与后台密码
别忘了这两个系统是完全独立的
使用密码后台创建的用户账户在rbc激活时不有效
所以如果我们输入admin作为用户名
并且admin作为密码
然后登录
我们已经连接到UI，使用这里显示的账户
如果你点击它
然后查看个人资料，你会得到不同的信息
例如账户的墙
登录次数，个人信息等
如果你点击dags
请注意，你有访问所有dags的权限
因为你是管理员
好的 如果你仔细观察
你应该注意到一个新的部分
安全选项就在标题栏那里
点击这里，你会看到多个页面分为两个类别
第一个是关于用户和工作
而第二个是关于可用权限的列表
例如 让我们去基础的权限这里
这里有基础的权限列表
比如canongracanscore list menu
Underscore access等等
然后回到安全视图
点击斜杠菜单可以查看所有可用视图
您可以限制访问的斜杠菜单
最后，视图菜单的权限
显示与视图相关的所有权限
斜杠菜单
如果我们点击搜索
然后添加过滤器权限并输入可以编辑
搜索 这些行是权限无法应用于编辑的视图
如果用户已设置在墙上
拥有编辑权限
那么所有这些视图都可能被潜在地修改
好的 为了把事情说清楚
让我们创建一个新用户
但这次从UI点击安全
然后列出用户
如何记录点击这里
我们得到了同样的信息来填写
然后使用命令行界面
假设第一个名字是埃里克
姓氏是皮特
用户名是埃里克
是否活跃 是
邮箱我们输入埃里克@airfield.
com
选择角色你是
密码是埃里克
点击保存
并且我们创建了一个新用户
如前所示，在进行下一步之前
请注意，所有观众都被分配给了埃里克
限制他只能阅读
这样他就无法修改或从用户界面触发任何事情
让我们看看
我们在用户名和密码字段中输入eric
登录后，我们以埃里克的身份登录
您可以看到安全按钮不再可用
如果我们尝试触发分数DAG上的DAG营销
例如
我们会收到访问被拒绝
如预期 好的
让我给你看一些东西
如果您点击浏览
然后日志
如您所见
我们有一个命令，用于以明文创建管理员用户
以明文创建密码
这很隐秘 不是吗
让我们编辑世界
这样观众将无法再访问此页面
注销
再看我
转到安全列表规则
查找这里的viewer并点击此按钮以编辑世界
从那里我们可以获得角色的名称
以及所有授予的权限
查找菜单权限
访问日志
这里并删除它
好的，点击保存
看
我们连接到erika con
现在如果我们点击行
日志按钮不再可用
完美 我还有一件事想给你看
假设rig属于营销团队
因此您可能不想给他访问DAG的权限
确实，finance underscore dag
如果您点击它
然后代码，我们可以看到DAG的代码
这可能不令人满意
如果它包含敏感数据
要修复这个问题
让我们创建一个专门为营销团队设计的新世界
管理员登录
安全列表词汇
选择世界查看器
然后操作
复制战争
好的
在页面底部我们有新世界编辑，更改名称由市场部完成
我们移除权限
可以在分数标签底部读取所有分数
所以到现在我不能读取任何分数
点击权限类型
可以读取
并在分数标签中选择市场部
保存最后事情
前往安全
列表用户
编辑埃里克
并更改世界从查看器到市场部
保存并返回埃里克账户
如你所见
这个计时器只能看属于他团队的狗
这就是你可以调整狗的权限的方式
这样你可以选择哪个
用户可以看到什么等等
我强烈建议你查看权限列表
并玩弄词汇和用户
有很多可用的权限
并且绝对需要正确配置它们
如果你在公司使用airflow
好的 这是一个很大的视频
但现在你可以为正确的用户设置正确的权限
并且更加放松
我希望你喜欢你所学到的东西 享受一个美好的休息，并在下一期视频中见到你
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/90_Udemy - Apache Airflow The Hands-On Guide p90 1. Backfill your DAGs in Airflow like a PRO.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个视频中 我们将深入探讨回填（back filling）
因为你会经常使用这个机制
让我们从定义回填开始吧
回填是在一个DAG中为前几日运行标签或特定任务的过程
例如
如果一个DAG从本月初就开始运行
并且向其中添加了一个新任务
这个新任务呢
你可能希望它也为前几日执行
为了做到这一点，你需要回填它
下一个问题是
你需要在什么时间回填一个DAG或任务
你可能需要回填一个DAG
当你创建一个新的DAG并且想要为过去的时间段运行它
这是非常经典的
另一个原因是当一个DAG或DAG内的任务失败
并且你想为失败的时间段重新运行它
这是非常常见的
不幸的是，这也是
如果你修改了一个现有的DAG，并且希望将这些更改应用到过去的运行中
你需要回溯你的DAG
例如，当添加一个新的任务或更改依赖关系等
最后一个原因是当你想重新运行一个特定的任务或一组任务
在某个日期范围内
有时候，重新运行一个特定的任务或一组任务
比重新运行所有任务更好
当你回溯DAG时
为了回溯一个任务或DAG在Airflow中
你不会使用用户界面
相反，你将使用命令行界面（CLI）使用以下命令
这个命令是backfill命令
我强烈建议你查看一下Airflow的文档
如你所见
因为选项很多，你可以使用那些对你有用的
取决于你的使用情况和你想要如何回填你的任务或DAG
别担心 我们会在视频中看到更多
让我们从一个简单的例子开始
来说明如何回填一个DAG或任务在Airflow
我们可以从改名这个流程开始
因为它不是一个很好的名字
我想我们在这一点上达成一致
所以我们用零售来给它改名
以及工作流程文件
让我们回到用户界面刷新页面
我们有这个新的零售流程
我们可以删除另一个测试流程
而现在为了展示备份机制
我们需要有一些流程运行和任务实例
因此，我们需要将番茄酱参数更改为true
由于起始日期在过去，到今天
Airflow将捕获所有不触发的ddiagrams
在起始日期和数据平面设定的日期之间
让我们这样做
保存文件并返回到用户界面
然后我们可以启用此管道的切换并转到页面
您可以看到现在我们有几个daggers和任务实例
让我们再次刷新所有内容
现在让我们设定匕首
好的 让我们从第一个用例开始
我们想要运行过去间隔的零售新dag
确实 如果你看一下代码
你可以看到起始日期是2024年1月1日
如果我们想在起始日期之前运行此管道
这就是为什么你需要使用Airflow dags backfield
假设我们想从2023年12月1日运行它
如您所见
如果您点击此图表
它在2024年1月1日触发
我们希望在此日期前一个月运行
让我们去终端并这里
正如我们所使用的ao ci
我们可以只键入def bash以访问Airflow cli
然后从2023年12月1日回填dag到2024年1月1日
我们键入airflow dags backfill以起始日期2023年12月1日
二十和结束日期2024年1月1日
二十二十三 十二
哦一和结束日期2024年1月1日 哦一
所以2024年1月1日
最后我们指定id
管道的唯一标识符
这是零售并像那样
如果您吃和等一会儿
Airflow正在回填零售dag直到2024年1月1日
实际上如果我们等一会儿
您可以看到回填过程已成功完成回到UI
如果您刷新页面
您将看到一些图表中的小图标
并且此图标表示此运行是在回填作业的上下文中执行的
Run type backfill
如果我们将日期更改为2023年12月1日
我们可以看到
这些图表如此处
另一件需要记住的事情是，我们已经能够回填dag
即使这是用户界面的摆拍
所以记住你不必暂停你的dag
如果你想回填它，好的，现在
让我们继续第二个用例
我们有失败的daggers或任务实例
并且我们希望重新运行它们以说明这个用例
我们需要使任务开始面
为此我们可以在任务后抛出异常
让我们说抛出value error并带有消息
现在让我们回到ui dags并取消暂停零售
然后翻转页面
现在我们有额外的dd图表
你可以看到他们都在失败，当我们抛出这个异常时
所以让我们再次暂停dag
我想我们有足够的图表
现在 下一步是找到您想要回填您的dig的时间间隔
为您想要回填的dig找到合适的时间间隔，你有两个选项
第一个是去浏览
第二个是使用find_interval
然后dag运行并搜索匕首
我会重新检查
所以你的匕首
然后你只需滚动到看到匕首开始失效的地方
你可以看到那是在2024年1月25日
所以现在我们有了开始时间
我们可以查看结束时间
这是2024年2月5日
如果你有一个匕首
多个dd图表每天
那么这就是我推荐的选项
我建议你使用
但如果你有一个dagg
每天跑一个dagg
相反，你可以使用第二个选项
这是通过使用日历视图
所以这里点击日历
只要看看日期
你看到dagg在2024年1月25日开始失败
并在2024年2月5日达到顶峰
所以我们有了我们的间隔
我们准备好在终端类型Airflow Dags中填充甲板了
填充
2024年的Dash s
2025年的Dash o 1 2024年的e
哦2
哦5和id零售
所以让我们等一会儿
正如你所看到的，我们遇到了很多错误
这里发生了什么
当我们还在任务开始时抛出异常
显然它仍然失败
这就是你在这里看到的每个任务实例
我们有一个失败的情况，正如右边所示
有一个列非常有趣
尝试次数
所以尝试次数是什么
一个任务有一个内部设置称为尝试次数
每次任务完成运行
这个数字增加
无论任务的状态如何
它可以成功或失败
这都不重要
然而 如果任务大部分时间失败
你将会重新运行它
假设你重新运行了这个任务
火车编号也会增加
然后任务再次失败
所以你再次运行它，尝试次数再次增加
但是尝试的极限是什么，嗯
这由一个参数定义，你可以定义它，叫做retries
所以，假设你有一个任务，你想让它最多尝试三次
在状态变为失败之前
你可以将retries设置为3
最终，airflow会在标记为失败之前尝试三次
关于backfill的评论有什么意义
嗯 如果我们回到终端
当前的尝试次数是7
它可能不同
但重点是如果我们再次运行backfield命令
你可以看到火车号码没有改变
实际上 如果我们定义一个retries参数
让我们说 retries等于九
比这里我们有的tri数字更大并再次运行命令
我们仍然得到相同的试号
所以你必须记住这里是默认的
如果你在重新安排那些已经失败的任务
重新安排任务命令不会自动重新运行那些任务
为了自动重新运行那些任务
你必须使用选项，这是一个选项
重新运行失败的任务
选项
如果你现在再试一次
你可以尝试第八次
但这里有一点奇怪，对吧
我的意思是 如果我们说我们尝试
让我们说十五
好的 所以我们想尝试这个任务十五次，然后才能在失败中获取它
然后我们再次运行backfield
你可以看到
backfield尝试运行任务只有一次
就是这样
那么为什么不是十五次
因为ddiagrams中任务失败的地方也会失败
这就是你看到的
每个dag运行都失败了
如果你想要遵循重试次数使用backfield命令
那么你也必须重置ddiagrams以及重置diagrams
好的 所以不仅重新运行任务
也要重新运行diagrams
以便你可以遵循重试
你需要添加选项--reset
重置
Dagrants 和如果你尝试
然后你确认这样你就可以看到所有那些图表都会被重置
现在
Backfill 我们会尝试按照 retries 参数定义的任务重新运行
这就是你可以在这里看到的
你看到 Backfill 正在等待任务再次运行
实际上你可以在 UI 中验证这一点
所以如果你刷新页面
你看到 dagons 正在运行并且任务正在等待重试
即使在 CLI 中你也有任务等待重试
所以你可以看到有12个任务等待执行
这太好了 因为后台机制会自动重试你的任务，直到达到免费尝试的次数
但这以显著减缓填充你的DAG的过程为代价
后填机制会显著减慢你的DAG填充过程
但如果你不想自动重试你的任务呢
当你后填DAG时
嗯 你有另一个选择
那就是dash
冲刺 禁用
重试并且以那种方式
如果任务失败
然后它失败了 那就是它
你不会像改任务那样多次重写
正如在重试次数中定义的那样
你将节省很多时间
通常当你补全一个有向无环图时
你不应该失败
让我们删除异常并运行注释
并且它做得完美
如果我们看一下dii
然后刷新页面
我们已经有了所有所有被补全的图表，正如预期
并且你可以再次看到那个小图标
有一个选项我还没有提到
因为我认为你不会用它，诚实地说
那就是dash
Dash
在失败时
如果设置补全，它将继续进行
即使任务失败
所以如果你不关心你的任务失败，好吧
那就是你应该使用的选项
让我们谈谈性能和优化
所以当你补全一个dag时
默认情况下 它会从最旧的dag开始到最新的
大多数时候，这就是你想要的
但是如果你想要相反的
如果你希望从最新的tag开始
运行最新的到最旧的，嗯
你可以用另一个选项做到这一点
那就是反向运行
这是一个很好的选项，如果你希望从最近的图表开始
然而，如果你使用这个选项来加快补全dag的过程
以便它可以继续运行接下来的日子
那不是最好的选项
让我看看你应该怎么做
回到dag
假设我们希望为夏天补全它
那么，我们有补全命令
我们只是更改起始日期
并且是2023年
直到2023年
1230
好的 并且不要忘记重置图表
因为否则，补全将不会重新运行那些已经成功执行的图表
所以，这是一个非常重要的选项
好的 现在我们想要模拟一个长时间的任务
并且我们可以使用slip来做到这一点
并且我们希望睡眠五分钟
并且不要忘记在顶部导入那个函数
让我看看如果你补全这个dag会发生什么
好的，现在
如果我们回到ui
我们有16个正在运行的图表
问题是为什么16个，嗯
因为默认情况下，您不能为特定图表运行超过16个图表
同时运行
这在Airflow配置文件中定义了max active runs per dag
或者您可以在图表级别定义它max
如果您在回填图表
并且您有16个图表
这意味着您将无法运行
您图表的当前日期或下一天的图表
因此您将错过这些图表
这不是您想要的
您仍然希望像往常一样运行您的图表规则
同时能够回填之前的图表
问题是您如何做到这一点
您需要克隆您的图表
所以让我们回到这里并暂停这个过程
然后在图表目录中
点击您的图表零售
然后复制并粘贴到同一目录中
让我们重命名它为retail backfill
并且我们可以使用相同的名称用于deck id
所以retail backfill
好的 所以我们刚刚克隆了图表
但具有不同的
所以保存
然后让我们回到UI
我们可以删除正在运行的图表
因为我们已经中断了回填过程
这就是为什么我们可能有一些卡住的运行图表
所以让我们回到图表
然后我们选择或运行图表
然后过滤状态并运行
然后搜索它们
选择所有
并且我们可以点击操作和所有
让我们说设置为成功
我们还有一些在cued中
让我们回到ddiagrams
搜索那些带有cued的图表
我们选择所有操作和成功
好的
所以现在我们要回填图表
返回12月
正如我们之前所做的
同时保持另一个零售图表像往常一样运行
所以让我们开启零售的切换
我们有一些正在运行的图表
好的 就像之前一样，十六
但现在我们想使用零售后场
为此，您需要回到cody详情
您使用与以前一样的指令
但使用零售后场，并且您希望运行此命令以获取12月的数据
所以，输入并稍等片刻
让我们回到四UI
您将看到我们有16个图表正在运行零售
因此，我们不应该能够运行其他任务
然而，如果您刷新此页面
您可以看到，使用零售后场
我们可以继续为12月的零售进行回溯
这就是您在日历中看到的
这就是您应该如何回溯您的图表
所以请记住
如果您想回溯一个图表
首先克隆它
然后使用该克隆回溯图表
以便您可以保持另一个运行
并且您不会错过任何一天
这很好 但有一个细节您需要处理这两个图表
使用相同的池
如果您不明白我在说什么
如果您去管理并池
有一个默认池
并且该默认池被所有您的图表使用
每当任务运行时
所以您看到，默认情况下
您可以同时运行1到28个任务
并且现在您有32个任务正在运行
这意味着什么
这意味着如果您用于回溯的图表
占用了池中所有可用的插槽
实际图表将无法执行其他任务
因为池已满
因此您将回到之前的例子
其中您的图表将等待回溯完成
为了避免这种情况，您需要创建一个专用池用于回溯过程
去管理
然后池创建一个新的池
我们称之为零售回溯，128个插槽
然后点击保存
现在我们有零售回溯
好的 现在我们有了这个
我们可以回到代码编辑器
在终端中，我们添加了选项池与池零售回溯
并且我们当然需要更改图表ID以用于回溯
这是零售回填
让我们运行命令
好的 我们将设置图表
如果我们往上滚动一点
我们可以看到回填使用了新的零售回填池
事实上 如果你回到UI
你看到零售回填池有16个任务正在运行
而默认池只有两个任务正在运行
通过这样做，我们确保了零售dag可以像往常一样运行
同时，可以使用克隆dag进行之前的dag的补货
好的 到目前为止，我们已经大大改进了补货过程
然而，我们并没有真正影响资源
即使我们创建一个池或克隆一个dag
它们仍然共享相同的资源
无论你是否补货
那么，如何确保你使用的用于补货的dag有自己的资源，以便它们不会影响你的其他dag
这样他们就不会影响应用的运行
你有三个选项
顺便说一下 这个部分是为高级用户设计的
所以第一个选项是使用选项local
这意味着后台将使用本地执行进行运行
因此，后台进程将在单个机器上运行
一台计算机
这意味着您必须在单个机器上拥有足够的资源来支持它
这可能很方便
但如果您有繁重的计算任务，那就不会很多
那么还有两个其他选项是什么
嗯 如果你使用kubernetes执行进行任务
在kubernetes中的pod中
你可以指定人类walker pod的资源量
这样你的任务不会超出你定义的限制
这就是你可以在这里看到的backfill要求
我们定义了我们请求的资源量
以及限制是什么
然后我们在dag的默认args中使用这个要求
所以这些要求适用于每个任务
但是现在如果你不用kubernetes执行者
而是使用saler执行者
那么你可以创建一个队列，将你的后台任务发送到特定的机器电脑
以某些资源，以便它们不会影响其他任务
这就是关于后台的全部
关于它还有其他事情要说
例如 你有这个选项，如果你希望根据ID模式后台填充多个dag，这可能会有用
基于模式在ID
例如你有
我不知道一个团队
然后你使用团队的名字作为dags的前缀
然后你可以使用这个前缀
所以带有这个选项的团队的名字
为了补全所有特定团队的dags
所以这非常有用
你也可以做同样的事情给任务，最后
但并非最后，你有conf在最上面
这可能有用，如果你想将一些配置设置传递给你的dagg运行
当你补全一个dagg
这就是这个视频的全部内容 我下次见你
```

### /content/drive/MyDrive/bilibili/ApacheAirflowHandsOnGuide/91_Udemy - Apache Airflow The Hands-On Guide p91 7. VIDEO The DockerOperator The basics and more.ai-zh.srt

```
【角色设定】
你是一位精通 AWS Certified Solutions Architect – Associate (SAA-C03) 认证知识的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS 专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 技术课程自动机翻而来，存在以下常见问题：
- AWS 专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 专业术语，使其与 AWS 官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS SAA-C03 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 官方表述），
然后提供一份 **AWS SAA-C03 认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好
已经一个月了，再次见到你们真好
因为我可能一个月前制作了一个视频
我非常兴奋在这里见到你们
因为在这个视频中，你将会发现
或者你可能学到一些关于Docker Operator的新知识
我真的很喜欢装饰器
因为正如你所看到的，Docker Operator确实可以帮助你
但你需要确切地知道如何使用它
关于它有什么秘密
为什么这对你来说很重要
别忘了订阅我的频道
这样你就不会错过任何视频
而且 更重要的是 你将会及时了解Airflow的最新功能
并且点赞按钮
这对YouTube算法总是好的，说到这里
让我们开始吧，确保你会得到
为什么这个功对你很重要
我喜欢用例子开始
这正是你将在Docker Operator中看到的
所以让我们想象你有以下脚本
你对这个脚本感到非常自豪
也许它负责处理你的数据
但问题是 这个脚本在运行时需要很多依赖项
例如 NumPy
或者它可能需要psychic learn
或者它需要
嗯
我不知道 火花来运行这个东西
这个脚本需要大量的依赖项
但现在让我们想象一下，你是在使用celery executors运行airflow
所以你有一个机器，web服务器和调度器在上面运行
也许你有另一台机器，你的任务将在那里执行
所以网络工人
然后第三台机器
第四个和以此类推
好的 所以你有多个机器，在那里你的任务将被执行
但问题是，当你编写脚本时
需要很多依赖项才能运行
你将不得不在每个机器上安装所有那些依赖项
否则你的脚本将不会运行
它将无法工作
这就是大问题所在
你必须安装那些依赖项
你将不得不维护这些依赖项
并且你必须确保这些依赖项不会和你机器上已经安装的依赖项冲突
所以在你的机器上，你将不得不确保这些依赖项不会和你机器上已经安装的依赖项冲突
所以你可以想象，你真的可能会陷入噩梦
但这还不是全部
正如你所知 在 airflow 的上下文中
在开始测试你的任务时可能会遇到一些困难
因为你不得不处理你的 dag 对象
图表对象等等 但问题是
如果你使用 curator 在这种情况下
测试你的任务会更容易
为什么，因为最终你的任务脚本
这个脚本将只是一个脚本
一个 python 脚本，它将在 docker 映像中被构建
所以你要做的就是测试你的脚本
只测试你的脚本，而不必处理 airflow 的概念
你不必处理 airflow 的概念
你知道的 airflow 的概念
所以，就像，
就像你
你通常在 python 的上下文中测试你的任务
最后 但并非最不重要的是，正如你所知
airflow 有很多不同的操作器供你使用
可能有超过80个不同的操作器
这很好，因为这意味着你可以与超过80种不同的工具进行交互
但问题是，真正理解每个操作器需要很长时间 再次，这就是 docker 操作器非常有帮助的地方
因为只要你了解 docker 操作器，你就可以使用它并使用它运行任何你想要的东西
例如，如果你想要提取数据，
你创建一个脚本来执行这个任务，
你把它放入 docker 映像中，
然后你使用 docker 操作器执行这个 docker 映像
你想要存储你的数据，
你创建一个脚本来执行这个任务，
你把它放入 docker 映像中，
然后你使用 docker 操作器执行这个 docker 映像
所以，最终，你只需要理解操作器
我不说你不应该使用任何操作器
我只是说，有了 docker 操作器，你可以使用更少的操作器，如果你不使用它的话
所以，最终，你只需要理解操作器
我不说你不应该使用任何操作器
我只是说，有了 docker 操作器，你可以使用更少的操作器，如果你不使用它的话 我不说你不应该使用任何操作器
我只是说，有了 docker 操作器，你可以使用更少的操作器，如果你不使用它的话
那么，话虽如此
让我们看看如何具体实现curator
好的 假设你有以下数据计划非常简单
只有两个任务
嗯 实际上现在是一个任务
T1和T2，T2将是Docker操作员
如果你在思考什么是ad dag和add任务这里，那么
这意味着你不知道测试API
如果你不知道aspi
请看一下
这是你在airflow中创建dags的一种新方法
那么
你如何在你的时间中使用docker ator
嗯 第一步是安装docker提供商
如果你去airflow文档
你会找到apache airflow提供商
Docker你必须安装该提供商
否则你将无法访问Docker操作员
一旦你有了它
这很简单 你需要导入你的Docker操作员
所以你输入from airflow that providers
点Docker点操作员
点Docker导入Docker操作员
就是这样
你现在准备好在你的Dag中使用装饰器了
所以我们在这里可以创建一个新的任务
T two等于Docker操作员
具有任务ID等于T two
那么现在您可以指定的参数是什么
显然第一个是您的任务对应的镜像
例如 这里假设您想使用Docker的Python镜像
您可以输入image等于python
然后three eight dash
Slim dash buster
这意味着您将要做的事情
你将在Docker镜像中执行你的任务
现在是Python3.8
请记住一件事
如果你在f的语境中使用它
例如 如果你使用序列执行者，这意味着你将拥有
你必须确保你的Docker镜像可以从你所有机器访问
因此，你通常需要设置一个本地注册表或在线注册表
你将在那里推送你的Docker镜像
好的 但是请记住，你的docker镜像必须从你所有机器上可用
话虽如此 你可以指定的第二个参数是comment
它对应于你将在docker容器中执行的命令
例如 等于，假设你想在容器的标准输出上打印comment
如果你的docker镜像有一个入口点
那么你可能不需要使用那个参数
因为 你的docker镜像的入口点将自动在你启动docker容器时执行
但是知道与论据评论很好
你可以几乎做任何你想做的事情
例如 如果你想运行一个脚本
您只需要输入python 3
然后输入我的脚本.py
好的 所以相当简单
此外 您可以指定Docker URL
这是设置为unix的，所以是unix冒号
斜杠 var
斜杠 run 斜杠
docker点suck
这与docker demon监听到的unix socket相对应
所以基本上它是主要的入口点
这样您的服务器操作员就可以与您机器上的Docker进行交互
如果您没有指定Docker套接字
那么您将无法使用Docker操作员运行Docker容器
还有一点需要注意
如果您在Docker容器中运行Airflow
并且您想使用Docker操作员
这样您就会得到一个对应于您任务的Docker容器
在您F实例对应的Docker容器中运行
您必须确保这个Docker套接字被挂载到Docker容器中
您的空 例如正在运行
再次授权
你将无法在你的Docker容器内运行你的Docker容器
好的 不要犹豫
嗯 再看一遍这个视频的部分
这可能会让人困惑
但你需要知道最后这个参数是网络模式
这个更具体
你知道 Docker 基本上你有桥接和主机的选择
通常你会选择桥接
这意味着你的Docker容器会有一个内部网络，与你的网络分开
如果你使用主机
这种情况下，你会与你的Docker容器共享同一个网络
但是再次，你应该总是将其设置为桥接
我的意思是，这真的很常见
现在你已经保存了文件
让我们看看它在UI上的表现
让我们打开DAG在汽车标签上的切换按钮并点击它
然后稍等片刻，直到T2完成
现在完成了
点击它 转到日志，你将获得以下输出
这基本上对应了你为你的Docker操作员指定的命令
在这里，你将获得在Docker容器中运行的命令输出
这已经在Docker容器中执行
正如那里显示的那样
所以现在你需要知道的一件事是，默认情况下，标准输出上打印的最后一行会自动存储为一个环境变量
最后一行打印到标准输出后，会自动存储为环境变量X_COM
如果你去admin和X_COM，你可以看到这一点
你可以看到键值对存储在X_COM的值中
值对应你的命令输出
你可以通过修改参数来启用或禁用这个功能
do_x_push参数被默认设置为true
这适用于你所有的操作
但这适用于你所有的操作
但是如果你有多行输出，该怎么办
实际上，也许你的评论会输出一系列内容
你想将这些内容存储在一个xcom中
如何做到这一点，很简单
但首先，让我向你展示
如何使用你自己的docker镜像
假设你的目标是运行以下脚本
是买入还是卖出
基本上，你在标准输出中打印顶级推荐
一旦你有了你的脚本
你创建一个docker文件
在那个docker文件中你可以放你的脚本
在这种情况下，stuck on the score data py
你也会使用requirements.txt文件
在那里你将指定你脚本所需的所有依赖项
一旦你有了这些，你就只需要构建你的docker镜像
所以在你的终端中，你会输入docker like that dash t stuck on a square image
你的docker镜像名称
如果你按回车
你可以看到docker镜像已经构建
现在您需要为您的Docker镜像打标签
我强烈建议您这样做
所以您只需输入docker tag并指定您的镜像
冒号latest
这是默认版本
然后指定您的镜像和版本
我始终建议您这样做
因为，嗯，您可能需要修改您的任务
您的脚本 因此每次您这样做
您将构建一个新的镜像
并且需要更新您的Docker镜像版本
否则您将不知道您的脚本执行了哪个版本
所以这确实是一个好做法
顺便说一句，您知道这里所有事情都应该在CI/CD流水线中进行
这样所有事情都将自动为您完成
基本上运行您的单元测试
您的脚本并构建镜像
一旦您完成了这些，请转到您的DAG
在这里您只需要更改镜像名称
所以指定您的镜像为score_image_v1.0.0
并且删除这个注释
您想要执行脚本
因此您将输入python3
然后指定score_data.py
如果您设置了文件并在UI上查看
您将获得以下输出，值为neutral
实际上，如果您查看您的XComs
您可以看到新的XCom值为neutral
好的 现在，如果您的脚本产生多行输出怎么办
如何在另一个put中开始它们
在这种情况下，您需要设置的唯一参数是XCom
设置为true，让我们修改脚本
因此，在这种情况下，您有
例如，您想要获取第二个推荐
设置文件
我们将使用gray_image并创建一个新标签
让我们保持简单，使用相同的标签
但是，在这种情况下，您应该使用v1.0.1
例如
让我们再次运行Dag
现在 任务已完成
如果您点击它并转到日志
这次您将获得neutral和by两条输出
如果您查看您的XComs
您可以看到新的XCom被创建
但这次您在相同的XCom中看到了neutral和by
并且，正如您所看到的，输出并不很清楚
显然，如果你这样做会更清晰
例如 如果你的脚本产生一个json值
如果你希望你的脚本能获取一些东西
但这不会打印到标准输出
你可以做的事情之一是利用pickle
例如 这里
假设你想象征化以下json值
在这种情况下，你会使用pickle点dump
你将其放入脚本out文件中
一旦你有了它
这在你的脚本中，你的dag中
更具体地说，在你的docker operator中
你可以使用两个参数
第一个是retrieve underscore output等于true
然后rewheel underscore output onscore path等于
在这种情况下cmp/slash/script.dot/out
如果你保存了文件并再次运行，
正如你在x comes中看到的
你将得到以下值test
好的 所以再次
这真的很有用
如果你有一些你想要保存的值
但这些值没有打印到标准输出
还有一件事，我非常喜欢优先级
你可以定义你的任务所需的资源
例如 你可以定义你想要的cpu比例
这真的很有用
如果你有多个任务使用cooperator
但你想说嘿
我的任务需要更多的权重，比其他任务
因为那个更cpu密集
例如 要了解如何定义这个参数
你必须查看docker的文档
更具体地说，查看该链接
另一个参数是mem onscore limit
这是你定义你的任务将使用的最大内存量
例如1GB或5GB
2MB
随你便
但重点是你可以说
好的 我想确保这个任务不会用超过我设定的内存限制
好的 这就是控制你任务的两个参数
这也是你的任务使用的
另一个相当有用的参数是自动删除，实际上我建议你将其设置为true
总是因为那样会删除Docker容器特性为你的任务
你知道这正好相当于docker或m
然后您的内容容器ID
所以将那个参数设置为真
好的，是时候谈谈一个非常重要的点了。
如果你希望在你的docker容器上挂载卷，该怎么办呢？
你怎么能做得好
首先，首先 如果你使用的是mac o
你需要知道一些事情
你需要启用这个设置
以便将你的卷挂载到你的docker容器中
否则你的docker容器将超时
好的 这很重要
这仅适用于mac os用户
一旦你做了这件事
你需要指定一个参数，那就是months
这个参数期望一个挂载对象的列表
那么在这种情况下，你需要导入docker类型
然后您创建一个挂载，就像那里一样，源
所以您想将您的机器绑定到目标
对应于容器中的挂载点
那么在这种情况下，我将绑定给脚本
从我的机器到脚本在容器的本地脚本
并且这个脚本有一个脚本输出，那个sh
我现在将运行它
所以如果您回到dag
正如您所看到的，注释现在是bash gp脚本输出h
让我们看看它是否在fli上起作用
在汽车dag上打开docker的桌子
让我们等一会儿
点击t两个并转到日志
如你所见，消息是我的脚本
这意味着使用卷脚本已成功执行
也请注意我拥有的那个小空间，它就在那里
确实 如果你不放那个空格，f会认为这个文件实际上是一个模板文件
因此它不会起作用
因为，嗯 这个文件实际上是一个脚本，你想要执行它
而不是一个模板文件
Airflow应该评估
现在 还有你绝对需要了解的一件事是
在幕后自动
Docker操作员会创建一个临时文件夹
一个临时的卷
并且这是由默认启用的
所以如果你有大文件
这些文件超出了容器10GB的磁盘大小
它会工作 但你必须小心
因为你在docker中运行docker
所以如果你的docker操作员在docker容器中运行
那么它将不会工作 然后你必须用months underscore tp参数禁用该功能
或者score等于false
但你仍然可以使用其他方式使用months参数挂载卷
好的 所以知道这一点很重要
并且这仅在您使用docker in docker时才适用
作为最佳实践的最后一点
我强烈建议您始终设置容器名称
例如 您可以指定容器名称
容器名称等于任务on the score t two
好的，因为
如果您最终有很多docker容器
可能很难区分什么是什么
如果你没有设置容器名称
好的 显然，你必须查看由你的docker容器执行的注释
这不是你想要的
所以，总是根据你的任务定义容器名称
ID
好的 这就是docker operator的全部内容
还有其他需要了解的东西
如何扩展运营商的功能性
并且 更具体地说
如果你想模板月份的参数怎么办
如何将数据在两个docker运营商之间共享
以及如何利用新的docker装饰器
以便使用docker运营商
所以我们将在另一段视频中看到
但我希望你喜欢这段视频
别忘了订阅并喜欢这个视频 我期待下次再见你，祝你有美好的一天
```